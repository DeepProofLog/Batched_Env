Running experiments for the following parameters: DATASET_NAME: ['kinship_family'] MODEL_NAME: ['PPO'] SEED: [[0]]
Experiment number  0  out of  1  experiments.
Seed 0  in  [0]

Run vars: kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-2-python-1 
 Namespace(dataset_name='kinship_family', model_name='PPO', learn_embeddings=True, atom_embedder='transe', state_embedder='mean', atom_embedding_size=64, seed=[0], max_depth=20, timesteps_train=4000000, restore_best_val_model=True, memory_pruning=True, rule_depend_var=False, dynamic_consult=True, corruption_mode='dynamic', train_neg_pos_ratio=1, false_rules=False, end_proof_action=True, skip_unary_actions=True, ent_coef=0.1, clip_range=0.2, engine='python', depth_filtered=2, truncate_atoms=True, truncate_states=True, padding_atoms=3, padding_states=10, non_provable_queries=True, non_provable_corruptions=True, corruption_scheme=['head', 'tail'], janus_file=None, data_path='./data/', train_file='train_depth_2.txt', valid_file='valid_depth_2.txt', test_file='test_depth_2.txt', rules_file='rules.txt', facts_file='train.txt', state_embedding_size=64, constant_embedding_size=64, predicate_embedding_size=64, variable_no=500, device='cuda', load_model=False, save_model=True, models_path='models/kinship_family', n_eval_queries=200, n_test_queries=None, valid_negatives=None, test_negatives=100, eval_freq=16384, n_envs=128, n_eval_envs=128, n_callback_envs=1, n_steps=128, n_epochs=10, batch_size=128, lr=0.0003, run_signature='kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-2-python-1', seed_run_i=0) 

kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-2-python-1
Device: cuda
CUDA available: True, Device count: 2
Using cuda device
Embedding dim in policy 64
Embedding dim in value 64
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=16384, episode_reward=0.23 +/- 0.42
Episode length: 1.35 +/- 0.56
New best mean reward!
---------------evaluation finished---------------
Epoch 1 completed in 191.52 seconds.
Improved rollout/ep_rew_mean to 0.2400
Time to collect_rollouts 191.56
Training model
Time to train 54.15
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.49     |
|    ep_rew_mean          | 0.24     |
| time/                   |          |
|    fps                  | 66       |
|    iterations           | 1        |
|    total_timesteps      | 16384    |
| train/                  |          |
|    approx_kl            | 9.862591 |
|    clip_fraction        | 0.419    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0728  |
|    explained_variance   | -47.6    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.167    |
|    n_updates            | 10       |
|    policy_gradient_loss | 0.142    |
|    value_loss           | 0.95     |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=32640, episode_reward=0.17 +/- 0.38
Episode length: 1.36 +/- 0.60
---------------evaluation finished---------------
Epoch 2 completed in 189.27 seconds.
Time to collect_rollouts 189.28
Training model
Time to train 56.23
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.52     |
|    ep_rew_mean          | 0.18     |
| time/                   |          |
|    fps                  | 66       |
|    iterations           | 2        |
|    total_timesteps      | 32768    |
| train/                  |          |
|    approx_kl            | 20.00214 |
|    clip_fraction        | 0.501    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0641  |
|    explained_variance   | 0.0301   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0666   |
|    n_updates            | 20       |
|    policy_gradient_loss | 0.0232   |
|    value_loss           | 0.157    |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=48896, episode_reward=0.58 +/- 0.49
Episode length: 1.94 +/- 0.53
New best mean reward!
---------------evaluation finished---------------
Epoch 3 completed in 208.95 seconds.
Improved rollout/ep_rew_mean to 0.3300
Time to collect_rollouts 209.03
Training model
Time to train 55.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.23      |
|    ep_rew_mean          | 0.33      |
| time/                   |           |
|    fps                  | 64        |
|    iterations           | 3         |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 7.2497516 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0492   |
|    explained_variance   | -0.00846  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0877    |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0278    |
|    value_loss           | 0.19      |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=65152, episode_reward=0.65 +/- 0.48
Episode length: 1.91 +/- 0.40
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 4 completed in 204.17 seconds.
Time to collect_rollouts 204.18
Training model
Time to train 55.96
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.82      |
|    ep_rew_mean          | 0.33      |
| time/                   |           |
|    fps                  | 64        |
|    iterations           | 4         |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 10.246278 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0446   |
|    explained_variance   | 0.00841   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0897    |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.00525   |
|    value_loss           | 0.213     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=81408, episode_reward=0.65 +/- 0.48
Episode length: 1.94 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 5 completed in 203.65 seconds.
Improved rollout/ep_rew_mean to 0.3700
Time to collect_rollouts 203.73
Training model
Time to train 54.99
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.3       |
|    ep_rew_mean          | 0.37      |
| time/                   |           |
|    fps                  | 64        |
|    iterations           | 5         |
|    total_timesteps      | 81920     |
| train/                  |           |
|    approx_kl            | 7.2951245 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0502   |
|    explained_variance   | 0.0273    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0834    |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.0113    |
|    value_loss           | 0.189     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=97664, episode_reward=0.62 +/- 0.49
Episode length: 1.90 +/- 0.37
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 6 completed in 192.96 seconds.
Time to collect_rollouts 192.97
Training model
Time to train 55.48
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.62     |
|    ep_rew_mean          | 0.36     |
| time/                   |          |
|    fps                  | 64       |
|    iterations           | 6        |
|    total_timesteps      | 98304    |
| train/                  |          |
|    approx_kl            | 6.234729 |
|    clip_fraction        | 0.302    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0595  |
|    explained_variance   | 0.0153   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0853   |
|    n_updates            | 60       |
|    policy_gradient_loss | 0.0322   |
|    value_loss           | 0.211    |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=113920, episode_reward=0.66 +/- 0.48
Episode length: 1.97 +/- 0.51
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 7 completed in 203.73 seconds.
Time to collect_rollouts 203.74
Training model
Time to train 55.67
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.34      |
|    ep_rew_mean          | 0.37      |
| time/                   |           |
|    fps                  | 64        |
|    iterations           | 7         |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 1.0065928 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.445    |
|    explained_variance   | 0.00195   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0289    |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.0577    |
|    value_loss           | 0.183     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=130176, episode_reward=0.63 +/- 0.48
Episode length: 1.96 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 8 completed in 201.81 seconds.
Time to collect_rollouts 201.82
Training model
Time to train 54.97
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.73       |
|    ep_rew_mean          | 0.31       |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 8          |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.49630898 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.975     |
|    explained_variance   | 0.0406     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.041      |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0477     |
|    value_loss           | 0.186      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=146432, episode_reward=0.66 +/- 0.47
Episode length: 1.98 +/- 0.50
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 9 completed in 197.11 seconds.
Time to collect_rollouts 197.11
Training model
Time to train 55.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.87       |
|    ep_rew_mean          | 0.32       |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 9          |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.10924704 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.0535     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0669    |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0185     |
|    value_loss           | 0.175      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=162688, episode_reward=0.68 +/- 0.47
Episode length: 2.02 +/- 0.64
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 10 completed in 193.10 seconds.
Time to collect_rollouts 193.1
Training model
Time to train 55.64
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.91       |
|    ep_rew_mean          | 0.29       |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 10         |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.05987533 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.0556     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0883    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00138   |
|    value_loss           | 0.168      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=178944, episode_reward=0.68 +/- 0.47
Episode length: 2.02 +/- 0.63
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 11 completed in 204.96 seconds.
Time to collect_rollouts 204.97
Training model
Time to train 56.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.11       |
|    ep_rew_mean          | 0.28       |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 11         |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.03824281 |
|    clip_fraction        | 0.087      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.0625     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0452    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 0.166      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=195200, episode_reward=0.67 +/- 0.47
Episode length: 2.00 +/- 0.55
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 12 completed in 202.05 seconds.
Time to collect_rollouts 202.06
Training model
Time to train 55.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 12          |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.013905038 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.0469      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0595     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.163       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=211456, episode_reward=0.68 +/- 0.47
Episode length: 2.00 +/- 0.54
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 13 completed in 200.49 seconds.
Time to collect_rollouts 200.5
Training model
Time to train 56.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 13          |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.008097019 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0412      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.163       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=227712, episode_reward=0.68 +/- 0.47
Episode length: 2.00 +/- 0.51
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 14 completed in 201.40 seconds.
Time to collect_rollouts 201.41
Training model
Time to train 29.64
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.24         |
|    ep_rew_mean          | 0.29         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 14           |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0062405653 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.075        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0406      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 0.159        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=243968, episode_reward=0.68 +/- 0.47
Episode length: 1.99 +/- 0.50
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 15 completed in 199.76 seconds.
Time to collect_rollouts 199.76
Training model
Time to train 57.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28        |
|    ep_rew_mean          | 0.34        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 15          |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.004384445 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.0753      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0336     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.163       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=260224, episode_reward=0.68 +/- 0.47
Episode length: 1.98 +/- 0.48
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 16 completed in 202.40 seconds.
Time to collect_rollouts 202.4
Training model
Time to train 58.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.34         |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 16           |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0055298978 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.977       |
|    explained_variance   | 0.0823       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0222      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 0.157        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=276480, episode_reward=0.69 +/- 0.46
Episode length: 1.97 +/- 0.47
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 17 completed in 200.50 seconds.
Time to collect_rollouts 200.5
Training model
Time to train 60.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47         |
|    ep_rew_mean          | 0.28         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 17           |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0040042563 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.948       |
|    explained_variance   | 0.0964       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0502      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.157        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=292736, episode_reward=0.69 +/- 0.46
Episode length: 1.98 +/- 0.49
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 18 completed in 199.34 seconds.
Time to collect_rollouts 199.35
Training model
Time to train 59.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34        |
|    ep_rew_mean          | 0.34        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 18          |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.003636492 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.0997      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0232     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.16        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=308992, episode_reward=0.69 +/- 0.46
Episode length: 1.97 +/- 0.48
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 19 completed in 201.84 seconds.
Time to collect_rollouts 201.85
Training model
Time to train 58.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.27         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 19           |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0034289244 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.9         |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0286      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.152        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=325248, episode_reward=0.70 +/- 0.46
Episode length: 1.97 +/- 0.45
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 20 completed in 201.43 seconds.
Improved rollout/ep_rew_mean to 0.3800
Time to collect_rollouts 201.5
Training model
Time to train 57.82
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.19         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 20           |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0055081574 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.878       |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0235      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.154        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=341504, episode_reward=0.72 +/- 0.45
Episode length: 1.97 +/- 0.44
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 21 completed in 200.78 seconds.
Time to collect_rollouts 200.79
Training model
Time to train 57.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.61         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 21           |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0032945932 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.87        |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0154      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00914     |
|    value_loss           | 0.15         |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=357760, episode_reward=0.72 +/- 0.45
Episode length: 1.97 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 22 completed in 202.90 seconds.
Improved rollout/ep_rew_mean to 0.4000
Time to collect_rollouts 202.98
Training model
Time to train 57.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.48         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 22           |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0034527841 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.852       |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0529      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.152        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=374016, episode_reward=0.72 +/- 0.45
Episode length: 1.97 +/- 0.45
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 23 completed in 199.85 seconds.
Time to collect_rollouts 199.86
Training model
Time to train 51.5
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33        |
|    ep_rew_mean          | 0.33        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 23          |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.002977718 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.134       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.02       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 0.149       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=390272, episode_reward=0.72 +/- 0.45
Episode length: 1.96 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 24 completed in 195.81 seconds.
Time to collect_rollouts 195.81
Training model
Time to train 51.68
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.3          |
|    ep_rew_mean          | 0.32         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 24           |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0030841408 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0223      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00931     |
|    value_loss           | 0.146        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=406528, episode_reward=0.72 +/- 0.45
Episode length: 1.97 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 25 completed in 199.94 seconds.
Time to collect_rollouts 199.95
Training model
Time to train 37.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.82         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 25           |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0031763106 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.831       |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0467      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00958     |
|    value_loss           | 0.145        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=422784, episode_reward=0.72 +/- 0.45
Episode length: 1.96 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 26 completed in 196.98 seconds.
Time to collect_rollouts 196.99
Training model
Time to train 26.93
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.46         |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 26           |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0035326632 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.828       |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0318      |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 0.144        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=439040, episode_reward=0.72 +/- 0.45
Episode length: 1.95 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 27 completed in 199.65 seconds.
Improved rollout/ep_rew_mean to 0.4400
Time to collect_rollouts 199.72
Training model
Time to train 27.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.3          |
|    ep_rew_mean          | 0.44         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 27           |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0039142976 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.809       |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0338      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.146        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=455296, episode_reward=0.73 +/- 0.44
Episode length: 1.96 +/- 0.42
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 28 completed in 194.89 seconds.
Time to collect_rollouts 194.9
Training model
Time to train 29.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.49         |
|    ep_rew_mean          | 0.33         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 28           |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0041045705 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.032       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00898     |
|    value_loss           | 0.142        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=471552, episode_reward=0.73 +/- 0.44
Episode length: 1.96 +/- 0.43
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 29 completed in 198.08 seconds.
Time to collect_rollouts 198.09
Training model
Time to train 37.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43        |
|    ep_rew_mean          | 0.31        |
| time/                   |             |
|    fps                  | 65          |
|    iterations           | 29          |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.003494005 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0346     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 0.145       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=487808, episode_reward=0.73 +/- 0.44
Episode length: 1.95 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 30 completed in 192.99 seconds.
Time to collect_rollouts 192.99
Training model
Time to train 53.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.46         |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 30           |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0039252774 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.793       |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0437      |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 0.14         |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=504064, episode_reward=0.74 +/- 0.44
Episode length: 1.93 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 31 completed in 195.92 seconds.
Time to collect_rollouts 195.93
Training model
Time to train 33.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43        |
|    ep_rew_mean          | 0.28        |
| time/                   |             |
|    fps                  | 65          |
|    iterations           | 31          |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.004518279 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0466     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.14        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=520320, episode_reward=0.74 +/- 0.44
Episode length: 1.95 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 32 completed in 196.94 seconds.
Time to collect_rollouts 196.94
Training model
Time to train 31.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.92         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 32           |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0045947516 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.765       |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0388      |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.139        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=536576, episode_reward=0.76 +/- 0.43
Episode length: 1.95 +/- 0.40
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 33 completed in 195.10 seconds.
Time to collect_rollouts 195.1
Training model
Time to train 31.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.6          |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 33           |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0041967025 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.767       |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0518      |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.133        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=552832, episode_reward=0.76 +/- 0.43
Episode length: 1.95 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 34 completed in 190.94 seconds.
Time to collect_rollouts 190.94
Training model
Time to train 31.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.13         |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 66           |
|    iterations           | 34           |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0045004357 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.747       |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0279      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.136        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=569088, episode_reward=0.76 +/- 0.43
Episode length: 1.94 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 35 completed in 189.27 seconds.
Time to collect_rollouts 189.27
Training model
Time to train 31.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 66          |
|    iterations           | 35          |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.008978834 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00839    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.134       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=585344, episode_reward=0.76 +/- 0.43
Episode length: 1.94 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 36 completed in 190.45 seconds.
Time to collect_rollouts 190.46
Training model
Time to train 31.1
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.6          |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 66           |
|    iterations           | 36           |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0067754444 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.761       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.048       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 0.131        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=601600, episode_reward=0.77 +/- 0.42
Episode length: 1.94 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 37 completed in 190.78 seconds.
Time to collect_rollouts 190.78
Training model
Time to train 31.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.52         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 66           |
|    iterations           | 37           |
|    total_timesteps      | 606208       |
| train/                  |              |
|    approx_kl            | 0.0052306443 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.743       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.037       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.131        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=617856, episode_reward=0.76 +/- 0.43
Episode length: 1.95 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 38 completed in 195.32 seconds.
Time to collect_rollouts 195.33
Training model
Time to train 31.2
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.38         |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 66           |
|    iterations           | 38           |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 0.0044630617 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.739       |
|    explained_variance   | 0.263        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.041       |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.13         |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=634112, episode_reward=0.77 +/- 0.42
Episode length: 1.95 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 39 completed in 191.53 seconds.
Time to collect_rollouts 191.53
Training model
Time to train 31.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.59        |
|    ep_rew_mean          | 0.3         |
| time/                   |             |
|    fps                  | 66          |
|    iterations           | 39          |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.004762838 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0459     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.131       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=650368, episode_reward=0.77 +/- 0.42
Episode length: 1.94 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 40 completed in 190.18 seconds.
Time to collect_rollouts 190.19
Training model
Time to train 30.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.81         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 40           |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0043730587 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0353      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.128        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=666624, episode_reward=0.75 +/- 0.43
Episode length: 1.95 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 41 completed in 185.99 seconds.
Time to collect_rollouts 186.0
Training model
Time to train 29.2
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 41           |
|    total_timesteps      | 671744       |
| train/                  |              |
|    approx_kl            | 0.0045812866 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.725       |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0293      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.127        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=682880, episode_reward=0.78 +/- 0.42
Episode length: 1.94 +/- 0.39
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 42 completed in 185.68 seconds.
Time to collect_rollouts 185.69
Training model
Time to train 30.09
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.41       |
|    ep_rew_mean          | 0.34       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 42         |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.00616669 |
|    clip_fraction        | 0.0255     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.281      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0095    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.127      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=699136, episode_reward=0.77 +/- 0.42
Episode length: 1.95 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 43 completed in 185.61 seconds.
Time to collect_rollouts 185.62
Training model
Time to train 30.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 43          |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.004326377 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0453     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.127       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=715392, episode_reward=0.77 +/- 0.42
Episode length: 1.94 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 44 completed in 187.63 seconds.
Time to collect_rollouts 187.63
Training model
Time to train 31.68
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.57         |
|    ep_rew_mean          | 0.35         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 44           |
|    total_timesteps      | 720896       |
| train/                  |              |
|    approx_kl            | 0.0049491366 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.713       |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00833     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 0.126        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=731648, episode_reward=0.77 +/- 0.42
Episode length: 1.94 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 45 completed in 189.93 seconds.
Time to collect_rollouts 189.94
Training model
Time to train 37.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.39         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 45           |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0054670973 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.704       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0557      |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 0.123        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=747904, episode_reward=0.77 +/- 0.42
Episode length: 1.95 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 46 completed in 185.14 seconds.
Time to collect_rollouts 185.14
Training model
Time to train 50.4
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.56         |
|    ep_rew_mean          | 0.44         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 46           |
|    total_timesteps      | 753664       |
| train/                  |              |
|    approx_kl            | 0.0048748874 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.697       |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0449      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.121        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=764160, episode_reward=0.77 +/- 0.42
Episode length: 1.93 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 47 completed in 185.30 seconds.
Time to collect_rollouts 185.31
Training model
Time to train 50.39
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.91         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 47           |
|    total_timesteps      | 770048       |
| train/                  |              |
|    approx_kl            | 0.0054525235 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.692       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0374      |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 0.119        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=780416, episode_reward=0.78 +/- 0.42
Episode length: 1.93 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 48 completed in 188.28 seconds.
Time to collect_rollouts 188.29
Training model
Time to train 50.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.78        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 48          |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.014694733 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.023      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.121       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=796672, episode_reward=0.78 +/- 0.42
Episode length: 1.95 +/- 0.53
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 49 completed in 186.49 seconds.
Time to collect_rollouts 186.49
Training model
Time to train 50.6
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.26         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 49           |
|    total_timesteps      | 802816       |
| train/                  |              |
|    approx_kl            | 0.0057652914 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.677       |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0308      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.119        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=812928, episode_reward=0.78 +/- 0.42
Episode length: 1.93 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 50 completed in 184.61 seconds.
Time to collect_rollouts 184.62
Training model
Time to train 51.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.63         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 50           |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0047077592 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.666       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0319      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.116        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=829184, episode_reward=0.78 +/- 0.42
Episode length: 1.94 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 51 completed in 191.90 seconds.
Time to collect_rollouts 191.9
Training model
Time to train 49.96
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.13       |
|    ep_rew_mean          | 0.36       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 51         |
|    total_timesteps      | 835584     |
| train/                  |            |
|    approx_kl            | 0.00486415 |
|    clip_fraction        | 0.0315     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.055     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 0.118      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=845440, episode_reward=0.77 +/- 0.42
Episode length: 1.93 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 52 completed in 188.60 seconds.
Improved rollout/ep_rew_mean to 0.4600
Time to collect_rollouts 188.68
Training model
Time to train 49.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48        |
|    ep_rew_mean          | 0.46        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 52          |
|    total_timesteps      | 851968      |
| train/                  |             |
|    approx_kl            | 0.005173937 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.117       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=861696, episode_reward=0.78 +/- 0.41
Episode length: 1.94 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 53 completed in 182.85 seconds.
Time to collect_rollouts 182.85
Training model
Time to train 49.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.74        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 53          |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.012131019 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.677      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0579     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.114       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=877952, episode_reward=0.78 +/- 0.42
Episode length: 1.94 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 54 completed in 188.17 seconds.
Improved rollout/ep_rew_mean to 0.4700
Time to collect_rollouts 188.26
Training model
Time to train 50.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.53         |
|    ep_rew_mean          | 0.47         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 54           |
|    total_timesteps      | 884736       |
| train/                  |              |
|    approx_kl            | 0.0048881234 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0499      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.115        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=894208, episode_reward=0.79 +/- 0.41
Episode length: 1.94 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 55 completed in 189.26 seconds.
Time to collect_rollouts 189.27
Training model
Time to train 49.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.62        |
|    ep_rew_mean          | 0.42        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 55          |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.011203935 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.112       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=910464, episode_reward=0.79 +/- 0.41
Episode length: 1.95 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 56 completed in 184.32 seconds.
Time to collect_rollouts 184.32
Training model
Time to train 49.45
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.6          |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 56           |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0056851157 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0216      |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 0.113        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=926720, episode_reward=0.79 +/- 0.41
Episode length: 1.95 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 57 completed in 185.44 seconds.
Time to collect_rollouts 185.45
Training model
Time to train 49.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.75        |
|    ep_rew_mean          | 0.34        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 57          |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.005021418 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0422     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.107       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=942976, episode_reward=0.80 +/- 0.40
Episode length: 1.95 +/- 0.38
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 58 completed in 185.62 seconds.
Time to collect_rollouts 185.63
Training model
Time to train 49.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 58          |
|    total_timesteps      | 950272      |
| train/                  |             |
|    approx_kl            | 0.005195087 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.055      |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.11        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=959232, episode_reward=0.80 +/- 0.40
Episode length: 1.95 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 59 completed in 194.65 seconds.
Time to collect_rollouts 194.66
Training model
Time to train 49.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.61         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 59           |
|    total_timesteps      | 966656       |
| train/                  |              |
|    approx_kl            | 0.0053156675 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.636       |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0363      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.106        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=975488, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.40
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 60 completed in 179.27 seconds.
Time to collect_rollouts 179.28
Training model
Time to train 49.81
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 60           |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0048989737 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0283      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.106        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=991744, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 61 completed in 182.26 seconds.
Time to collect_rollouts 182.27
Training model
Time to train 49.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64         |
|    ep_rew_mean          | 0.46         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 61           |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 0.0053863055 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.638       |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0485      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.103        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1008000, episode_reward=0.80 +/- 0.40
Episode length: 1.95 +/- 0.36
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 62 completed in 186.92 seconds.
Time to collect_rollouts 186.92
Training model
Time to train 50.1
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.48       |
|    ep_rew_mean          | 0.43       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 62         |
|    total_timesteps      | 1015808    |
| train/                  |            |
|    approx_kl            | 0.00615732 |
|    clip_fraction        | 0.0359     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.625     |
|    explained_variance   | 0.416      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0386    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.105      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1024256, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 63 completed in 185.46 seconds.
Time to collect_rollouts 185.47
Training model
Time to train 49.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.79         |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 63           |
|    total_timesteps      | 1032192      |
| train/                  |              |
|    approx_kl            | 0.0050503956 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.41         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.033       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 0.105        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1040512, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 64 completed in 185.63 seconds.
Time to collect_rollouts 185.63
Training model
Time to train 49.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.61         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 64           |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0049888464 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0375      |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.104        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1056768, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 65 completed in 184.31 seconds.
Time to collect_rollouts 184.32
Training model
Time to train 50.1
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.43         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 65           |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0057722707 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.42         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0382      |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.103        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1073024, episode_reward=0.81 +/- 0.40
Episode length: 1.96 +/- 0.39
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 66 completed in 185.07 seconds.
Time to collect_rollouts 185.07
Training model
Time to train 50.82
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.26         |
|    ep_rew_mean          | 0.41         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 66           |
|    total_timesteps      | 1081344      |
| train/                  |              |
|    approx_kl            | 0.0048722345 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0433      |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.0981       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1089280, episode_reward=0.81 +/- 0.40
Episode length: 1.96 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 67 completed in 183.04 seconds.
Time to collect_rollouts 183.05
Training model
Time to train 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.97         |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 67           |
|    total_timesteps      | 1097728      |
| train/                  |              |
|    approx_kl            | 0.0075117727 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.439        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0451      |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.0995       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1105536, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 68 completed in 186.92 seconds.
Time to collect_rollouts 186.93
Training model
Time to train 50.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.63         |
|    ep_rew_mean          | 0.35         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 68           |
|    total_timesteps      | 1114112      |
| train/                  |              |
|    approx_kl            | 0.0055721533 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0502      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 0.1          |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1121792, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 69 completed in 180.48 seconds.
Time to collect_rollouts 180.49
Training model
Time to train 50.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53        |
|    ep_rew_mean          | 0.34        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 69          |
|    total_timesteps      | 1130496     |
| train/                  |             |
|    approx_kl            | 0.005034794 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0964      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1138048, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 70 completed in 186.18 seconds.
Time to collect_rollouts 186.18
Training model
Time to train 50.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 70          |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.013243213 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.0908      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1154304, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 71 completed in 193.99 seconds.
Time to collect_rollouts 194.0
Training model
Time to train 49.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.1         |
|    ep_rew_mean          | 0.46        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 71          |
|    total_timesteps      | 1163264     |
| train/                  |             |
|    approx_kl            | 0.005743658 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0506     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.0943      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1170560, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 72 completed in 190.22 seconds.
Time to collect_rollouts 190.22
Training model
Time to train 50.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.07         |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 72           |
|    total_timesteps      | 1179648      |
| train/                  |              |
|    approx_kl            | 0.0056703137 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.611       |
|    explained_variance   | 0.448        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0238      |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.0964       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1186816, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 73 completed in 181.86 seconds.
Time to collect_rollouts 181.86
Training model
Time to train 50.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.35         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 73           |
|    total_timesteps      | 1196032      |
| train/                  |              |
|    approx_kl            | 0.0049897786 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.482        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0566      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.0917       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1203072, episode_reward=0.81 +/- 0.40
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 74 completed in 185.90 seconds.
Time to collect_rollouts 185.91
Training model
Time to train 50.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.42         |
|    ep_rew_mean          | 0.41         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 74           |
|    total_timesteps      | 1212416      |
| train/                  |              |
|    approx_kl            | 0.0057941624 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.492        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0432      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.0905       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1219328, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 75 completed in 186.84 seconds.
Time to collect_rollouts 186.85
Training model
Time to train 50.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.91        |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 75          |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.005289385 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0146     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0894      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1235584, episode_reward=0.81 +/- 0.40
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 76 completed in 179.77 seconds.
Time to collect_rollouts 179.78
Training model
Time to train 49.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.09        |
|    ep_rew_mean          | 0.43        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 76          |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.004978867 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.0883      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1251840, episode_reward=0.81 +/- 0.40
Episode length: 1.97 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 77 completed in 184.03 seconds.
Time to collect_rollouts 184.04
Training model
Time to train 50.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.72         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 77           |
|    total_timesteps      | 1261568      |
| train/                  |              |
|    approx_kl            | 0.0056062494 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.494        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0272      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.0868       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1268096, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 78 completed in 181.61 seconds.
Time to collect_rollouts 181.62
Training model
Time to train 50.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.31         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 78           |
|    total_timesteps      | 1277952      |
| train/                  |              |
|    approx_kl            | 0.0054121986 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.587       |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0335      |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.0922       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1284352, episode_reward=0.81 +/- 0.40
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 79 completed in 185.06 seconds.
Time to collect_rollouts 185.07
Training model
Time to train 50.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.3          |
|    ep_rew_mean          | 0.46         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 79           |
|    total_timesteps      | 1294336      |
| train/                  |              |
|    approx_kl            | 0.0054601473 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.525        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0513      |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.0849       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1300608, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 80 completed in 186.10 seconds.
Time to collect_rollouts 186.11
Training model
Time to train 49.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.89         |
|    ep_rew_mean          | 0.35         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 80           |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0056350864 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.513        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.032       |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 0.0865       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1316864, episode_reward=0.81 +/- 0.39
Episode length: 1.97 +/- 0.40
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 81 completed in 184.31 seconds.
Improved rollout/ep_rew_mean to 0.4900
Time to collect_rollouts 184.4
Training model
Time to train 49.83
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.51         |
|    ep_rew_mean          | 0.49         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 81           |
|    total_timesteps      | 1327104      |
| train/                  |              |
|    approx_kl            | 0.0057488093 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.587       |
|    explained_variance   | 0.522        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0285      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.0828       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1333120, episode_reward=0.81 +/- 0.39
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 82 completed in 186.52 seconds.
Time to collect_rollouts 186.52
Training model
Time to train 50.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96         |
|    ep_rew_mean          | 0.49         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 82           |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0050927796 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0484      |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.0834       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1349376, episode_reward=0.81 +/- 0.39
Episode length: 1.97 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 83 completed in 188.11 seconds.
Time to collect_rollouts 188.11
Training model
Time to train 50.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73         |
|    ep_rew_mean          | 0.35         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 83           |
|    total_timesteps      | 1359872      |
| train/                  |              |
|    approx_kl            | 0.0054382305 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0159      |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.0806       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1365632, episode_reward=0.81 +/- 0.40
Episode length: 1.98 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 84 completed in 179.38 seconds.
Time to collect_rollouts 179.39
Training model
Time to train 50.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47        |
|    ep_rew_mean          | 0.4         |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 84          |
|    total_timesteps      | 1376256     |
| train/                  |             |
|    approx_kl            | 0.005420978 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0386     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.08        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1381888, episode_reward=0.81 +/- 0.39
Episode length: 1.98 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 85 completed in 185.47 seconds.
Time to collect_rollouts 185.48
Training model
Time to train 50.33
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.94         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 85           |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0053263856 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.51         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0341      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.0848       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1398144, episode_reward=0.80 +/- 0.40
Episode length: 1.98 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 86 completed in 189.49 seconds.
Time to collect_rollouts 189.5
Training model
Time to train 50.55
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.93         |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 86           |
|    total_timesteps      | 1409024      |
| train/                  |              |
|    approx_kl            | 0.0054463972 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0443      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.0785       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1414400, episode_reward=0.80 +/- 0.40
Episode length: 1.98 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 87 completed in 191.75 seconds.
Time to collect_rollouts 191.76
Training model
