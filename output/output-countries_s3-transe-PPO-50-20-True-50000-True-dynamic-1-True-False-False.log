Experiment number  0  out of  1  experiments.
Seed 0  in  [0]

Run vars: countries_s3-transe-PPO-50-20-True-50000-True-dynamic-1-True-False-False 
 Namespace(train_neg_pos_ratio=1, limit_space=True, corruption_mode='dynamic', dataset_name='countries_s3', data_path='./data/', domain_file=None, janus_file='train.pl', train_file='train_label.txt', valid_file='valid_label.txt', test_file='test_label.txt', dynamic_consult=False, learn_embeddings=True, kge='transe', model_name='PPO', atom_embedding_size=50, constant_embedding_size=50, predicate_embedding_size=50, constant_emb_file='./data/countries_s3/constant_embeddings.pkl', predicate_emb_file='./data/countries_s3/predicate_embeddings.pkl', rule_depend_var=False, variable_no=500, device='cpu', seed=[0], restore_best_val_model=True, load_model=False, save_model=True, models_path='models/countries_s3', timesteps_train=50000, n_epochs=10, n_steps=2048, eval_freq=5000, batch_size=64, lr=0.0003, max_depth=20, run_signature='countries_s3-transe-PPO-50-20-True-50000-True-dynamic-1-True-False-False', seed_run_i=0) 

countries_s3-transe-PPO-50-20-True-50000-True-dynamic-1-True-False-False
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Epoch 1 completed in 2.92 seconds.
Metric "train/loss" not found. Available: dict_keys([])
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12       |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 696      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
Epoch 2 completed in 2.99 seconds.
Improved train/loss to 0.0241
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 12.1       |
|    ep_rew_mean          | 0.37       |
| time/                   |            |
|    fps                  | 445        |
|    iterations           | 2          |
|    time_elapsed         | 9          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02108679 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.442     |
|    explained_variance   | -0.24      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0241     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.0977     |
----------------------------------------
Eval num_timesteps=5000, episode_reward=0.79 +/- 0.41
Episode length: 8.04 +/- 5.70
New best mean reward!
Epoch 3 completed in 3.86 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 368         |
|    iterations           | 3           |
|    time_elapsed         | 16          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.013032214 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.0795      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0387      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.0886      |
-----------------------------------------
Epoch 4 completed in 3.14 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.3        |
|    ep_rew_mean          | 0.4         |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 4           |
|    time_elapsed         | 23          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014712926 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0442      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0903      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=0.79 +/- 0.41
Episode length: 7.04 +/- 5.48
Epoch 5 completed in 3.43 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | 0.44        |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 5           |
|    time_elapsed         | 30          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.014348602 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.368      |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0385      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0977      |
-----------------------------------------
Epoch 6 completed in 3.25 seconds.
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 10.9       |
|    ep_rew_mean          | 0.47       |
| time/                   |            |
|    fps                  | 333        |
|    iterations           | 6          |
|    time_elapsed         | 36         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01804584 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.182      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0303     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.105      |
----------------------------------------
Epoch 7 completed in 3.31 seconds.
Improved train/loss to 0.0088
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.9        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 7           |
|    time_elapsed         | 43          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015968826 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00878     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0921      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=0.92 +/- 0.28
Episode length: 5.67 +/- 3.76
New best mean reward!
Epoch 8 completed in 3.58 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 325         |
|    iterations           | 8           |
|    time_elapsed         | 50          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013167864 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0261      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0839      |
-----------------------------------------
Epoch 9 completed in 3.47 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.7        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 9           |
|    time_elapsed         | 56          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011425529 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0371      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0892      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=0.88 +/- 0.33
Episode length: 5.38 +/- 3.49
Epoch 10 completed in 3.54 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 10          |
|    time_elapsed         | 62          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009454292 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0103      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 0.0747      |
-----------------------------------------
Epoch 11 completed in 3.71 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10          |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 11          |
|    time_elapsed         | 68          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011485612 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0124      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 0.0665      |
-----------------------------------------
Epoch 12 completed in 3.70 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.3        |
|    ep_rew_mean          | 0.46        |
| time/                   |             |
|    fps                  | 327         |
|    iterations           | 12          |
|    time_elapsed         | 74          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010564928 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0141      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 0.0516      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=0.88 +/- 0.33
Episode length: 5.29 +/- 3.45
Epoch 13 completed in 3.83 seconds.
Improved train/loss to 0.0085
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.69        |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 324         |
|    iterations           | 13          |
|    time_elapsed         | 82          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.009969096 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00855     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00992    |
|    value_loss           | 0.0477      |
-----------------------------------------
Epoch 14 completed in 4.91 seconds.
Improved train/loss to -0.0174
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.9        |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 314         |
|    iterations           | 14          |
|    time_elapsed         | 91          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010972892 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0316      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=0.96 +/- 0.20
Episode length: 5.46 +/- 3.49
New best mean reward!
Epoch 15 completed in 3.85 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.2        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 15          |
|    time_elapsed         | 98          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.009372354 |
|    clip_fraction        | 0.0685      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00626     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 0.0236      |
-----------------------------------------
Epoch 16 completed in 3.75 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 310         |
|    iterations           | 16          |
|    time_elapsed         | 105         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011526337 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00866     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00945    |
|    value_loss           | 0.0226      |
-----------------------------------------
Epoch 17 completed in 3.87 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 17          |
|    time_elapsed         | 112         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.010188231 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00566     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 0.0133      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=0.96 +/- 0.20
Episode length: 6.04 +/- 4.88
Epoch 18 completed in 4.16 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.2        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 18          |
|    time_elapsed         | 120         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.008311754 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 0.00885     |
-----------------------------------------
Epoch 19 completed in 3.97 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.1        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 303         |
|    iterations           | 19          |
|    time_elapsed         | 128         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.009022191 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00615     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 0.00961     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=0.96 +/- 0.20
Episode length: 5.46 +/- 3.81
Epoch 20 completed in 4.06 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 20          |
|    time_elapsed         | 135         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.008725084 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00117     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 0.00883     |
-----------------------------------------
Epoch 21 completed in 5.47 seconds.
Improved train/loss to -0.0189
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 11.5       |
|    ep_rew_mean          | 0.49       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 21         |
|    time_elapsed         | 144        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.02475246 |
|    clip_fraction        | 0.0721     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0189    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00731   |
|    value_loss           | 0.00768    |
----------------------------------------
Eval num_timesteps=45000, episode_reward=0.96 +/- 0.20
Episode length: 5.79 +/- 4.27
Epoch 22 completed in 4.57 seconds.
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 11.6       |
|    ep_rew_mean          | 0.49       |
| time/                   |            |
|    fps                  | 295        |
|    iterations           | 22         |
|    time_elapsed         | 152        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.01538774 |
|    clip_fraction        | 0.0745     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00543    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00815   |
|    value_loss           | 0.00883    |
----------------------------------------
Epoch 23 completed in 5.28 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.48        |
|    ep_rew_mean          | 0.5         |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 23          |
|    time_elapsed         | 161         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.008624658 |
|    clip_fraction        | 0.0631      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00235    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 0.00575     |
-----------------------------------------
Epoch 24 completed in 4.47 seconds.
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.1        |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 24          |
|    time_elapsed         | 170         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.017306486 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0409      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 0.00564     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=0.96 +/- 0.20
Episode length: 5.04 +/- 3.96
Epoch 25 completed in 4.53 seconds.
Improved train/loss to -0.0202
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 25          |
|    time_elapsed         | 178         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.012591683 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 0.00835     |
-----------------------------------------

Training completed!
Total training time: 97.62 seconds.
Average epoch time: 3.90 seconds.
Restored best model from step 30000, with best_mean_reward=0.958.
