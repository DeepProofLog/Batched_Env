(rl) castellanoontiv@MSI:~/Batched_env$ /home/castellanoontiv/miniconda3/envs/rl/bin/python /home/castellanoontiv/Batched_env/runner.py
Using device: cuda


============================================================
Experiment 1/1
============================================================


============================================================
Seed 0 in [0]
============================================================

Run vars: countries_s3-250-True-0.5-0.2-4-4-5-3e-05-128-torchrl 
 {'atom_embedder': 'transe', 'atom_embedding_size': 250, 'batch_size': 8192, 'batch_size_env': 128, 'batch_size_env_eval': 128, 'canonical_action_order': False, 'clip_range': 0.2, 'clip_range_vf': 0.5, 'constant_embedding_size': 250, 'corruption_mode': 'dynamic', 'corruption_scheme': ['tail'], 'data_path': './data/', 'dataset_name': 'countries_s3', 'debug_ppo': False, 'depth_info': True, 'deterministic': False, 'device': 'cuda', 'end_proof_action': True, 'ent_coef': 0.5, 'ent_coef_decay': True, 'ent_coef_end': 1.0, 'ent_coef_final_value': 0.01, 'ent_coef_init_value': 0.5, 'ent_coef_start': 0.0, 'ent_coef_transform': 'linear', 'eval_best_metric': 'mrr', 'eval_freq': 1024, 'eval_neg_samples': None, 'extended_eval_info': True, 'facts_file': 'train.txt', 'gae_lambda': 0.95, 'gamma': 0.99, 'learn_embeddings': True, 'load_depth_info': True, 'load_model': False, 'logger_path': './runs/', 'lr': 3e-05, 'lr_decay': True, 'lr_end': 1.0, 'lr_final_value': 1e-06, 'lr_init_value': 3e-05, 'lr_start': 0.0, 'lr_transform': 'linear', 'max_depth': 20, 'max_grad_norm': 0.5, 'max_total_vars': 100, 'memory_pruning': True, 'min_gpu_memory_gb': 2.0, 'model_name': 'PPO', 'models_path': 'models/', 'n_epochs': 5, 'n_eval_queries': None, 'n_steps': 256, 'n_test_queries': None, 'n_train_queries': None, 'padding_atoms': 6, 'padding_states': 20, 'plot': False, 'plot_trajectories': False, 'predicate_embedding_size': 250, 'prover_verbose': False, 'restore_best_val_model': True, 'reward_type': 4, 'rollout_device': None, 'rules_file': 'rules.txt', 'run_signature': 'countries_s3-250-True-0.5-0.2-4-4-5-3e-05-128-torchrl', 'sample_deterministic_per_env': False, 'save_model': True, 'seed': [0], 'seed_run_i': 0, 'skip_unary_actions': True, 'sqrt_scale': True, 'state_embedder': 'mean', 'state_embedding_size': 250, 'target_kl': 0.07, 'temperature': 1.0, 'test_depth': None, 'test_file': 'test.txt', 'test_neg_samples': None, 'timesteps_train': 7000000, 'train_depth': {1, 2, 3, 4, 5, 6}, 'train_file': 'train_depths.txt', 'train_neg_ratio': 4, 'use_amp': True, 'use_compile': True, 'use_exact_memory': False, 'use_l2_norm': False, 'use_logger': True, 'use_wb': False, 'valid_depth': None, 'valid_file': 'valid.txt', 'verbose': False, 'verbose_cb': False, 'verbose_env': 0, 'verbose_prover': 0, 'vf_coef': 2.0, 'wb_path': './../wandb/'} 

Device: cuda. CUDA available: True, Device count: 1
Queries loaded - Train: 33/33, Valid: 24, Test: 24
[Create Environments] Predicate Mapping: True=5, False=6, Endf=7
LR Decay: 3e-05 -> 1e-06
Entropy Decay: 0.5 -> 0.01
[PPO] AMP enabled with bfloat16 (no GradScaler required)
[PPO] Device sync optimization: _same_device=True (self.device=cuda:0, env.device=cuda:0)
[PPO] Compiled self.policy with mode='reduce-overhead', fullgraph=True

============================================================
Initial evaluation (untrained model)
============================================================
DEBUG: eval_corruptions called with n_corruptions=10, corruption_modes=('tail',)
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
Initial MRR: 0.8375
Initial Hits@1: 0.7500
Initial success_rate: 0.8750
============================================================

[Annealing] Set lr to 3e-05 (progress=0.000)
[Annealing] Set ent_coef to 0.5 (progress=0.000)
[Annealing] Set lr to 3e-05 (progress=0.000)
[Annealing] Set ent_coef to 0.5 (progress=0.000)

[PPO] Starting training for 7000000 timesteps
[PPO] Rollout size: 256 steps x 128 envs = 32768 samples per rollout
[Annealing] Set lr to 3e-05 (progress=0.000)
[Annealing] Set ent_coef to 0.5 (progress=0.000)

[PPO] ===== Iteration 1 (0/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.92s
[PPO] Recent episodes: reward=0.400, length=10.4
[PPO] Rollout collected in 22.92s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.693                    |
|    ep_rew_mean          | 0.229                    |
|    fps                  | 1429                     |
|    len                  | 8.693 +/- 7.78 (3664)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (286)     |
|    len_d_2_pos          | 6.796 +/- 5.45 (382)     |
|    len_d_3_pos          | 14.725 +/- 5.09 (69)     |
|    len_d_4_pos          | 13.400 +/- 6.07 (50)     |
|    len_d_unknown_neg    | 9.384 +/- 8.08 (2877)    |
|    len_neg              | 9.384 +/- 8.08 (2877)    |
|    len_pos              | 6.168 +/- 5.94 (787)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (286)     |
|    proven_d_2_pos       | 0.809 +/- 0.39 (382)     |
|    proven_d_3_pos       | 0.580 +/- 0.49 (69)      |
|    proven_d_4_pos       | 0.980 +/- 0.14 (50)      |
|    proven_d_unknown_neg | 0.128 +/- 0.33 (2877)    |
|    proven_neg           | 0.128 +/- 0.33 (2877)    |
|    proven_pos           | 0.869 +/- 0.34 (787)     |
|    reward               | 0.229 +/- 0.55 (3664)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (286)     |
|    reward_d_2_pos       | 0.618 +/- 0.79 (382)     |
|    reward_d_3_pos       | 0.159 +/- 0.99 (69)      |
|    reward_d_4_pos       | 0.960 +/- 0.28 (50)      |
|    reward_d_unknown_neg | 0.090 +/- 0.42 (2877)    |
|    reward_neg           | 0.090 +/- 0.42 (2877)    |
|    reward_pos           | 0.738 +/- 0.67 (787)     |
|    success_rate         | 0.287                    |
|    total_timesteps      | 32768                    |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 4.12783, policy 0.00409, value 2.27723, entropy -0.03895, approx_kl 0.00450 clip_fraction 0.01999. 
Epoch 2/5. 
Losses: total 3.65336, policy 0.00540, value 2.08919, entropy -0.03980, approx_kl 0.01207 clip_fraction 0.02867. 
Epoch 3/5. 
Losses: total 3.50971, policy 0.00563, value 1.99193, entropy -0.04034, approx_kl 0.01331 clip_fraction 0.03117. 
Epoch 4/5. 
Losses: total 3.40128, policy 0.00553, value 1.92273, entropy -0.04099, approx_kl 0.01618 clip_fraction 0.03345. 
Epoch 5/5. 
Losses: total 3.19088, policy 0.00549, value 1.87218, entropy -0.04186, approx_kl 0.02312 clip_fraction 0.03604. 
[PPO] Values: min=-3.922, max=6.250, mean=0.578, std=1.586
[PPO] Returns: min=-2.255, max=4.303, mean=0.250, std=0.508
[PPO] Explained variance: -8.7037
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.023                    |
|    clip_fraction        | 0.036                    |
|    entropy              | 0.042                    |
|    explained_var        | -8.704                   |
|    iterations           | 1                        |
|    policy_loss          | 0.005                    |
|    total_timesteps      | 32768                    |
|    value_loss           | 1.872                    |
----------------------------------------------------

[PPO] Training completed in 8.67s
[PPO] Metrics: policy_loss=0.0055, value_loss=1.8722, entropy=0.0419
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 1, step 32768. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
/home/castellanoontiv/Batched_env/model_eval.py:644: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)
  return _format_stat_string(t.mean().item(), t.std().item(), t.numel())
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.844                    |
|    ep_len_mean          | 15.142                   |
|    ep_rew_mean          | 0.817                    |
|    hits1                | 0.750                    |
|    hits10               | 1.000                    |
|    hits3                | 0.875                    |
|    len                  | 15.142 +/- 6.62 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.250 +/- 3.32 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.385 +/- 4.82 (96)     |
|    len_neg              | 17.385 +/- 4.82 (96)     |
|    len_pos              | 6.167 +/- 5.05 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.073 +/- 0.26 (96)      |
|    proven_neg           | 0.073 +/- 0.26 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.817 +/- 0.58 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.854 +/- 0.52 (96)      |
|    reward_neg           | 0.854 +/- 0.52 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.225                    |
|    total_timesteps      | 32768                    |
----------------------------------------------------

[MRR] New best: 0.8438 at iteration 1
[MRR] MRR: current=0.844, best=0.844 (iter 1), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 875                      |
|    iterations           | 1                        |
|    total_timesteps      | 32768                    |
----------------------------------------------------

[Annealing] Set lr to 2.986424685714286e-05 (progress=0.005)
[Annealing] Set ent_coef to 0.49770624 (progress=0.005)

[PPO] ===== Iteration 2 (32768/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.88s
[PPO] Recent episodes: reward=0.325, length=5.8
[PPO] Rollout collected in 22.88s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.132                    |
|    ep_rew_mean          | 0.217                    |
|    fps                  | 1431                     |
|    len                  | 9.132 +/- 7.87 (3575)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (309)     |
|    len_d_2_pos          | 7.036 +/- 5.64 (306)     |
|    len_d_3_pos          | 15.149 +/- 4.96 (67)     |
|    len_d_4_pos          | 11.944 +/- 6.32 (36)     |
|    len_d_unknown_neg    | 9.951 +/- 8.09 (2857)    |
|    len_neg              | 9.951 +/- 8.09 (2857)    |
|    len_pos              | 5.872 +/- 5.89 (718)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (309)     |
|    proven_d_2_pos       | 0.788 +/- 0.41 (306)     |
|    proven_d_3_pos       | 0.478 +/- 0.50 (67)      |
|    proven_d_4_pos       | 0.917 +/- 0.28 (36)      |
|    proven_d_unknown_neg | 0.126 +/- 0.33 (2857)    |
|    proven_neg           | 0.126 +/- 0.33 (2857)    |
|    proven_pos           | 0.857 +/- 0.35 (718)     |
|    reward               | 0.217 +/- 0.55 (3575)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (309)     |
|    reward_d_2_pos       | 0.575 +/- 0.82 (306)     |
|    reward_d_3_pos       | -0.045 +/- 1.00 (67)     |
|    reward_d_4_pos       | 0.833 +/- 0.55 (36)      |
|    reward_d_unknown_neg | 0.092 +/- 0.41 (2857)    |
|    reward_neg           | 0.092 +/- 0.41 (2857)    |
|    reward_pos           | 0.713 +/- 0.70 (718)     |
|    success_rate         | 0.273                    |
|    total_timesteps      | 65536                    |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 2.18506, policy 0.00289, value 1.18092, entropy -0.04954, approx_kl 0.00511 clip_fraction 0.02441. 
Epoch 2/5. 
Losses: total 1.82075, policy 0.00431, value 1.07911, entropy -0.04961, approx_kl 0.01586 clip_fraction 0.03510. 
Epoch 3/5. 
Losses: total 1.64988, policy 0.00483, value 1.00571, entropy -0.04966, approx_kl 0.02418 clip_fraction 0.03858. 
Epoch 4/5. 
Losses: total 1.61341, policy 0.00508, value 0.95730, entropy -0.05047, approx_kl 0.02410 clip_fraction 0.04137. 
Epoch 5/5. 
Losses: total 1.49426, policy 0.00526, value 0.91892, entropy -0.05234, approx_kl 0.02210 clip_fraction 0.04357. 
[PPO] Values: min=-3.094, max=4.875, mean=0.456, std=1.079
[PPO] Returns: min=-1.748, max=3.243, mean=0.207, std=0.438
[PPO] Explained variance: -5.2011
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.022                    |
|    clip_fraction        | 0.044                    |
|    entropy              | 0.052                    |
|    explained_var        | -5.201                   |
|    iterations           | 2                        |
|    policy_loss          | 0.005                    |
|    total_timesteps      | 65536                    |
|    value_loss           | 0.919                    |
----------------------------------------------------

[PPO] Training completed in 9.37s
[PPO] Metrics: policy_loss=0.0053, value_loss=0.9189, entropy=0.0523
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 2, step 65536. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
/home/castellanoontiv/Batched_env/model_eval.py:644: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)
  return _format_stat_string(t.mean().item(), t.std().item(), t.numel())
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.826                    |
|    ep_len_mean          | 15.183                   |
|    ep_rew_mean          | 0.817                    |
|    hits1                | 0.708                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 15.183 +/- 6.64 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.083 +/- 3.28 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.458 +/- 4.79 (96)     |
|    len_neg              | 17.458 +/- 4.79 (96)     |
|    len_pos              | 6.083 +/- 5.03 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.073 +/- 0.26 (96)      |
|    proven_neg           | 0.073 +/- 0.26 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.817 +/- 0.58 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.854 +/- 0.52 (96)      |
|    reward_neg           | 0.854 +/- 0.52 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.225                    |
|    total_timesteps      | 65536                    |
----------------------------------------------------

[MRR] MRR: current=0.826, best=0.844 (iter 1), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 875                      |
|    iterations           | 2                        |
|    total_timesteps      | 65536                    |
----------------------------------------------------

[Annealing] Set lr to 2.9728493714285716e-05 (progress=0.009)
[Annealing] Set ent_coef to 0.49541248 (progress=0.009)

[PPO] ===== Iteration 3 (65536/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.46s
[PPO] Recent episodes: reward=0.125, length=7.8
[PPO] Rollout collected in 21.46s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.171                    |
|    ep_rew_mean          | 0.224                    |
|    fps                  | 1526                     |
|    len                  | 9.171 +/- 7.86 (3585)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (267)     |
|    len_d_2_pos          | 6.965 +/- 5.13 (346)     |
|    len_d_3_pos          | 13.051 +/- 5.13 (59)     |
|    len_d_4_pos          | 11.067 +/- 6.23 (45)     |
|    len_d_unknown_neg    | 9.995 +/- 8.15 (2868)    |
|    len_neg              | 9.995 +/- 8.15 (2868)    |
|    len_pos              | 5.874 +/- 5.42 (717)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (267)     |
|    proven_d_2_pos       | 0.789 +/- 0.41 (346)     |
|    proven_d_3_pos       | 0.627 +/- 0.48 (59)      |
|    proven_d_4_pos       | 0.956 +/- 0.21 (45)      |
|    proven_d_unknown_neg | 0.122 +/- 0.33 (2868)    |
|    proven_neg           | 0.122 +/- 0.33 (2868)    |
|    proven_pos           | 0.865 +/- 0.34 (717)     |
|    reward               | 0.224 +/- 0.54 (3585)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (267)     |
|    reward_d_2_pos       | 0.578 +/- 0.82 (346)     |
|    reward_d_3_pos       | 0.254 +/- 0.97 (59)      |
|    reward_d_4_pos       | 0.911 +/- 0.41 (45)      |
|    reward_d_unknown_neg | 0.097 +/- 0.41 (2868)    |
|    reward_neg           | 0.097 +/- 0.41 (2868)    |
|    reward_pos           | 0.729 +/- 0.68 (717)     |
|    success_rate         | 0.271                    |
|    total_timesteps      | 98304                    |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 1.01954, policy 0.00238, value 0.54107, entropy -0.05797, approx_kl 0.00552 clip_fraction 0.03311. 
Epoch 2/5. 
Losses: total 0.84919, policy 0.00312, value 0.50675, entropy -0.05861, approx_kl 0.01042 clip_fraction 0.04008. 
Epoch 3/5. 
Losses: total 0.73765, policy 0.00343, value 0.46852, entropy -0.05966, approx_kl 0.01445 clip_fraction 0.04495. 
Epoch 4/5. 
Losses: total 0.66533, policy 0.00352, value 0.44058, entropy -0.06045, approx_kl 0.01773 clip_fraction 0.04799. 
Epoch 5/5. 
Losses: total 0.59882, policy 0.00339, value 0.41786, entropy -0.06086, approx_kl 0.02024 clip_fraction 0.04901. 
[PPO] Values: min=-2.812, max=4.156, mean=0.271, std=0.693
[PPO] Returns: min=-1.853, max=2.712, mean=0.173, std=0.402
[PPO] Explained variance: -2.3713
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.020                    |
|    clip_fraction        | 0.049                    |
|    entropy              | 0.061                    |
|    explained_var        | -2.371                   |
|    iterations           | 3                        |
|    policy_loss          | 0.003                    |
|    total_timesteps      | 98304                    |
|    value_loss           | 0.418                    |
----------------------------------------------------

[PPO] Training completed in 7.23s
[PPO] Metrics: policy_loss=0.0034, value_loss=0.4179, entropy=0.0609
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 3, step 98304. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.819                    |
|    ep_len_mean          | 15.000                   |
|    ep_rew_mean          | 0.850                    |
|    hits1                | 0.708                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 15.000 +/- 6.65 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.083 +/- 3.28 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.229 +/- 4.91 (96)     |
|    len_neg              | 17.229 +/- 4.91 (96)     |
|    len_pos              | 6.083 +/- 5.03 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.052 +/- 0.22 (96)      |
|    proven_neg           | 0.052 +/- 0.22 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.850 +/- 0.53 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.896 +/- 0.44 (96)      |
|    reward_neg           | 0.896 +/- 0.44 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.208                    |
|    total_timesteps      | 98304                    |
----------------------------------------------------

[MRR] MRR: current=0.819, best=0.844 (iter 1), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 905                      |
|    iterations           | 3                        |
|    total_timesteps      | 98304                    |
----------------------------------------------------

[Annealing] Set lr to 2.9592740571428574e-05 (progress=0.014)
[Annealing] Set ent_coef to 0.49311872 (progress=0.014)

[PPO] ===== Iteration 4 (98304/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.95s
[PPO] Recent episodes: reward=-0.100, length=9.3
[PPO] Rollout collected in 20.95s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.019                    |
|    ep_rew_mean          | 0.237                    |
|    fps                  | 1564                     |
|    len                  | 9.019 +/- 7.84 (3640)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (292)     |
|    len_d_2_pos          | 6.426 +/- 4.66 (345)     |
|    len_d_3_pos          | 12.344 +/- 5.26 (64)     |
|    len_d_4_pos          | 13.286 +/- 5.86 (28)     |
|    len_d_unknown_neg    | 9.916 +/- 8.15 (2911)    |
|    len_neg              | 9.916 +/- 8.15 (2911)    |
|    len_pos              | 5.436 +/- 5.08 (729)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (292)     |
|    proven_d_2_pos       | 0.855 +/- 0.35 (345)     |
|    proven_d_3_pos       | 0.641 +/- 0.48 (64)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (28)      |
|    proven_d_unknown_neg | 0.123 +/- 0.33 (2911)    |
|    proven_neg           | 0.123 +/- 0.33 (2911)    |
|    proven_pos           | 0.900 +/- 0.30 (729)     |
|    reward               | 0.237 +/- 0.53 (3640)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (292)     |
|    reward_d_2_pos       | 0.710 +/- 0.70 (345)     |
|    reward_d_3_pos       | 0.281 +/- 0.96 (64)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (28)      |
|    reward_d_unknown_neg | 0.097 +/- 0.41 (2911)    |
|    reward_neg           | 0.097 +/- 0.41 (2911)    |
|    reward_pos           | 0.800 +/- 0.60 (729)     |
|    success_rate         | 0.278                    |
|    total_timesteps      | 131072                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.54868, policy 0.00295, value 0.31073, entropy -0.06542, approx_kl 0.00698 clip_fraction 0.04211. 
Epoch 2/5. 
Losses: total 0.49681, policy 0.00363, value 0.29403, entropy -0.06607, approx_kl 0.01396 clip_fraction 0.04701. 
Epoch 3/5. 
Losses: total 0.43839, policy 0.00434, value 0.27700, entropy -0.06754, approx_kl 0.01921 clip_fraction 0.05136. 
Epoch 4/5. 
Losses: total 0.39828, policy 0.00455, value 0.26315, entropy -0.06893, approx_kl 0.01735 clip_fraction 0.05414. 
Epoch 5/5. 
Losses: total 0.36520, policy 0.00444, value 0.25095, entropy -0.06995, approx_kl 0.01795 clip_fraction 0.05658. 
[PPO] Values: min=-2.594, max=3.594, mean=0.217, std=0.522
[PPO] Returns: min=-1.024, max=1.452, mean=0.169, std=0.384
[PPO] Explained variance: -1.1977
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.018                    |
|    clip_fraction        | 0.057                    |
|    entropy              | 0.070                    |
|    explained_var        | -1.198                   |
|    iterations           | 4                        |
|    policy_loss          | 0.004                    |
|    total_timesteps      | 131072                   |
|    value_loss           | 0.251                    |
----------------------------------------------------

[PPO] Training completed in 7.25s
[PPO] Metrics: policy_loss=0.0044, value_loss=0.2510, entropy=0.0699
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 4, step 131072. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.847                    |
|    ep_len_mean          | 15.017                   |
|    ep_rew_mean          | 0.850                    |
|    hits1                | 0.750                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 15.017 +/- 6.61 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.083 +/- 3.28 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.250 +/- 4.84 (96)     |
|    len_neg              | 17.250 +/- 4.84 (96)     |
|    len_pos              | 6.083 +/- 5.03 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.052 +/- 0.22 (96)      |
|    proven_neg           | 0.052 +/- 0.22 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.850 +/- 0.53 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.896 +/- 0.44 (96)      |
|    reward_neg           | 0.896 +/- 0.44 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.208                    |
|    total_timesteps      | 131072                   |
----------------------------------------------------

[MRR] New best: 0.8472 at iteration 4
[MRR] MRR: current=0.847, best=0.847 (iter 4), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 927                      |
|    iterations           | 4                        |
|    total_timesteps      | 131072                   |
----------------------------------------------------

[Annealing] Set lr to 2.9456987428571428e-05 (progress=0.019)
[Annealing] Set ent_coef to 0.49082496 (progress=0.019)

[PPO] ===== Iteration 5 (131072/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.52s
[PPO] Recent episodes: reward=0.250, length=14.0
[PPO] Rollout collected in 20.52s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.116                    |
|    ep_rew_mean          | 0.244                    |
|    fps                  | 1597                     |
|    len                  | 9.116 +/- 7.85 (3602)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (282)     |
|    len_d_2_pos          | 6.138 +/- 3.92 (320)     |
|    len_d_3_pos          | 12.919 +/- 5.43 (62)     |
|    len_d_4_pos          | 12.528 +/- 6.19 (53)     |
|    len_d_unknown_neg    | 9.998 +/- 8.16 (2885)    |
|    len_neg              | 9.998 +/- 8.16 (2885)    |
|    len_pos              | 5.569 +/- 5.06 (717)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (282)     |
|    proven_d_2_pos       | 0.844 +/- 0.36 (320)     |
|    proven_d_3_pos       | 0.661 +/- 0.47 (62)      |
|    proven_d_4_pos       | 0.906 +/- 0.29 (53)      |
|    proven_d_unknown_neg | 0.113 +/- 0.32 (2885)    |
|    proven_neg           | 0.113 +/- 0.32 (2885)    |
|    proven_pos           | 0.894 +/- 0.31 (717)     |
|    reward               | 0.244 +/- 0.52 (3602)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (282)     |
|    reward_d_2_pos       | 0.688 +/- 0.73 (320)     |
|    reward_d_3_pos       | 0.323 +/- 0.95 (62)      |
|    reward_d_4_pos       | 0.811 +/- 0.58 (53)      |
|    reward_d_unknown_neg | 0.109 +/- 0.40 (2885)    |
|    reward_neg           | 0.109 +/- 0.40 (2885)    |
|    reward_pos           | 0.788 +/- 0.62 (717)     |
|    success_rate         | 0.268                    |
|    total_timesteps      | 163840                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.28975, policy 0.00279, value 0.16606, entropy -0.07255, approx_kl 0.00839 clip_fraction 0.04907. 
Epoch 2/5. 
Losses: total 0.25325, policy 0.00319, value 0.15803, entropy -0.07475, approx_kl 0.01966 clip_fraction 0.05951. 
Epoch 3/5. 
Losses: total 0.22560, policy 0.00360, value 0.15082, entropy -0.07670, approx_kl 0.02421 clip_fraction 0.06527. 
Epoch 4/5. 
Losses: total 0.20664, policy 0.00378, value 0.14438, entropy -0.07766, approx_kl 0.03326 clip_fraction 0.06813. 
Epoch 5/5. 
Losses: total 0.18577, policy 0.00376, value 0.13880, entropy -0.07914, approx_kl 0.03597 clip_fraction 0.07053. 
[PPO] Values: min=-2.531, max=3.031, mean=0.193, std=0.374
[PPO] Returns: min=-1.000, max=1.165, mean=0.173, std=0.378
[PPO] Explained variance: -0.1931
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.036                    |
|    clip_fraction        | 0.071                    |
|    entropy              | 0.079                    |
|    explained_var        | -0.193                   |
|    iterations           | 5                        |
|    policy_loss          | 0.004                    |
|    total_timesteps      | 163840                   |
|    value_loss           | 0.139                    |
----------------------------------------------------

[PPO] Training completed in 7.29s
[PPO] Metrics: policy_loss=0.0038, value_loss=0.1388, entropy=0.0791
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 5, step 163840. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.881                    |
|    ep_len_mean          | 14.858                   |
|    ep_rew_mean          | 0.850                    |
|    hits1                | 0.833                    |
|    hits10               | 1.000                    |
|    hits3                | 0.875                    |
|    len                  | 14.858 +/- 6.59 (120)    |
|    len_d_1_pos          | 4.500 +/- 5.28 (10)      |
|    len_d_2_pos          | 6.500 +/- 4.48 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.990 +/- 4.87 (96)     |
|    len_neg              | 16.990 +/- 4.87 (96)     |
|    len_pos              | 6.333 +/- 5.58 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.052 +/- 0.22 (96)      |
|    proven_neg           | 0.052 +/- 0.22 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.850 +/- 0.53 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.896 +/- 0.44 (96)      |
|    reward_neg           | 0.896 +/- 0.44 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.208                    |
|    total_timesteps      | 163840                   |
----------------------------------------------------

[MRR] New best: 0.8813 at iteration 5
[MRR] MRR: current=0.881, best=0.881 (iter 5), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 946                      |
|    iterations           | 5                        |
|    total_timesteps      | 163840                   |
----------------------------------------------------

[Annealing] Set lr to 2.9321234285714285e-05 (progress=0.023)
[Annealing] Set ent_coef to 0.4885312 (progress=0.023)

[PPO] ===== Iteration 6 (163840/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.61s
[PPO] Recent episodes: reward=0.350, length=5.5
[PPO] Rollout collected in 21.61s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.799                    |
|    ep_rew_mean          | 0.255                    |
|    fps                  | 1516                     |
|    len                  | 8.799 +/- 7.84 (3707)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (294)     |
|    len_d_2_pos          | 5.439 +/- 3.43 (344)     |
|    len_d_3_pos          | 15.152 +/- 5.63 (66)     |
|    len_d_4_pos          | 11.889 +/- 6.25 (36)     |
|    len_d_unknown_neg    | 9.683 +/- 8.15 (2967)    |
|    len_neg              | 9.683 +/- 8.15 (2967)    |
|    len_pos              | 5.253 +/- 5.03 (740)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (294)     |
|    proven_d_2_pos       | 0.901 +/- 0.30 (344)     |
|    proven_d_3_pos       | 0.712 +/- 0.45 (66)      |
|    proven_d_4_pos       | 0.917 +/- 0.28 (36)      |
|    proven_d_unknown_neg | 0.115 +/- 0.32 (2967)    |
|    proven_neg           | 0.115 +/- 0.32 (2967)    |
|    proven_pos           | 0.924 +/- 0.26 (740)     |
|    reward               | 0.255 +/- 0.52 (3707)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (294)     |
|    reward_d_2_pos       | 0.802 +/- 0.60 (344)     |
|    reward_d_3_pos       | 0.424 +/- 0.91 (66)      |
|    reward_d_4_pos       | 0.833 +/- 0.55 (36)      |
|    reward_d_unknown_neg | 0.107 +/- 0.40 (2967)    |
|    reward_neg           | 0.107 +/- 0.40 (2967)    |
|    reward_pos           | 0.849 +/- 0.53 (740)     |
|    success_rate         | 0.276                    |
|    total_timesteps      | 196608                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.21088, policy 0.00190, value 0.12982, entropy -0.08515, approx_kl 0.00876 clip_fraction 0.05457. 
Epoch 2/5. 
Losses: total 0.19767, policy 0.00223, value 0.12472, entropy -0.08693, approx_kl 0.01778 clip_fraction 0.06258. 
Epoch 3/5. 
Losses: total 0.16234, policy 0.00184, value 0.12037, entropy -0.08873, approx_kl 0.02821 clip_fraction 0.06878. 
Epoch 4/5. 
Losses: total 0.15934, policy 0.00162, value 0.11646, entropy -0.09019, approx_kl 0.02960 clip_fraction 0.07288. 
Epoch 5/5. 
Losses: total 0.15332, policy 0.00160, value 0.11307, entropy -0.09144, approx_kl 0.03524 clip_fraction 0.07639. 
[PPO] Values: min=-2.422, max=2.422, mean=0.186, std=0.343
[PPO] Returns: min=-1.000, max=1.099, mean=0.186, std=0.374
[PPO] Explained variance: 0.0182
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.035                    |
|    clip_fraction        | 0.076                    |
|    entropy              | 0.091                    |
|    explained_var        | 0.018                    |
|    iterations           | 6                        |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 196608                   |
|    value_loss           | 0.113                    |
----------------------------------------------------

[PPO] Training completed in 7.20s
[PPO] Metrics: policy_loss=0.0016, value_loss=0.1131, entropy=0.0914
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 6, step 196608. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.887                    |
|    ep_len_mean          | 15.167                   |
|    ep_rew_mean          | 0.867                    |
|    hits1                | 0.833                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 15.167 +/- 6.66 (120)    |
|    len_d_1_pos          | 4.500 +/- 5.28 (10)      |
|    len_d_2_pos          | 6.667 +/- 4.50 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.354 +/- 4.87 (96)     |
|    len_neg              | 17.354 +/- 4.87 (96)     |
|    len_pos              | 6.417 +/- 5.59 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.042 +/- 0.20 (96)      |
|    proven_neg           | 0.042 +/- 0.20 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.867 +/- 0.50 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.917 +/- 0.40 (96)      |
|    reward_neg           | 0.917 +/- 0.40 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.200                    |
|    total_timesteps      | 196608                   |
----------------------------------------------------

[MRR] New best: 0.8868 at iteration 6
[MRR] MRR: current=0.887, best=0.887 (iter 6), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 954                      |
|    iterations           | 6                        |
|    total_timesteps      | 196608                   |
----------------------------------------------------

[Annealing] Set lr to 2.9185481142857143e-05 (progress=0.028)
[Annealing] Set ent_coef to 0.48623744 (progress=0.028)

[PPO] ===== Iteration 7 (196608/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.01s
[PPO] Recent episodes: reward=0.625, length=10.0
[PPO] Rollout collected in 22.01s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.958                    |
|    ep_rew_mean          | 0.270                    |
|    fps                  | 1488                     |
|    len                  | 8.958 +/- 7.92 (3668)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (302)     |
|    len_d_2_pos          | 5.393 +/- 3.44 (321)     |
|    len_d_3_pos          | 13.053 +/- 5.67 (76)     |
|    len_d_4_pos          | 12.429 +/- 6.47 (42)     |
|    len_d_unknown_neg    | 9.911 +/- 8.24 (2927)    |
|    len_neg              | 9.911 +/- 8.24 (2927)    |
|    len_pos              | 5.194 +/- 4.93 (741)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (302)     |
|    proven_d_2_pos       | 0.950 +/- 0.22 (321)     |
|    proven_d_3_pos       | 0.645 +/- 0.48 (76)      |
|    proven_d_4_pos       | 0.905 +/- 0.29 (42)      |
|    proven_d_unknown_neg | 0.107 +/- 0.31 (2927)    |
|    proven_neg           | 0.107 +/- 0.31 (2927)    |
|    proven_pos           | 0.937 +/- 0.24 (741)     |
|    reward               | 0.270 +/- 0.51 (3668)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (302)     |
|    reward_d_2_pos       | 0.900 +/- 0.44 (321)     |
|    reward_d_3_pos       | 0.289 +/- 0.96 (76)      |
|    reward_d_4_pos       | 0.810 +/- 0.59 (42)      |
|    reward_d_unknown_neg | 0.117 +/- 0.39 (2927)    |
|    reward_neg           | 0.117 +/- 0.39 (2927)    |
|    reward_pos           | 0.873 +/- 0.49 (741)     |
|    success_rate         | 0.274                    |
|    total_timesteps      | 229376                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.18591, policy 0.00219, value 0.11536, entropy -0.09266, approx_kl 0.01031 clip_fraction 0.05792. 
Epoch 2/5. 
Losses: total 0.17953, policy 0.00305, value 0.11158, entropy -0.09344, approx_kl 0.02100 clip_fraction 0.06677. 
Epoch 3/5. 
Losses: total 0.16224, policy 0.00318, value 0.10849, entropy -0.09405, approx_kl 0.02624 clip_fraction 0.07253. 
Epoch 4/5. 
Losses: total 0.14228, policy 0.00325, value 0.10579, entropy -0.09459, approx_kl 0.02625 clip_fraction 0.07506. 
Epoch 5/5. 
Losses: total 0.13661, policy 0.00321, value 0.10343, entropy -0.09495, approx_kl 0.02602 clip_fraction 0.07673. 
[PPO] Values: min=-2.266, max=2.203, mean=0.197, std=0.344
[PPO] Returns: min=-1.000, max=1.000, mean=0.192, std=0.362
[PPO] Explained variance: 0.0547
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.026                    |
|    clip_fraction        | 0.077                    |
|    entropy              | 0.095                    |
|    explained_var        | 0.055                    |
|    iterations           | 7                        |
|    policy_loss          | 0.003                    |
|    total_timesteps      | 229376                   |
|    value_loss           | 0.103                    |
----------------------------------------------------

[PPO] Training completed in 7.27s
[PPO] Metrics: policy_loss=0.0032, value_loss=0.1034, entropy=0.0950
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 7, step 229376. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.887                    |
|    ep_len_mean          | 14.958                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.833                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 14.958 +/- 6.59 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.250 +/- 3.32 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 12.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.240 +/- 4.86 (96)     |
|    len_neg              | 17.240 +/- 4.86 (96)     |
|    len_pos              | 5.833 +/- 4.34 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 1.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.042 +/- 0.20 (96)      |
|    proven_neg           | 0.042 +/- 0.20 (96)      |
|    proven_pos           | 0.875 +/- 0.33 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (1)       |
|    reward_d_unknown_neg | 0.917 +/- 0.40 (96)      |
|    reward_neg           | 0.917 +/- 0.40 (96)      |
|    reward_pos           | 0.750 +/- 0.66 (24)      |
|    success_rate         | 0.208                    |
|    total_timesteps      | 229376                   |
----------------------------------------------------

[MRR] MRR: current=0.887, best=0.887 (iter 6), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 954                      |
|    iterations           | 7                        |
|    total_timesteps      | 229376                   |
----------------------------------------------------

[Annealing] Set lr to 2.9049728e-05 (progress=0.033)
[Annealing] Set ent_coef to 0.48394368 (progress=0.033)

[PPO] ===== Iteration 8 (229376/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 24.28s
[PPO] Recent episodes: reward=0.475, length=8.6
[PPO] Rollout collected in 24.28s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.032                    |
|    ep_rew_mean          | 0.282                    |
|    fps                  | 1349                     |
|    len                  | 9.032 +/- 7.84 (3632)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (286)     |
|    len_d_2_pos          | 4.972 +/- 2.41 (316)     |
|    len_d_3_pos          | 11.918 +/- 5.37 (73)     |
|    len_d_4_pos          | 11.739 +/- 6.29 (46)     |
|    len_d_unknown_neg    | 10.049 +/- 8.17 (2911)   |
|    len_neg              | 10.049 +/- 8.17 (2911)   |
|    len_pos              | 4.928 +/- 4.39 (721)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (286)     |
|    proven_d_2_pos       | 0.991 +/- 0.10 (316)     |
|    proven_d_3_pos       | 0.671 +/- 0.47 (73)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (46)      |
|    proven_d_unknown_neg | 0.102 +/- 0.30 (2911)    |
|    proven_neg           | 0.102 +/- 0.30 (2911)    |
|    proven_pos           | 0.963 +/- 0.19 (721)     |
|    reward               | 0.282 +/- 0.50 (3632)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (286)     |
|    reward_d_2_pos       | 0.981 +/- 0.19 (316)     |
|    reward_d_3_pos       | 0.342 +/- 0.94 (73)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (46)      |
|    reward_d_unknown_neg | 0.122 +/- 0.38 (2911)    |
|    reward_neg           | 0.122 +/- 0.38 (2911)    |
|    reward_pos           | 0.925 +/- 0.38 (721)     |
|    success_rate         | 0.273                    |
|    total_timesteps      | 262144                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.12582, policy 0.00247, value 0.08239, entropy -0.09672, approx_kl 0.00797 clip_fraction 0.05106. 
Epoch 2/5. 
Losses: total 0.11025, policy 0.00300, value 0.08024, entropy -0.09711, approx_kl 0.02147 clip_fraction 0.06453. 
Epoch 3/5. 
Losses: total 0.10059, policy 0.00302, value 0.07847, entropy -0.09794, approx_kl 0.02769 clip_fraction 0.07172. 
Epoch 4/5. 
Losses: total 0.10206, policy 0.00283, value 0.07688, entropy -0.09901, approx_kl 0.02553 clip_fraction 0.07519. 
Epoch 5/5. 
Losses: total 0.09601, policy 0.00258, value 0.07543, entropy -0.09945, approx_kl 0.02515 clip_fraction 0.07710. 
[PPO] Values: min=-1.625, max=1.484, mean=0.200, std=0.295
[PPO] Returns: min=-1.000, max=1.072, mean=0.214, std=0.343
[PPO] Explained variance: 0.2607
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.025                    |
|    clip_fraction        | 0.077                    |
|    entropy              | 0.099                    |
|    explained_var        | 0.261                    |
|    iterations           | 8                        |
|    policy_loss          | 0.003                    |
|    total_timesteps      | 262144                   |
|    value_loss           | 0.075                    |
----------------------------------------------------

[PPO] Training completed in 7.09s
[PPO] Metrics: policy_loss=0.0026, value_loss=0.0754, entropy=0.0994
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 8, step 262144. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.856                    |
|    ep_len_mean          | 15.142                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.792                    |
|    hits10               | 1.000                    |
|    hits3                | 0.875                    |
|    len                  | 15.142 +/- 6.58 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.250 +/- 3.32 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.385 +/- 4.76 (96)     |
|    len_neg              | 17.385 +/- 4.76 (96)     |
|    len_pos              | 6.167 +/- 5.05 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 262144                   |
----------------------------------------------------

[MRR] MRR: current=0.856, best=0.887 (iter 6), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 947                      |
|    iterations           | 8                        |
|    total_timesteps      | 262144                   |
----------------------------------------------------

[Annealing] Set lr to 2.8913974857142858e-05 (progress=0.037)
[Annealing] Set ent_coef to 0.48164992 (progress=0.037)

[PPO] ===== Iteration 9 (262144/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.63s
[PPO] Recent episodes: reward=0.150, length=7.7
[PPO] Rollout collected in 22.63s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.057                    |
|    ep_rew_mean          | 0.293                    |
|    fps                  | 1448                     |
|    len                  | 9.057 +/- 7.85 (3620)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (287)     |
|    len_d_2_pos          | 5.042 +/- 2.46 (334)     |
|    len_d_3_pos          | 10.585 +/- 4.49 (65)     |
|    len_d_4_pos          | 11.850 +/- 6.17 (40)     |
|    len_d_unknown_neg    | 10.148 +/- 8.20 (2894)   |
|    len_neg              | 10.148 +/- 8.20 (2894)   |
|    len_pos              | 4.711 +/- 3.94 (726)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (287)     |
|    proven_d_2_pos       | 0.994 +/- 0.08 (334)     |
|    proven_d_3_pos       | 0.754 +/- 0.43 (65)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (40)      |
|    proven_d_unknown_neg | 0.097 +/- 0.30 (2894)    |
|    proven_neg           | 0.097 +/- 0.30 (2894)    |
|    proven_pos           | 0.975 +/- 0.16 (726)     |
|    reward               | 0.293 +/- 0.49 (3620)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (287)     |
|    reward_d_2_pos       | 0.988 +/- 0.15 (334)     |
|    reward_d_3_pos       | 0.508 +/- 0.86 (65)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (40)      |
|    reward_d_unknown_neg | 0.129 +/- 0.37 (2894)    |
|    reward_neg           | 0.129 +/- 0.37 (2894)    |
|    reward_pos           | 0.950 +/- 0.31 (726)     |
|    success_rate         | 0.273                    |
|    total_timesteps      | 294912                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.10221, policy 0.00170, value 0.07787, entropy -0.10078, approx_kl 0.01024 clip_fraction 0.06339. 
Epoch 2/5. 
Losses: total 0.10728, policy 0.00217, value 0.07558, entropy -0.10143, approx_kl 0.01860 clip_fraction 0.07265. 
Epoch 3/5. 
Losses: total 0.09090, policy 0.00258, value 0.07417, entropy -0.10257, approx_kl 0.02919 clip_fraction 0.07993. 
Epoch 4/5. 
Losses: total 0.09180, policy 0.00244, value 0.07306, entropy -0.10321, approx_kl 0.02868 clip_fraction 0.08265. 
Epoch 5/5. 
Losses: total 0.08360, policy 0.00225, value 0.07209, entropy -0.10392, approx_kl 0.02660 clip_fraction 0.08426. 
[PPO] Values: min=-1.312, max=2.172, mean=0.230, std=0.305
[PPO] Returns: min=-1.000, max=1.000, mean=0.229, std=0.328
[PPO] Explained variance: 0.2109
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.027                    |
|    clip_fraction        | 0.084                    |
|    entropy              | 0.104                    |
|    explained_var        | 0.211                    |
|    iterations           | 9                        |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 294912                   |
|    value_loss           | 0.072                    |
----------------------------------------------------

[PPO] Training completed in 7.08s
[PPO] Metrics: policy_loss=0.0022, value_loss=0.0721, entropy=0.1039
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 9, step 294912. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.858                    |
|    ep_len_mean          | 14.942                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.792                    |
|    hits10               | 1.000                    |
|    hits3                | 0.875                    |
|    len                  | 14.942 +/- 6.72 (120)    |
|    len_d_1_pos          | 4.500 +/- 5.28 (10)      |
|    len_d_2_pos          | 6.667 +/- 4.50 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.073 +/- 5.09 (96)     |
|    len_neg              | 17.073 +/- 5.09 (96)     |
|    len_pos              | 6.417 +/- 5.59 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 294912                   |
----------------------------------------------------

[MRR] MRR: current=0.858, best=0.887 (iter 6), trend=insufficient_data
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 948                      |
|    iterations           | 9                        |
|    total_timesteps      | 294912                   |
----------------------------------------------------

[Annealing] Set lr to 2.8778221714285716e-05 (progress=0.042)
[Annealing] Set ent_coef to 0.47935616 (progress=0.042)

[PPO] ===== Iteration 10 (294912/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.89s
[PPO] Recent episodes: reward=0.350, length=6.6
[PPO] Rollout collected in 21.89s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.949                    |
|    ep_rew_mean          | 0.298                    |
|    fps                  | 1496                     |
|    len                  | 8.949 +/- 7.87 (3647)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (316)     |
|    len_d_2_pos          | 4.808 +/- 2.03 (308)     |
|    len_d_3_pos          | 11.406 +/- 5.09 (69)     |
|    len_d_4_pos          | 11.707 +/- 6.16 (41)     |
|    len_d_unknown_neg    | 10.044 +/- 8.21 (2913)   |
|    len_neg              | 10.044 +/- 8.21 (2913)   |
|    len_pos              | 4.605 +/- 4.05 (734)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (316)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (308)     |
|    proven_d_3_pos       | 0.681 +/- 0.47 (69)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (41)      |
|    proven_d_unknown_neg | 0.091 +/- 0.29 (2913)    |
|    proven_neg           | 0.091 +/- 0.29 (2913)    |
|    proven_pos           | 0.970 +/- 0.17 (734)     |
|    reward               | 0.298 +/- 0.48 (3647)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (316)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (308)     |
|    reward_d_3_pos       | 0.362 +/- 0.93 (69)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (41)      |
|    reward_d_unknown_neg | 0.136 +/- 0.36 (2913)    |
|    reward_neg           | 0.136 +/- 0.36 (2913)    |
|    reward_pos           | 0.940 +/- 0.34 (734)     |
|    success_rate         | 0.268                    |
|    total_timesteps      | 327680                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.06421, policy 0.00195, value 0.06058, entropy -0.10569, approx_kl 0.00978 clip_fraction 0.06366. 
Epoch 2/5. 
Losses: total 0.06663, policy 0.00242, value 0.05917, entropy -0.10668, approx_kl 0.01981 clip_fraction 0.07590. 
Epoch 3/5. 
Losses: total 0.06131, policy 0.00233, value 0.05826, entropy -0.10762, approx_kl 0.02393 clip_fraction 0.08165. 
Epoch 4/5. 
Losses: total 0.06917, policy 0.00229, value 0.05746, entropy -0.10806, approx_kl 0.02377 clip_fraction 0.08283. 
Epoch 5/5. 
Losses: total 0.05489, policy 0.00220, value 0.05679, entropy -0.10810, approx_kl 0.02614 clip_fraction 0.08562. 
[PPO] Values: min=-1.133, max=1.289, mean=0.228, std=0.268
[PPO] Returns: min=-1.000, max=1.019, mean=0.231, std=0.322
[PPO] Explained variance: 0.3849
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.026                    |
|    clip_fraction        | 0.086                    |
|    entropy              | 0.108                    |
|    explained_var        | 0.385                    |
|    iterations           | 10                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 327680                   |
|    value_loss           | 0.057                    |
----------------------------------------------------

[PPO] Training completed in 7.03s
[PPO] Metrics: policy_loss=0.0022, value_loss=0.0568, entropy=0.1081
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 10, step 327680. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.896                    |
|    ep_len_mean          | 14.875                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.833                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 14.875 +/- 6.56 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.250 +/- 3.32 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.052 +/- 4.87 (96)     |
|    len_neg              | 17.052 +/- 4.87 (96)     |
|    len_pos              | 6.167 +/- 5.05 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 327680                   |
----------------------------------------------------

[MRR] New best: 0.8958 at iteration 10
[MRR] MRR: current=0.896, best=0.896 (iter 10), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 951                      |
|    iterations           | 10                       |
|    total_timesteps      | 327680                   |
----------------------------------------------------

[Annealing] Set lr to 2.8642468571428573e-05 (progress=0.047)
[Annealing] Set ent_coef to 0.4770624 (progress=0.047)

[PPO] ===== Iteration 11 (327680/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.97s
[PPO] Recent episodes: reward=0.400, length=8.0
[PPO] Rollout collected in 20.97s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.108                    |
|    ep_rew_mean          | 0.292                    |
|    fps                  | 1562                     |
|    len                  | 9.108 +/- 7.80 (3598)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (284)     |
|    len_d_2_pos          | 5.067 +/- 2.56 (328)     |
|    len_d_3_pos          | 12.969 +/- 4.78 (65)     |
|    len_d_4_pos          | 12.222 +/- 6.25 (36)     |
|    len_d_unknown_neg    | 10.141 +/- 8.11 (2885)   |
|    len_neg              | 10.141 +/- 8.11 (2885)   |
|    len_pos              | 4.927 +/- 4.37 (713)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (284)     |
|    proven_d_2_pos       | 0.988 +/- 0.11 (328)     |
|    proven_d_3_pos       | 0.646 +/- 0.48 (65)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (36)      |
|    proven_d_unknown_neg | 0.092 +/- 0.29 (2885)    |
|    proven_neg           | 0.092 +/- 0.29 (2885)    |
|    proven_pos           | 0.962 +/- 0.19 (713)     |
|    reward               | 0.292 +/- 0.48 (3598)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (284)     |
|    reward_d_2_pos       | 0.976 +/- 0.22 (328)     |
|    reward_d_3_pos       | 0.292 +/- 0.96 (65)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (36)      |
|    reward_d_unknown_neg | 0.136 +/- 0.36 (2885)    |
|    reward_neg           | 0.136 +/- 0.36 (2885)    |
|    reward_pos           | 0.924 +/- 0.38 (713)     |
|    success_rate         | 0.264                    |
|    total_timesteps      | 360448                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.09878, policy 0.00139, value 0.07575, entropy -0.10320, approx_kl 0.01111 clip_fraction 0.06409. 
Epoch 2/5. 
Losses: total 0.09334, policy 0.00176, value 0.07348, entropy -0.10391, approx_kl 0.01789 clip_fraction 0.07097. 
Epoch 3/5. 
Losses: total 0.07923, policy 0.00159, value 0.07202, entropy -0.10518, approx_kl 0.02667 clip_fraction 0.07878. 
Epoch 4/5. 
Losses: total 0.08399, policy 0.00155, value 0.07087, entropy -0.10636, approx_kl 0.03184 clip_fraction 0.08303. 
Epoch 5/5. 
Losses: total 0.07521, policy 0.00147, value 0.06983, entropy -0.10693, approx_kl 0.02993 clip_fraction 0.08390. 
[PPO] Values: min=-1.117, max=2.047, mean=0.247, std=0.298
[PPO] Returns: min=-1.000, max=1.002, mean=0.225, std=0.333
[PPO] Explained variance: 0.2613
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.030                    |
|    clip_fraction        | 0.084                    |
|    entropy              | 0.107                    |
|    explained_var        | 0.261                    |
|    iterations           | 11                       |
|    policy_loss          | 0.001                    |
|    total_timesteps      | 360448                   |
|    value_loss           | 0.070                    |
----------------------------------------------------

[PPO] Training completed in 7.04s
[PPO] Metrics: policy_loss=0.0015, value_loss=0.0698, entropy=0.1069
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 11, step 360448. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.920                    |
|    ep_len_mean          | 15.008                   |
|    ep_rew_mean          | 0.867                    |
|    hits1                | 0.875                    |
|    hits10               | 1.000                    |
|    hits3                | 0.958                    |
|    len                  | 15.008 +/- 6.66 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.229 +/- 4.83 (96)     |
|    len_neg              | 17.229 +/- 4.83 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.042 +/- 0.20 (96)      |
|    proven_neg           | 0.042 +/- 0.20 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.867 +/- 0.50 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.917 +/- 0.40 (96)      |
|    reward_neg           | 0.917 +/- 0.40 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.200                    |
|    total_timesteps      | 360448                   |
----------------------------------------------------

[MRR] New best: 0.9201 at iteration 11
[MRR] MRR: current=0.920, best=0.920 (iter 11), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 958                      |
|    iterations           | 11                       |
|    total_timesteps      | 360448                   |
----------------------------------------------------

[Annealing] Set lr to 2.8506715428571428e-05 (progress=0.051)
[Annealing] Set ent_coef to 0.47476864 (progress=0.051)

[PPO] ===== Iteration 12 (360448/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.09s
[PPO] Recent episodes: reward=0.200, length=10.5
[PPO] Rollout collected in 21.09s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.844                    |
|    ep_rew_mean          | 0.304                    |
|    fps                  | 1553                     |
|    len                  | 8.844 +/- 7.79 (3715)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (292)     |
|    len_d_2_pos          | 4.500 +/- 1.52 (330)     |
|    len_d_3_pos          | 13.108 +/- 5.17 (65)     |
|    len_d_4_pos          | 11.962 +/- 6.47 (52)     |
|    len_d_unknown_neg    | 9.850 +/- 8.12 (2976)    |
|    len_neg              | 9.850 +/- 8.12 (2976)    |
|    len_pos              | 4.794 +/- 4.38 (739)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (292)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (330)     |
|    proven_d_3_pos       | 0.692 +/- 0.46 (65)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (52)      |
|    proven_d_unknown_neg | 0.084 +/- 0.28 (2976)    |
|    proven_neg           | 0.084 +/- 0.28 (2976)    |
|    proven_pos           | 0.973 +/- 0.16 (739)     |
|    reward               | 0.304 +/- 0.47 (3715)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (292)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (330)     |
|    reward_d_3_pos       | 0.385 +/- 0.92 (65)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (52)      |
|    reward_d_unknown_neg | 0.145 +/- 0.35 (2976)    |
|    reward_neg           | 0.145 +/- 0.35 (2976)    |
|    reward_pos           | 0.946 +/- 0.32 (739)     |
|    success_rate         | 0.261                    |
|    total_timesteps      | 393216                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.07381, policy 0.00285, value 0.06100, entropy -0.11064, approx_kl 0.01039 clip_fraction 0.06699. 
Epoch 2/5. 
Losses: total 0.06115, policy 0.00281, value 0.06016, entropy -0.11078, approx_kl 0.01946 clip_fraction 0.07794. 
Epoch 3/5. 
Losses: total 0.06828, policy 0.00247, value 0.05927, entropy -0.11226, approx_kl 0.02208 clip_fraction 0.08364. 
Epoch 4/5. 
Losses: total 0.05772, policy 0.00223, value 0.05843, entropy -0.11290, approx_kl 0.02365 clip_fraction 0.08745. 
Epoch 5/5. 
Losses: total 0.05400, policy 0.00216, value 0.05772, entropy -0.11372, approx_kl 0.02639 clip_fraction 0.08928. 
[PPO] Values: min=-1.117, max=1.578, mean=0.237, std=0.255
[PPO] Returns: min=-1.000, max=1.000, mean=0.232, std=0.327
[PPO] Explained variance: 0.4178
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.026                    |
|    clip_fraction        | 0.089                    |
|    entropy              | 0.114                    |
|    explained_var        | 0.418                    |
|    iterations           | 12                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 393216                   |
|    value_loss           | 0.058                    |
----------------------------------------------------

[PPO] Training completed in 7.13s
[PPO] Metrics: policy_loss=0.0022, value_loss=0.0577, entropy=0.1137
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 12, step 393216. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.944                    |
|    ep_len_mean          | 14.458                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 14.458 +/- 6.64 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.542 +/- 5.09 (96)     |
|    len_neg              | 16.542 +/- 5.09 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 393216                   |
----------------------------------------------------

[MRR] New best: 0.9444 at iteration 12
[MRR] MRR: current=0.944, best=0.944 (iter 12), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 960                      |
|    iterations           | 12                       |
|    total_timesteps      | 393216                   |
----------------------------------------------------

[Annealing] Set lr to 2.8370962285714285e-05 (progress=0.056)
[Annealing] Set ent_coef to 0.47247488 (progress=0.056)

[PPO] ===== Iteration 13 (393216/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.90s
[PPO] Recent episodes: reward=0.275, length=10.1
[PPO] Rollout collected in 22.90s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 9.013                    |
|    ep_rew_mean          | 0.290                    |
|    fps                  | 1430                     |
|    len                  | 9.013 +/- 7.72 (3619)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (276)     |
|    len_d_2_pos          | 4.503 +/- 1.48 (346)     |
|    len_d_3_pos          | 11.797 +/- 5.09 (69)     |
|    len_d_4_pos          | 12.045 +/- 6.65 (44)     |
|    len_d_unknown_neg    | 10.112 +/- 8.03 (2884)   |
|    len_neg              | 10.112 +/- 8.03 (2884)   |
|    len_pos              | 4.699 +/- 4.10 (735)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (276)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (346)     |
|    proven_d_3_pos       | 0.710 +/- 0.45 (69)      |
|    proven_d_4_pos       | 0.909 +/- 0.29 (44)      |
|    proven_d_unknown_neg | 0.100 +/- 0.30 (2884)    |
|    proven_neg           | 0.100 +/- 0.30 (2884)    |
|    proven_pos           | 0.967 +/- 0.18 (735)     |
|    reward               | 0.290 +/- 0.49 (3619)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (276)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (346)     |
|    reward_d_3_pos       | 0.420 +/- 0.91 (69)      |
|    reward_d_4_pos       | 0.818 +/- 0.57 (44)      |
|    reward_d_unknown_neg | 0.126 +/- 0.37 (2884)    |
|    reward_neg           | 0.126 +/- 0.37 (2884)    |
|    reward_pos           | 0.935 +/- 0.36 (735)     |
|    success_rate         | 0.276                    |
|    total_timesteps      | 425984                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.08503, policy 0.00484, value 0.06513, entropy -0.11080, approx_kl 0.01167 clip_fraction 0.06378. 
Epoch 2/5. 
Losses: total 0.06977, policy 0.00426, value 0.06365, entropy -0.11276, approx_kl 0.01676 clip_fraction 0.07529. 
Epoch 3/5. 
Losses: total 0.07125, policy 0.00360, value 0.06262, entropy -0.11332, approx_kl 0.02190 clip_fraction 0.08173. 
Epoch 4/5. 
Losses: total 0.07141, policy 0.00340, value 0.06169, entropy -0.11381, approx_kl 0.02439 clip_fraction 0.08550. 
Epoch 5/5. 
Losses: total 0.07049, policy 0.00329, value 0.06090, entropy -0.11439, approx_kl 0.02502 clip_fraction 0.08846. 
[PPO] Values: min=-1.039, max=1.430, mean=0.242, std=0.266
[PPO] Returns: min=-1.000, max=1.000, mean=0.221, std=0.337
[PPO] Explained variance: 0.4021
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.025                    |
|    clip_fraction        | 0.088                    |
|    entropy              | 0.114                    |
|    explained_var        | 0.402                    |
|    iterations           | 13                       |
|    policy_loss          | 0.003                    |
|    total_timesteps      | 425984                   |
|    value_loss           | 0.061                    |
----------------------------------------------------

[PPO] Training completed in 7.08s
[PPO] Metrics: policy_loss=0.0033, value_loss=0.0609, entropy=0.1144
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 13, step 425984. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.944                    |
|    ep_len_mean          | 14.233                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 14.233 +/- 6.65 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.260 +/- 5.22 (96)     |
|    len_neg              | 16.260 +/- 5.22 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 425984                   |
----------------------------------------------------

[MRR] MRR: current=0.944, best=0.944 (iter 12), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 959                      |
|    iterations           | 13                       |
|    total_timesteps      | 425984                   |
----------------------------------------------------

[Annealing] Set lr to 2.8235209142857143e-05 (progress=0.061)
[Annealing] Set ent_coef to 0.47018112 (progress=0.061)

[PPO] ===== Iteration 14 (425984/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.36s
[PPO] Recent episodes: reward=0.475, length=7.0
[PPO] Rollout collected in 20.36s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.914                    |
|    ep_rew_mean          | 0.294                    |
|    fps                  | 1609                     |
|    len                  | 8.914 +/- 7.75 (3677)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (299)     |
|    len_d_2_pos          | 4.317 +/- 0.75 (325)     |
|    len_d_3_pos          | 11.618 +/- 5.08 (68)     |
|    len_d_4_pos          | 12.541 +/- 6.70 (37)     |
|    len_d_unknown_neg    | 10.014 +/- 8.07 (2948)   |
|    len_neg              | 10.014 +/- 8.07 (2948)   |
|    len_pos              | 4.465 +/- 3.94 (729)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (299)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (325)     |
|    proven_d_3_pos       | 0.676 +/- 0.47 (68)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (37)      |
|    proven_d_unknown_neg | 0.093 +/- 0.29 (2948)    |
|    proven_neg           | 0.093 +/- 0.29 (2948)    |
|    proven_pos           | 0.970 +/- 0.17 (729)     |
|    reward               | 0.294 +/- 0.48 (3677)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (299)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (325)     |
|    reward_d_3_pos       | 0.353 +/- 0.94 (68)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (37)      |
|    reward_d_unknown_neg | 0.134 +/- 0.36 (2948)    |
|    reward_neg           | 0.134 +/- 0.36 (2948)    |
|    reward_pos           | 0.940 +/- 0.34 (729)     |
|    success_rate         | 0.267                    |
|    total_timesteps      | 458752                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.07160, policy 0.00150, value 0.05534, entropy -0.11250, approx_kl 0.00890 clip_fraction 0.05771. 
Epoch 2/5. 
Losses: total 0.05598, policy 0.00203, value 0.05406, entropy -0.11241, approx_kl 0.01757 clip_fraction 0.07057. 
Epoch 3/5. 
Losses: total 0.04564, policy 0.00193, value 0.05334, entropy -0.11317, approx_kl 0.02565 clip_fraction 0.07626. 
Epoch 4/5. 
Losses: total 0.04017, policy 0.00177, value 0.05274, entropy -0.11389, approx_kl 0.02275 clip_fraction 0.08022. 
Epoch 5/5. 
Losses: total 0.04883, policy 0.00166, value 0.05223, entropy -0.11467, approx_kl 0.02338 clip_fraction 0.08304. 
[PPO] Values: min=-1.062, max=1.047, mean=0.214, std=0.265
[PPO] Returns: min=-1.035, max=1.000, mean=0.221, std=0.329
[PPO] Explained variance: 0.4633
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.023                    |
|    clip_fraction        | 0.083                    |
|    entropy              | 0.115                    |
|    explained_var        | 0.463                    |
|    iterations           | 14                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 458752                   |
|    value_loss           | 0.052                    |
----------------------------------------------------

[PPO] Training completed in 7.19s
[PPO] Metrics: policy_loss=0.0017, value_loss=0.0522, entropy=0.1147
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 14, step 458752. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.944                    |
|    ep_len_mean          | 14.492                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 14.492 +/- 6.65 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.583 +/- 5.10 (96)     |
|    len_neg              | 16.583 +/- 5.10 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 458752                   |
----------------------------------------------------

[MRR] MRR: current=0.944, best=0.944 (iter 12), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 962                      |
|    iterations           | 14                       |
|    total_timesteps      | 458752                   |
----------------------------------------------------

[Annealing] Set lr to 2.8099456e-05 (progress=0.066)
[Annealing] Set ent_coef to 0.46788736 (progress=0.066)

[PPO] ===== Iteration 15 (458752/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.91s
[PPO] Recent episodes: reward=0.200, length=11.2
[PPO] Rollout collected in 21.91s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.682                    |
|    ep_rew_mean          | 0.313                    |
|    fps                  | 1495                     |
|    len                  | 8.682 +/- 7.71 (3786)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (299)     |
|    len_d_2_pos          | 4.348 +/- 0.85 (351)     |
|    len_d_3_pos          | 11.701 +/- 4.99 (67)     |
|    len_d_4_pos          | 12.267 +/- 6.21 (45)     |
|    len_d_unknown_neg    | 9.726 +/- 8.07 (3024)    |
|    len_neg              | 9.726 +/- 8.07 (3024)    |
|    len_pos              | 4.541 +/- 3.92 (762)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (299)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (351)     |
|    proven_d_3_pos       | 0.642 +/- 0.48 (67)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (45)      |
|    proven_d_unknown_neg | 0.075 +/- 0.26 (3024)    |
|    proven_neg           | 0.075 +/- 0.26 (3024)    |
|    proven_pos           | 0.969 +/- 0.17 (762)     |
|    reward               | 0.313 +/- 0.46 (3786)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (299)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (351)     |
|    reward_d_3_pos       | 0.284 +/- 0.96 (67)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (45)      |
|    reward_d_unknown_neg | 0.156 +/- 0.33 (3024)    |
|    reward_neg           | 0.156 +/- 0.33 (3024)    |
|    reward_pos           | 0.937 +/- 0.35 (762)     |
|    success_rate         | 0.255                    |
|    total_timesteps      | 491520                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.04534, policy 0.00189, value 0.05247, entropy -0.11337, approx_kl 0.01081 clip_fraction 0.07010. 
Epoch 2/5. 
Losses: total 0.04464, policy 0.00191, value 0.05116, entropy -0.11273, approx_kl 0.01713 clip_fraction 0.07773. 
Epoch 3/5. 
Losses: total 0.04865, policy 0.00190, value 0.05038, entropy -0.11376, approx_kl 0.02341 clip_fraction 0.08411. 
Epoch 4/5. 
Losses: total 0.04208, policy 0.00178, value 0.04977, entropy -0.11461, approx_kl 0.02124 clip_fraction 0.08656. 
Epoch 5/5. 
Losses: total 0.03532, policy 0.00167, value 0.04924, entropy -0.11520, approx_kl 0.02132 clip_fraction 0.08818. 
[PPO] Values: min=-1.117, max=1.203, mean=0.224, std=0.268
[PPO] Returns: min=-1.000, max=1.000, mean=0.228, std=0.325
[PPO] Explained variance: 0.4802
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.021                    |
|    clip_fraction        | 0.088                    |
|    entropy              | 0.115                    |
|    explained_var        | 0.480                    |
|    iterations           | 15                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 491520                   |
|    value_loss           | 0.049                    |
----------------------------------------------------

[PPO] Training completed in 7.21s
[PPO] Metrics: policy_loss=0.0017, value_loss=0.0492, entropy=0.1152
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 15, step 491520. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.948                    |
|    ep_len_mean          | 14.442                   |
|    ep_rew_mean          | 0.867                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 0.958                    |
|    len                  | 14.442 +/- 6.60 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.521 +/- 5.04 (96)     |
|    len_neg              | 16.521 +/- 5.04 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.042 +/- 0.20 (96)      |
|    proven_neg           | 0.042 +/- 0.20 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.867 +/- 0.50 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.917 +/- 0.40 (96)      |
|    reward_neg           | 0.917 +/- 0.40 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.200                    |
|    total_timesteps      | 491520                   |
----------------------------------------------------

[MRR] New best: 0.9479 at iteration 15
[MRR] MRR: current=0.948, best=0.948 (iter 15), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 962                      |
|    iterations           | 15                       |
|    total_timesteps      | 491520                   |
----------------------------------------------------

[Annealing] Set lr to 2.7963702857142858e-05 (progress=0.070)
[Annealing] Set ent_coef to 0.4655936 (progress=0.070)

[PPO] ===== Iteration 16 (491520/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.43s
[PPO] Recent episodes: reward=0.275, length=5.9
[PPO] Rollout collected in 22.43s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.835                    |
|    ep_rew_mean          | 0.301                    |
|    fps                  | 1460                     |
|    len                  | 8.835 +/- 7.78 (3698)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (285)     |
|    len_d_2_pos          | 4.336 +/- 0.72 (342)     |
|    len_d_3_pos          | 11.754 +/- 5.16 (69)     |
|    len_d_4_pos          | 12.326 +/- 5.57 (43)     |
|    len_d_unknown_neg    | 9.895 +/- 8.13 (2959)    |
|    len_neg              | 9.895 +/- 8.13 (2959)    |
|    len_pos              | 4.593 +/- 3.93 (739)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (285)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (342)     |
|    proven_d_3_pos       | 0.652 +/- 0.48 (69)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (43)      |
|    proven_d_unknown_neg | 0.086 +/- 0.28 (2959)    |
|    proven_neg           | 0.086 +/- 0.28 (2959)    |
|    proven_pos           | 0.968 +/- 0.18 (739)     |
|    reward               | 0.301 +/- 0.47 (3698)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (285)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (342)     |
|    reward_d_3_pos       | 0.304 +/- 0.95 (69)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (43)      |
|    reward_d_unknown_neg | 0.143 +/- 0.35 (2959)    |
|    reward_neg           | 0.143 +/- 0.35 (2959)    |
|    reward_pos           | 0.935 +/- 0.35 (739)     |
|    success_rate         | 0.262                    |
|    total_timesteps      | 524288                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.04733, policy 0.00131, value 0.05363, entropy -0.11692, approx_kl 0.01194 clip_fraction 0.06989. 
Epoch 2/5. 
Losses: total 0.05382, policy 0.00168, value 0.05244, entropy -0.11540, approx_kl 0.02085 clip_fraction 0.08276. 
Epoch 3/5. 
Losses: total 0.04474, policy 0.00157, value 0.05166, entropy -0.11703, approx_kl 0.01921 clip_fraction 0.08600. 
Epoch 4/5. 
Losses: total 0.03927, policy 0.00159, value 0.05101, entropy -0.11789, approx_kl 0.02365 clip_fraction 0.08842. 
Epoch 5/5. 
Losses: total 0.04497, policy 0.00161, value 0.05046, entropy -0.11909, approx_kl 0.02746 clip_fraction 0.09120. 
[PPO] Values: min=-1.062, max=1.016, mean=0.216, std=0.265
[PPO] Returns: min=-1.000, max=1.000, mean=0.225, std=0.327
[PPO] Explained variance: 0.4848
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.027                    |
|    clip_fraction        | 0.091                    |
|    entropy              | 0.119                    |
|    explained_var        | 0.485                    |
|    iterations           | 16                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 524288                   |
|    value_loss           | 0.050                    |
----------------------------------------------------

[PPO] Training completed in 7.20s
[PPO] Metrics: policy_loss=0.0016, value_loss=0.0505, entropy=0.1191
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 16, step 524288. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.924                    |
|    ep_len_mean          | 14.467                   |
|    ep_rew_mean          | 0.900                    |
|    hits1                | 0.875                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 14.467 +/- 6.47 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.552 +/- 4.81 (96)     |
|    len_neg              | 16.552 +/- 4.81 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.021 +/- 0.14 (96)      |
|    proven_neg           | 0.021 +/- 0.14 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.900 +/- 0.44 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.958 +/- 0.29 (96)      |
|    reward_neg           | 0.958 +/- 0.29 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.183                    |
|    total_timesteps      | 524288                   |
----------------------------------------------------

[MRR] MRR: current=0.924, best=0.948 (iter 15), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 963                      |
|    iterations           | 16                       |
|    total_timesteps      | 524288                   |
----------------------------------------------------

[Annealing] Set lr to 2.7827949714285716e-05 (progress=0.075)
[Annealing] Set ent_coef to 0.46329984 (progress=0.075)

[PPO] ===== Iteration 17 (524288/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 25.02s
[PPO] Recent episodes: reward=0.150, length=7.4
[PPO] Rollout collected in 25.02s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.659                    |
|    ep_rew_mean          | 0.306                    |
|    fps                  | 1309                     |
|    len                  | 8.659 +/- 7.64 (3788)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (275)     |
|    len_d_2_pos          | 4.314 +/- 0.73 (357)     |
|    len_d_3_pos          | 11.200 +/- 4.82 (85)     |
|    len_d_4_pos          | 9.333 +/- 4.11 (36)      |
|    len_d_unknown_neg    | 9.695 +/- 8.03 (3035)    |
|    len_neg              | 9.695 +/- 8.03 (3035)    |
|    len_pos              | 4.486 +/- 3.49 (753)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (275)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (357)     |
|    proven_d_3_pos       | 0.694 +/- 0.46 (85)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (36)      |
|    proven_d_unknown_neg | 0.079 +/- 0.27 (3035)    |
|    proven_neg           | 0.079 +/- 0.27 (3035)    |
|    proven_pos           | 0.965 +/- 0.18 (753)     |
|    reward               | 0.306 +/- 0.46 (3788)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (275)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (357)     |
|    reward_d_3_pos       | 0.388 +/- 0.92 (85)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (36)      |
|    reward_d_unknown_neg | 0.151 +/- 0.34 (3035)    |
|    reward_neg           | 0.151 +/- 0.34 (3035)    |
|    reward_pos           | 0.931 +/- 0.37 (753)     |
|    success_rate         | 0.256                    |
|    total_timesteps      | 557056                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.07799, policy 0.00200, value 0.05927, entropy -0.11473, approx_kl 0.01065 clip_fraction 0.07153. 
Epoch 2/5. 
Losses: total 0.06293, policy 0.00209, value 0.05703, entropy -0.11209, approx_kl 0.01988 clip_fraction 0.07957. 
Epoch 3/5. 
Losses: total 0.06020, policy 0.00212, value 0.05578, entropy -0.11443, approx_kl 0.02651 clip_fraction 0.08609. 
Epoch 4/5. 
Losses: total 0.05027, policy 0.00209, value 0.05481, entropy -0.11593, approx_kl 0.03415 clip_fraction 0.09084. 
Epoch 5/5. 
Losses: total 0.05621, policy 0.00207, value 0.05407, entropy -0.11688, approx_kl 0.03755 clip_fraction 0.09283. 
[PPO] Values: min=-1.008, max=1.500, mean=0.248, std=0.277
[PPO] Returns: min=-1.000, max=1.000, mean=0.225, std=0.336
[PPO] Explained variance: 0.4577
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.038                    |
|    clip_fraction        | 0.093                    |
|    entropy              | 0.117                    |
|    explained_var        | 0.458                    |
|    iterations           | 17                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 557056                   |
|    value_loss           | 0.054                    |
----------------------------------------------------

[PPO] Training completed in 7.17s
[PPO] Metrics: policy_loss=0.0021, value_loss=0.0541, entropy=0.1169
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 17, step 557056. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.948                    |
|    ep_len_mean          | 14.450                   |
|    ep_rew_mean          | 0.900                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 0.958                    |
|    len                  | 14.450 +/- 6.50 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.531 +/- 4.87 (96)     |
|    len_neg              | 16.531 +/- 4.87 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.021 +/- 0.14 (96)      |
|    proven_neg           | 0.021 +/- 0.14 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.900 +/- 0.44 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.958 +/- 0.29 (96)      |
|    reward_neg           | 0.958 +/- 0.29 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.183                    |
|    total_timesteps      | 557056                   |
----------------------------------------------------

[MRR] MRR: current=0.948, best=0.948 (iter 15), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 958                      |
|    iterations           | 17                       |
|    total_timesteps      | 557056                   |
----------------------------------------------------

[Annealing] Set lr to 2.7692196571428573e-05 (progress=0.080)
[Annealing] Set ent_coef to 0.46100608 (progress=0.080)

[PPO] ===== Iteration 18 (557056/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.00s
[PPO] Recent episodes: reward=0.500, length=6.8
[PPO] Rollout collected in 22.00s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.756                    |
|    ep_rew_mean          | 0.307                    |
|    fps                  | 1489                     |
|    len                  | 8.756 +/- 7.59 (3750)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (281)     |
|    len_d_2_pos          | 4.469 +/- 1.02 (354)     |
|    len_d_3_pos          | 11.671 +/- 5.12 (76)     |
|    len_d_4_pos          | 10.227 +/- 4.35 (44)     |
|    len_d_unknown_neg    | 9.801 +/- 7.95 (2995)    |
|    len_neg              | 9.801 +/- 7.95 (2995)    |
|    len_pos              | 4.611 +/- 3.69 (755)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (281)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (354)     |
|    proven_d_3_pos       | 0.671 +/- 0.47 (76)      |
|    proven_d_4_pos       | 0.977 +/- 0.15 (44)      |
|    proven_d_unknown_neg | 0.080 +/- 0.27 (2995)    |
|    proven_neg           | 0.080 +/- 0.27 (2995)    |
|    proven_pos           | 0.966 +/- 0.18 (755)     |
|    reward               | 0.307 +/- 0.47 (3750)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (281)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (354)     |
|    reward_d_3_pos       | 0.342 +/- 0.94 (76)      |
|    reward_d_4_pos       | 0.955 +/- 0.30 (44)      |
|    reward_d_unknown_neg | 0.150 +/- 0.34 (2995)    |
|    reward_neg           | 0.150 +/- 0.34 (2995)    |
|    reward_pos           | 0.931 +/- 0.36 (755)     |
|    success_rate         | 0.258                    |
|    total_timesteps      | 589824                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.04707, policy 0.00182, value 0.05029, entropy -0.12163, approx_kl 0.01098 clip_fraction 0.07361. 
Epoch 2/5. 
Losses: total 0.04041, policy 0.00210, value 0.04870, entropy -0.12070, approx_kl 0.01705 clip_fraction 0.08240. 
Epoch 3/5. 
Losses: total 0.03585, policy 0.00228, value 0.04761, entropy -0.12219, approx_kl 0.02453 clip_fraction 0.08776. 
Epoch 4/5. 
Losses: total 0.02498, policy 0.00215, value 0.04664, entropy -0.12294, approx_kl 0.02719 clip_fraction 0.09199. 
Epoch 5/5. 
Losses: total 0.03798, policy 0.00220, value 0.04583, entropy -0.12403, approx_kl 0.02490 clip_fraction 0.09353. 
[PPO] Values: min=-1.086, max=1.047, mean=0.200, std=0.274
[PPO] Returns: min=-1.000, max=1.000, mean=0.221, std=0.336
[PPO] Explained variance: 0.5596
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.025                    |
|    clip_fraction        | 0.094                    |
|    entropy              | 0.124                    |
|    explained_var        | 0.560                    |
|    iterations           | 18                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 589824                   |
|    value_loss           | 0.046                    |
----------------------------------------------------

[PPO] Training completed in 7.22s
[PPO] Metrics: policy_loss=0.0022, value_loss=0.0458, entropy=0.1240
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 18, step 589824. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.951                    |
|    ep_len_mean          | 15.158                   |
|    ep_rew_mean          | 0.900                    |
|    hits1                | 0.917                    |
|    hits10               | 1.000                    |
|    hits3                | 1.000                    |
|    len                  | 15.158 +/- 6.62 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 17.417 +/- 4.67 (96)     |
|    len_neg              | 17.417 +/- 4.67 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.021 +/- 0.14 (96)      |
|    proven_neg           | 0.021 +/- 0.14 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.900 +/- 0.44 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.958 +/- 0.29 (96)      |
|    reward_neg           | 0.958 +/- 0.29 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.183                    |
|    total_timesteps      | 589824                   |
----------------------------------------------------

[MRR] New best: 0.9514 at iteration 18
[MRR] MRR: current=0.951, best=0.951 (iter 18), trend=improving
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 960                      |
|    iterations           | 18                       |
|    total_timesteps      | 589824                   |
----------------------------------------------------

[Annealing] Set lr to 2.7556443428571428e-05 (progress=0.084)
[Annealing] Set ent_coef to 0.45871232 (progress=0.084)

[PPO] ===== Iteration 19 (589824/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 23.90s
[PPO] Recent episodes: reward=0.325, length=13.4
[PPO] Rollout collected in 23.90s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.483                    |
|    ep_rew_mean          | 0.291                    |
|    fps                  | 1370                     |
|    len                  | 8.483 +/- 7.57 (3861)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (296)     |
|    len_d_2_pos          | 4.336 +/- 0.89 (348)     |
|    len_d_3_pos          | 12.746 +/- 5.33 (67)     |
|    len_d_4_pos          | 8.464 +/- 2.56 (56)      |
|    len_d_unknown_neg    | 9.477 +/- 7.96 (3094)    |
|    len_neg              | 9.477 +/- 7.96 (3094)    |
|    len_pos              | 4.471 +/- 3.58 (767)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (296)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (348)     |
|    proven_d_3_pos       | 0.582 +/- 0.49 (67)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (56)      |
|    proven_d_unknown_neg | 0.093 +/- 0.29 (3094)    |
|    proven_neg           | 0.093 +/- 0.29 (3094)    |
|    proven_pos           | 0.963 +/- 0.19 (767)     |
|    reward               | 0.291 +/- 0.48 (3861)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (296)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (348)     |
|    reward_d_3_pos       | 0.164 +/- 0.99 (67)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (56)      |
|    reward_d_unknown_neg | 0.133 +/- 0.36 (3094)    |
|    reward_neg           | 0.133 +/- 0.36 (3094)    |
|    reward_pos           | 0.927 +/- 0.38 (767)     |
|    success_rate         | 0.266                    |
|    total_timesteps      | 622592                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.04235, policy 0.00226, value 0.05194, entropy -0.13061, approx_kl 0.01071 clip_fraction 0.06699. 
Epoch 2/5. 
Losses: total 0.06036, policy 0.00302, value 0.05077, entropy -0.12961, approx_kl 0.01928 clip_fraction 0.07799. 
Epoch 3/5. 
Losses: total 0.03748, policy 0.00259, value 0.04994, entropy -0.13021, approx_kl 0.02488 clip_fraction 0.08593. 
Epoch 4/5. 
Losses: total 0.03691, policy 0.00240, value 0.04930, entropy -0.13063, approx_kl 0.02619 clip_fraction 0.08973. 
Epoch 5/5. 
Losses: total 0.04206, policy 0.00230, value 0.04875, entropy -0.13099, approx_kl 0.02522 clip_fraction 0.09180. 
[PPO] Values: min=-1.055, max=1.062, mean=0.229, std=0.281
[PPO] Returns: min=-1.039, max=1.036, mean=0.218, std=0.345
[PPO] Explained variance: 0.5515
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.025                    |
|    clip_fraction        | 0.092                    |
|    entropy              | 0.131                    |
|    explained_var        | 0.551                    |
|    iterations           | 19                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 622592                   |
|    value_loss           | 0.049                    |
----------------------------------------------------

[PPO] Training completed in 7.16s
[PPO] Metrics: policy_loss=0.0023, value_loss=0.0487, entropy=0.1310
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 19, step 622592. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.913                    |
|    ep_len_mean          | 14.567                   |
|    ep_rew_mean          | 0.883                    |
|    hits1                | 0.875                    |
|    hits10               | 1.000                    |
|    hits3                | 0.958                    |
|    len                  | 14.567 +/- 6.71 (120)    |
|    len_d_1_pos          | 4.400 +/- 4.98 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.677 +/- 5.15 (96)     |
|    len_neg              | 16.677 +/- 5.15 (96)     |
|    len_pos              | 6.125 +/- 5.46 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.883 +/- 0.47 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.192                    |
|    total_timesteps      | 622592                   |
----------------------------------------------------

[MRR] MRR: current=0.913, best=0.951 (iter 18), trend=stable
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 955                      |
|    iterations           | 19                       |
|    total_timesteps      | 622592                   |
----------------------------------------------------

[Annealing] Set lr to 2.7420690285714285e-05 (progress=0.089)
[Annealing] Set ent_coef to 0.45641856000000003 (progress=0.089)

[PPO] ===== Iteration 20 (622592/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 31.09s
[PPO] Recent episodes: reward=0.400, length=7.0
[PPO] Rollout collected in 31.09s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.649                    |
|    ep_rew_mean          | 0.286                    |
|    fps                  | 1053                     |
|    len                  | 8.649 +/- 7.65 (3780)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (312)     |
|    len_d_2_pos          | 4.650 +/- 1.51 (320)     |
|    len_d_3_pos          | 11.693 +/- 5.17 (88)     |
|    len_d_4_pos          | 9.024 +/- 2.17 (41)      |
|    len_d_unknown_neg    | 9.666 +/- 8.04 (3019)    |
|    len_neg              | 9.666 +/- 8.04 (3019)    |
|    len_pos              | 4.614 +/- 3.73 (761)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (312)     |
|    proven_d_2_pos       | 0.972 +/- 0.17 (320)     |
|    proven_d_3_pos       | 0.670 +/- 0.47 (88)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (41)      |
|    proven_d_unknown_neg | 0.095 +/- 0.29 (3019)    |
|    proven_neg           | 0.095 +/- 0.29 (3019)    |
|    proven_pos           | 0.950 +/- 0.22 (761)     |
|    reward               | 0.286 +/- 0.49 (3780)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (312)     |
|    reward_d_2_pos       | 0.944 +/- 0.33 (320)     |
|    reward_d_3_pos       | 0.341 +/- 0.94 (88)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (41)      |
|    reward_d_unknown_neg | 0.132 +/- 0.37 (3019)    |
|    reward_neg           | 0.132 +/- 0.37 (3019)    |
|    reward_pos           | 0.900 +/- 0.44 (761)     |
|    success_rate         | 0.267                    |
|    total_timesteps      | 655360                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.05253, policy 0.00146, value 0.05279, entropy -0.12499, approx_kl 0.01277 clip_fraction 0.07428. 
Epoch 2/5. 
Losses: total 0.04614, policy 0.00202, value 0.05169, entropy -0.12306, approx_kl 0.02397 clip_fraction 0.08365. 
Epoch 3/5. 
Losses: total 0.03520, policy 0.00209, value 0.05086, entropy -0.12455, approx_kl 0.02526 clip_fraction 0.09092. 
Epoch 4/5. 
Losses: total 0.04514, policy 0.00205, value 0.05016, entropy -0.12543, approx_kl 0.02400 clip_fraction 0.09377. 
Epoch 5/5. 
Losses: total 0.02943, policy 0.00196, value 0.04958, entropy -0.12640, approx_kl 0.02459 clip_fraction 0.09409. 
[PPO] Values: min=-1.062, max=1.047, mean=0.203, std=0.285
[PPO] Returns: min=-1.000, max=1.000, mean=0.211, std=0.352
[PPO] Explained variance: 0.5597
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.025                    |
|    clip_fraction        | 0.094                    |
|    entropy              | 0.126                    |
|    explained_var        | 0.560                    |
|    iterations           | 20                       |
|    policy_loss          | 0.002                    |
|    total_timesteps      | 655360                   |
|    value_loss           | 0.050                    |
----------------------------------------------------

[PPO] Training completed in 7.44s
[PPO] Metrics: policy_loss=0.0020, value_loss=0.0496, entropy=0.1264
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 20, step 655360. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.792                   |
|    ep_rew_mean          | 0.900                    |
|    hits1                | 0.875                    |
|    hits10               | 1.000                    |
|    hits3                | 0.917                    |
|    len                  | 14.792 +/- 6.72 (120)    |
|    len_d_1_pos          | 4.500 +/- 5.28 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.948 +/- 5.05 (96)     |
|    len_neg              | 16.948 +/- 5.05 (96)     |
|    len_pos              | 6.167 +/- 5.56 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.021 +/- 0.14 (96)      |
|    proven_neg           | 0.021 +/- 0.14 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.900 +/- 0.44 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 0.958 +/- 0.29 (96)      |
|    reward_neg           | 0.958 +/- 0.29 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.183                    |
|    total_timesteps      | 655360                   |
----------------------------------------------------

[MRR] MRR: current=0.915, best=0.951 (iter 18), trend=declining
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 941                      |
|    iterations           | 20                       |
|    total_timesteps      | 655360                   |
----------------------------------------------------

[Annealing] Set lr to 2.7284937142857143e-05 (progress=0.094)
[Annealing] Set ent_coef to 0.4541248 (progress=0.094)

[PPO] ===== Iteration 21 (655360/7000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 27.42s
[PPO] Recent episodes: reward=0.400, length=8.7
[PPO] Rollout collected in 27.42s
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 8.833                    |
|    ep_rew_mean          | 0.304                    |
|    fps                  | 1195                     |
|    len                  | 8.833 +/- 7.72 (3704)    |
|    len_d_1_pos          | 2.000 +/- 0.00 (291)     |
|    len_d_2_pos          | 4.324 +/- 0.91 (330)     |
|    len_d_3_pos          | 12.338 +/- 5.45 (68)     |
|    len_d_4_pos          | 9.574 +/- 2.47 (47)      |
|    len_d_unknown_neg    | 9.912 +/- 8.08 (2968)    |
|    len_neg              | 9.912 +/- 8.08 (2968)    |
|    len_pos              | 4.481 +/- 3.65 (736)     |
|    proven_d_1_pos       | 1.000 +/- 0.00 (291)     |
|    proven_d_2_pos       | 1.000 +/- 0.00 (330)     |
|    proven_d_3_pos       | 0.632 +/- 0.48 (68)      |
|    proven_d_4_pos       | 1.000 +/- 0.00 (47)      |
|    proven_d_unknown_neg | 0.082 +/- 0.27 (2968)    |
|    proven_neg           | 0.082 +/- 0.27 (2968)    |
|    proven_pos           | 0.966 +/- 0.18 (736)     |
|    reward               | 0.304 +/- 0.47 (3704)    |
|    reward_d_1_pos       | 1.000 +/- 0.00 (291)     |
|    reward_d_2_pos       | 1.000 +/- 0.00 (330)     |
|    reward_d_3_pos       | 0.265 +/- 0.96 (68)      |
|    reward_d_4_pos       | 1.000 +/- 0.00 (47)      |
|    reward_d_unknown_neg | 0.148 +/- 0.34 (2968)    |
|    reward_neg           | 0.148 +/- 0.34 (2968)    |
|    reward_pos           | 0.932 +/- 0.36 (736)     |
|    success_rate         | 0.258                    |
|    total_timesteps      | 688128                   |
----------------------------------------------------


[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.04277, policy 0.00252, value 0.05105, entropy -0.12830, approx_kl 0.01131 clip_fraction 0.07275. 
Epoch 2/5. 
Losses: total 0.04680, policy 0.00327, value 0.04931, entropy -0.12546, approx_kl 0.02051 clip_fraction 0.08623. 
Epoch 3/5. 
Losses: total 0.04346, policy 0.00357, value 0.04850, entropy -0.12685, approx_kl 0.02518 clip_fraction 0.09203. 
Epoch 4/5. 
Losses: total 0.03932, policy 0.00322, value 0.04792, entropy -0.12800, approx_kl 0.02408 clip_fraction 0.09546. 
Epoch 5/5. 
Losses: total 0.02994, policy 0.00312, value 0.04742, entropy -0.12890, approx_kl 0.02242 clip_fraction 0.09712. 
[PPO] Values: min=-1.062, max=1.523, mean=0.231, std=0.285
[PPO] Returns: min=-1.000, max=1.019, mean=0.226, std=0.327
[PPO] Explained variance: 0.4980
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.022                    |
|    clip_fraction        | 0.097                    |
|    entropy              | 0.129                    |
|    explained_var        | 0.498                    |
|    iterations           | 21                       |
|    policy_loss          | 0.003                    |
|    total_timesteps      | 688128                   |
|    value_loss           | 0.047                    |
----------------------------------------------------

[PPO] Training completed in 8.34s
[PPO] Metrics: policy_loss=0.0031, value_loss=0.0474, entropy=0.1289
[MRREvaluationCallback] Starting evaluation of 24 queries, at iter 21, step 688128. Resetting collector.
DEBUG: eval_corruptions called with n_corruptions=None, corruption_modes=['tail']
DEBUG: need_head=False, need_tail=True
DEBUG: mode=tail, corrs_list[0] shape=torch.Size([4, 3]), total corruptions for query 0: 4
----------------------------------------------------
| eval/                   |                          |
|    _mrr                 | 0.927                    |
|    ep_len_mean          | 14.108                   |
|    ep_rew_mean          | 0.933                    |
|    hits1                | 0.875                    |
|    hits10               | 1.000                    |
|    hits3                | 0.958                    |
|    len                  | 14.108 +/- 6.58 (120)    |
|    len_d_1_pos          | 4.200 +/- 4.40 (10)      |
|    len_d_2_pos          | 6.167 +/- 4.43 (12)      |
|    len_d_3_pos          | 9.000 +/- 0.00 (1)       |
|    len_d_4_pos          | 20.000 +/- 0.00 (1)      |
|    len_d_unknown_neg    | 16.125 +/- 5.19 (96)     |
|    len_neg              | 16.125 +/- 5.19 (96)     |
|    len_pos              | 6.042 +/- 5.27 (24)      |
|    proven_d_1_pos       | 0.900 +/- 0.30 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.28 (12)      |
|    proven_d_3_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_4_pos       | 0.000 +/- 0.00 (1)       |
|    proven_d_unknown_neg | 0.000 +/- 0.00 (96)      |
|    proven_neg           | 0.000 +/- 0.00 (96)      |
|    proven_pos           | 0.833 +/- 0.37 (24)      |
|    reward               | 0.933 +/- 0.36 (120)     |
|    reward_d_1_pos       | 0.800 +/- 0.60 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.55 (12)      |
|    reward_d_3_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_4_pos       | -1.000 +/- 0.00 (1)      |
|    reward_d_unknown_neg | 1.000 +/- 0.00 (96)      |
|    reward_neg           | 1.000 +/- 0.00 (96)      |
|    reward_pos           | 0.667 +/- 0.75 (24)      |
|    success_rate         | 0.167                    |
|    total_timesteps      | 688128                   |
----------------------------------------------------

[MRR] MRR: current=0.927, best=0.951 (iter 18), trend=declining
----------------------------------------------------
| rollout/                |                          |
|    fps                  | 931                      |
|    iterations           | 21                       |
|    total_timesteps      | 688128                   |
----------------------------------------------------
