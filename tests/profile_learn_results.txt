Device: cuda
Total timesteps: 90
Training time: 98.06s

================================================================================
Top by Cumulative Time
================================================================================
         57163409 function calls (54211348 primitive calls) in 93.142 seconds

   Ordered by: cumulative time
   List reduced from 8301 to 30 due to restriction <30>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        6    0.000    0.000   16.116    2.686 graph.py:2351(_compile_to_module_lines)
        6    0.000    0.000   15.317    2.553 graph.py:2256(codegen)
      443    0.003    0.000   15.116    0.034 utils.py:3013(run)
      438    0.006    0.000   14.400    0.033 cudagraph_trees.py:2241(execute_node)
      438    0.004    0.000   14.354    0.033 cudagraph_trees.py:1105(run)
      438    0.002    0.000   14.166    0.032 cudagraph_trees.py:1219(run_graph)
      443   13.180    0.030   13.435    0.030 graphs.py:139(replay)
      170    0.015    0.000   11.928    0.070 benchmarking.py:34(wrapper)
      170    0.080    0.000   10.986    0.065 benchmarking.py:207(benchmark_gpu)
        6    0.000    0.000    9.885    1.647 scheduler.py:5194(codegen)
        6    0.000    0.000    9.883    1.647 scheduler.py:5318(_codegen_partitions)
        6    0.000    0.000    9.505    1.584 scheduler.py:5202(_codegen_partition_wrapper)
        6    0.010    0.002    9.026    1.504 scheduler.py:5351(_codegen)
      266    0.001    0.000    8.911    0.033 cuda_combined_scheduling.py:126(codegen_node)
      266    0.006    0.000    8.910    0.033 simd.py:1384(codegen_node)
      266    0.016    0.000    8.201    0.031 simd.py:1445(codegen_node_schedule)
      128    0.280    0.002    7.653    0.060 env.py:722(step_and_maybe_reset)
      120    0.002    0.000    7.051    0.059 triton_heuristics.py:833(bench)
       80    0.004    0.000    6.971    0.087 optimizer.py:497(wrapper)
       80    0.003    0.000    6.955    0.087 optimizer.py:62(_use_grad)
       80    0.001    0.000    6.950    0.087 adam.py:213(step)
      833    0.006    0.000    6.933    0.008 __init__.py:1073(synchronize)
       80    0.001    0.000    6.927    0.087 optimizer.py:133(maybe_fallback)
       80    0.001    0.000    6.926    0.087 adam.py:885(adam)
       80    0.002    0.000    6.922    0.087 adam.py:789(_fused_adam)
      833    6.904    0.008    6.904    0.008 {built-in method torch._C._cuda_synchronize}
      245    0.232    0.001    6.377    0.026 env.py:789(_compute_derived_states)
      266    0.012    0.000    6.119    0.023 simd.py:1534(codegen_node_schedule_with_kernel)
      571    0.016    0.000    5.392    0.009 scheduler.py:1346(codegen)
        6    0.000    0.000    5.335    0.889 graph.py:2245(_update_scheduler)




================================================================================
Top by Total Time
================================================================================
         57163409 function calls (54211348 primitive calls) in 93.142 seconds

   Ordered by: internal time
   List reduced from 8301 to 30 due to restriction <30>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      443   13.180    0.030   13.435    0.030 graphs.py:139(replay)
      833    6.904    0.008    6.904    0.008 {built-in method torch._C._cuda_synchronize}
       48    4.929    0.103    4.930    0.103 {method 'item' of 'torch._C.TensorBase' objects}
       80    4.698    0.059    4.698    0.059 {built-in method torch._foreach_add_}
       80    2.216    0.028    2.216    0.028 {built-in method torch._fused_adam_}
       90    1.871    0.021    3.500    0.039 rollout.py:246(get)
  355/352    1.847    0.005    1.862    0.005 {method 'mean' of 'torch._C.TensorBase' objects}
     12/0    1.612    0.134    0.000          {built-in method torch.randperm}
4231/2267    1.554    0.000    1.181    0.001 {method 'read' of '_io.BufferedReader' objects}
     53/6    1.507    0.028    0.000    0.000 {method 'acquire' of '_thread.lock' objects}
8799089/8715680    1.369    0.000    1.967    0.000 {built-in method builtins.isinstance}
  448/443    1.103    0.002    1.103    0.002 {built-in method torch._foreach_copy_}
      128    1.082    0.008    1.197    0.009 fast_distributions.py:51(sample)
2498293/2446249    0.691    0.000    0.965    0.000 {built-in method builtins.getattr}
11254/7901    0.636    0.000    0.776    0.000 {method 'clone' of 'torch._C.TensorBase' objects}
54736/31483    0.596    0.000    0.623    0.000 {method 'zero_' of 'torch._C.TensorBase' objects}
 2866/246    0.555    0.000    0.109    0.000 functional_tensor.py:356(__torch_dispatch__)
  966/964    0.506    0.001    0.517    0.001 {built-in method torch._ops.aten.mm}
514635/514455    0.503    0.000    1.105    0.000 sympify.py:124(sympify)
532848/487660    0.464    0.000    1.677    0.000 cache.py:69(wrapper)
      245    0.460    0.002    1.098    0.004 unification.py:925(unify_with_rules)
    51559    0.440    0.000    0.905    0.000 relational.py:1422(is_eq)
  1757453    0.421    0.000    0.535    0.000 {method 'get' of 'dict' objects}
      486    0.418    0.001    0.418    0.001 {built-in method torch._ops.aten.bmm}
      245    0.391    0.002    0.745    0.003 unification.py:1266(standardize_derived_states)
118945/41341    0.379    0.000    1.061    0.000 _pytree.py:1272(helper)
      245    0.364    0.001    0.957    0.004 unification.py:796(unify_with_facts)
1486648/1137926    0.353    0.000    0.470    0.000 {built-in method builtins.hash}
    11761    0.352    0.000    0.382    0.000 {_launch_kernel}
259471/259175    0.336    0.000    0.580    0.000 _config_module.py:370(__getattr__)


