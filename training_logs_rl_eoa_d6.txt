Running experiments for the following parameters: DATASET_NAME: ['kinship_family'] MODEL_NAME: ['PPO'] SEED: [[0]]
Experiment number  0  out of  1  experiments.
Seed 0  in  [0]

Run vars: kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-6-python-1 
 Namespace(dataset_name='kinship_family', model_name='PPO', learn_embeddings=True, atom_embedder='transe', state_embedder='mean', atom_embedding_size=64, seed=[0], max_depth=20, timesteps_train=4000000, restore_best_val_model=True, memory_pruning=True, rule_depend_var=False, dynamic_consult=True, corruption_mode='dynamic', train_neg_pos_ratio=1, false_rules=False, end_proof_action=True, skip_unary_actions=True, ent_coef=0.1, clip_range=0.2, engine='python', depth_filtered=6, truncate_atoms=True, truncate_states=True, padding_atoms=3, padding_states=10, non_provable_queries=True, non_provable_corruptions=True, corruption_scheme=['head', 'tail'], janus_file=None, data_path='./data/', train_file='train_depth_6.txt', valid_file='valid_depth_6.txt', test_file='test_depth_6.txt', rules_file='rules.txt', facts_file='train.txt', state_embedding_size=64, constant_embedding_size=64, predicate_embedding_size=64, variable_no=500, device='cuda', load_model=False, save_model=True, models_path='models/kinship_family', n_eval_queries=200, n_test_queries=None, valid_negatives=None, test_negatives=100, eval_freq=16384, n_envs=128, n_eval_envs=128, n_callback_envs=1, n_steps=128, n_epochs=10, batch_size=128, lr=0.0003, run_signature='kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-6-python-1', seed_run_i=0) 

kinship_family-transe-mean-64-3-10-dynamic-True-True-1-True-False-True-True-True-True-True-False-20-True-0.1-0.2-6-python-1
Device: cuda
CUDA available: True, Device count: 2
Using cuda device
Embedding dim in policy 64
Embedding dim in value 64
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=16384, episode_reward=0.23 +/- 0.42
Episode length: 1.35 +/- 0.56
New best mean reward!
---------------evaluation finished---------------
Epoch 1 completed in 169.08 seconds.
Improved rollout/ep_rew_mean to 0.1800
Time to collect_rollouts 169.12
Training model
Time to train 48.49
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.34      |
|    ep_rew_mean          | 0.18      |
| time/                   |           |
|    fps                  | 74        |
|    iterations           | 1         |
|    total_timesteps      | 16384     |
| train/                  |           |
|    approx_kl            | 7.6279755 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0695   |
|    explained_variance   | -47.8     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.585     |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.137     |
|    value_loss           | 0.922     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=32640, episode_reward=0.14 +/- 0.35
Episode length: 1.23 +/- 0.47
---------------evaluation finished---------------
Epoch 2 completed in 161.19 seconds.
Improved rollout/ep_rew_mean to 0.2100
Time to collect_rollouts 161.26
Training model
Time to train 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.43      |
|    ep_rew_mean          | 0.21      |
| time/                   |           |
|    fps                  | 76        |
|    iterations           | 2         |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 28.283907 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0622   |
|    explained_variance   | 0.0566    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0534    |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.031     |
|    value_loss           | 0.154     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
---------------evaluation started---------------
Eval num_timesteps=48896, episode_reward=0.56 +/- 0.50
Episode length: 2.12 +/- 0.82
New best mean reward!
---------------evaluation finished---------------
Epoch 3 completed in 180.35 seconds.
Improved rollout/ep_rew_mean to 0.4000
Time to collect_rollouts 180.42
Training model
Time to train 48.78
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.17      |
|    ep_rew_mean          | 0.4       |
| time/                   |           |
|    fps                  | 74        |
|    iterations           | 3         |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 5.7161236 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0518   |
|    explained_variance   | -0.022    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.118     |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0405    |
|    value_loss           | 0.186     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=65152, episode_reward=0.59 +/- 0.49
Episode length: 1.97 +/- 0.58
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 4 completed in 185.82 seconds.
Time to collect_rollouts 185.83
Training model
Time to train 48.5
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.96     |
|    ep_rew_mean          | 0.3      |
| time/                   |          |
|    fps                  | 73       |
|    iterations           | 4        |
|    total_timesteps      | 65536    |
| train/                  |          |
|    approx_kl            | 7.056585 |
|    clip_fraction        | 0.283    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0454  |
|    explained_variance   | 0.0155   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.117    |
|    n_updates            | 40       |
|    policy_gradient_loss | 0.0307   |
|    value_loss           | 0.2      |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=81408, episode_reward=0.62 +/- 0.48
Episode length: 1.99 +/- 0.58
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 5 completed in 177.79 seconds.
Improved rollout/ep_rew_mean to 0.4500
Time to collect_rollouts 177.85
Training model
Time to train 48.61
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.14     |
|    ep_rew_mean          | 0.45     |
| time/                   |          |
|    fps                  | 73       |
|    iterations           | 5        |
|    total_timesteps      | 81920    |
| train/                  |          |
|    approx_kl            | 6.416111 |
|    clip_fraction        | 0.228    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0416  |
|    explained_variance   | 0.0271   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0993   |
|    n_updates            | 50       |
|    policy_gradient_loss | 0.0237   |
|    value_loss           | 0.182    |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=97664, episode_reward=0.62 +/- 0.48
Episode length: 1.98 +/- 0.55
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 6 completed in 183.44 seconds.
Time to collect_rollouts 183.44
Training model
Time to train 48.92
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.83     |
|    ep_rew_mean          | 0.42     |
| time/                   |          |
|    fps                  | 72       |
|    iterations           | 6        |
|    total_timesteps      | 98304    |
| train/                  |          |
|    approx_kl            | 8.839572 |
|    clip_fraction        | 0.26     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.039   |
|    explained_variance   | 0.0121   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0834   |
|    n_updates            | 60       |
|    policy_gradient_loss | 0.00734  |
|    value_loss           | 0.206    |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=113920, episode_reward=0.60 +/- 0.49
Episode length: 1.97 +/- 0.55
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 7 completed in 175.82 seconds.
Time to collect_rollouts 175.83
Training model
Time to train 48.57
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.42     |
|    ep_rew_mean          | 0.34     |
| time/                   |          |
|    fps                  | 72       |
|    iterations           | 7        |
|    total_timesteps      | 114688   |
| train/                  |          |
|    approx_kl            | 8.03323  |
|    clip_fraction        | 0.273    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0425  |
|    explained_variance   | 0.0285   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0889   |
|    n_updates            | 70       |
|    policy_gradient_loss | 0.0196   |
|    value_loss           | 0.177    |
--------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=130176, episode_reward=0.59 +/- 0.49
Episode length: 1.92 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 8 completed in 173.90 seconds.
Time to collect_rollouts 173.91
Training model
Time to train 48.74
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.78      |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 72        |
|    iterations           | 8         |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 7.3955116 |
|    clip_fraction        | 0.28      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0507   |
|    explained_variance   | 0.0298    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0477    |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.00481  |
|    value_loss           | 0.204     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=146432, episode_reward=0.60 +/- 0.49
Episode length: 1.96 +/- 0.57
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 9 completed in 173.69 seconds.
Time to collect_rollouts 173.7
Training model
Time to train 48.85
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.46       |
|    ep_rew_mean          | 0.32       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 9          |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.55068123 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.47      |
|    explained_variance   | 0.0505     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00482    |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0517     |
|    value_loss           | 0.169      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=162688, episode_reward=0.60 +/- 0.49
Episode length: 1.89 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 10 completed in 177.52 seconds.
Time to collect_rollouts 177.53
Training model
Time to train 48.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.79      |
|    ep_rew_mean          | 0.33      |
| time/                   |           |
|    fps                  | 72        |
|    iterations           | 10        |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.1491284 |
|    clip_fraction        | 0.285     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.933    |
|    explained_variance   | 0.074     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00864   |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.027     |
|    value_loss           | 0.178     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=178944, episode_reward=0.65 +/- 0.48
Episode length: 1.98 +/- 0.59
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 11 completed in 176.22 seconds.
Time to collect_rollouts 176.23
Training model
Time to train 49.05
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.89       |
|    ep_rew_mean          | 0.25       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 11         |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.10769096 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.0373     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0173    |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.0103     |
|    value_loss           | 0.174      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=195200, episode_reward=0.63 +/- 0.48
Episode length: 1.99 +/- 0.60
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 12 completed in 181.52 seconds.
Time to collect_rollouts 181.52
Training model
Time to train 48.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02        |
|    ep_rew_mean          | 0.28        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 12          |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.056204118 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.0613      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0355     |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 0.168       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=211456, episode_reward=0.62 +/- 0.49
Episode length: 1.98 +/- 0.60
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 13 completed in 176.45 seconds.
Time to collect_rollouts 176.45
Training model
Time to train 49.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02        |
|    ep_rew_mean          | 0.28        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 13          |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.034766283 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.0769      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0756     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 0.165       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=227712, episode_reward=0.63 +/- 0.48
Episode length: 2.00 +/- 0.61
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 14 completed in 177.53 seconds.
Time to collect_rollouts 177.53
Training model
Time to train 48.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16        |
|    ep_rew_mean          | 0.28        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 14          |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.023172539 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.0788      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.044      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 0.166       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=243968, episode_reward=0.61 +/- 0.49
Episode length: 1.96 +/- 0.52
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 15 completed in 181.31 seconds.
Time to collect_rollouts 181.32
Training model
Time to train 48.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22        |
|    ep_rew_mean          | 0.33        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 15          |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.017340384 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.0908      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0371     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00835    |
|    value_loss           | 0.161       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=260224, episode_reward=0.61 +/- 0.49
Episode length: 1.95 +/- 0.53
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 16 completed in 179.29 seconds.
Time to collect_rollouts 179.29
Training model
Time to train 48.85
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79        |
|    ep_rew_mean          | 0.26        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 16          |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.019123623 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.0665      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0808     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.159       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=276480, episode_reward=0.62 +/- 0.49
Episode length: 1.96 +/- 0.53
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 17 completed in 180.18 seconds.
Time to collect_rollouts 180.19
Training model
Time to train 48.72
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.31       |
|    ep_rew_mean          | 0.29       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 17         |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.01212183 |
|    clip_fraction        | 0.0441     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.0898     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.055     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0077    |
|    value_loss           | 0.158      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=292736, episode_reward=0.67 +/- 0.47
Episode length: 1.97 +/- 0.50
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 18 completed in 181.46 seconds.
Time to collect_rollouts 181.47
Training model
Time to train 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15        |
|    ep_rew_mean          | 0.35        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 18          |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.014324969 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.97       |
|    explained_variance   | 0.104       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0527     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 0.156       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=308992, episode_reward=0.69 +/- 0.46
Episode length: 1.99 +/- 0.47
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 19 completed in 181.92 seconds.
Time to collect_rollouts 181.92
Training model
Time to train 49.4
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02        |
|    ep_rew_mean          | 0.19        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 19          |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.014316702 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 0.155       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=325248, episode_reward=0.67 +/- 0.47
Episode length: 1.97 +/- 0.50
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 20 completed in 179.89 seconds.
Time to collect_rollouts 179.89
Training model
Time to train 49.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.22       |
|    ep_rew_mean          | 0.38       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 20         |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.00921032 |
|    clip_fraction        | 0.0275     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.916     |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0374    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00992   |
|    value_loss           | 0.155      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=341504, episode_reward=0.69 +/- 0.46
Episode length: 1.97 +/- 0.47
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 21 completed in 183.19 seconds.
Time to collect_rollouts 183.2
Training model
Time to train 48.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.7         |
|    ep_rew_mean          | 0.3         |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 21          |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.009101133 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0694     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 0.152       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=357760, episode_reward=0.69 +/- 0.46
Episode length: 1.96 +/- 0.47
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 22 completed in 173.79 seconds.
Time to collect_rollouts 173.8
Training model
Time to train 48.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2         |
|    ep_rew_mean          | 0.33        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 22          |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.011145214 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 0.153       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=374016, episode_reward=0.68 +/- 0.47
Episode length: 1.98 +/- 0.48
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 23 completed in 181.36 seconds.
Time to collect_rollouts 181.37
Training model
Time to train 48.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 23          |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.011987313 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 0.148       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=390272, episode_reward=0.70 +/- 0.46
Episode length: 1.97 +/- 0.52
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 24 completed in 176.24 seconds.
Time to collect_rollouts 176.24
Training model
Time to train 48.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.41         |
|    ep_rew_mean          | 0.27         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 24           |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0070748925 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.871       |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0397      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00873     |
|    value_loss           | 0.15         |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=406528, episode_reward=0.72 +/- 0.45
Episode length: 2.01 +/- 0.60
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 25 completed in 178.37 seconds.
Time to collect_rollouts 178.38
Training model
Time to train 37.2
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35        |
|    ep_rew_mean          | 0.3         |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 25          |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.008158351 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.862      |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.065      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 0.155       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=422784, episode_reward=0.74 +/- 0.44
Episode length: 2.00 +/- 0.52
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 26 completed in 177.34 seconds.
Time to collect_rollouts 177.34
Training model
Time to train 48.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73        |
|    ep_rew_mean          | 0.25        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 26          |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.008755492 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.147       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=439040, episode_reward=0.73 +/- 0.44
Episode length: 1.96 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 27 completed in 177.21 seconds.
Time to collect_rollouts 177.21
Training model
Time to train 47.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.32         |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 27           |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0065714237 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.836       |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0197      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.146        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
---------------evaluation started---------------
Eval num_timesteps=455296, episode_reward=0.73 +/- 0.44
Episode length: 1.96 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 125/128 steps
Epoch 28 completed in 177.48 seconds.
Time to collect_rollouts 177.48
Training model
Time to train 48.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 28          |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.006238324 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 0.147       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=471552, episode_reward=0.73 +/- 0.44
Episode length: 1.99 +/- 0.52
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 29 completed in 176.06 seconds.
Time to collect_rollouts 176.07
Training model
Time to train 47.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.72         |
|    ep_rew_mean          | 0.41         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 29           |
|    total_timesteps      | 475136       |
| train/                  |              |
|    approx_kl            | 0.0076034786 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.813       |
|    explained_variance   | 0.152        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0423      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00998     |
|    value_loss           | 0.147        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=487808, episode_reward=0.75 +/- 0.43
Episode length: 1.98 +/- 0.46
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 30 completed in 172.89 seconds.
Time to collect_rollouts 172.89
Training model
Time to train 48.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22        |
|    ep_rew_mean          | 0.33        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 30          |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.005420477 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0155     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 0.145       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=504064, episode_reward=0.74 +/- 0.44
Episode length: 1.98 +/- 0.45
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 31 completed in 178.14 seconds.
Time to collect_rollouts 178.15
Training model
Time to train 40.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.35       |
|    ep_rew_mean          | 0.38       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 31         |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.01268969 |
|    clip_fraction        | 0.0205     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.17       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0296    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 0.145      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=520320, episode_reward=0.75 +/- 0.43
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 32 completed in 178.12 seconds.
Time to collect_rollouts 178.13
Training model
Time to train 48.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 32          |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.004874573 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.187       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.141       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=536576, episode_reward=0.74 +/- 0.44
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 33 completed in 176.69 seconds.
Time to collect_rollouts 176.7
Training model
Time to train 48.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 33          |
|    total_timesteps      | 540672      |
| train/                  |             |
|    approx_kl            | 0.004896827 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.139       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=552832, episode_reward=0.75 +/- 0.43
Episode length: 1.97 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 34 completed in 174.11 seconds.
Time to collect_rollouts 174.12
Training model
Time to train 48.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.31         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 34           |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0052275965 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.754       |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0196      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 0.142        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=569088, episode_reward=0.74 +/- 0.44
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 35 completed in 172.82 seconds.
Time to collect_rollouts 172.83
Training model
Time to train 48.68
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.15         |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 35           |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0063804975 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0361      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.135        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=585344, episode_reward=0.75 +/- 0.43
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 36 completed in 177.29 seconds.
Time to collect_rollouts 177.3
Training model
Time to train 48.76
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.15         |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 36           |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0055564884 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.753       |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0167      |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.137        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=601600, episode_reward=0.77 +/- 0.42
Episode length: 1.98 +/- 0.44
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 37 completed in 173.75 seconds.
Time to collect_rollouts 173.76
Training model
Time to train 47.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.42         |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 37           |
|    total_timesteps      | 606208       |
| train/                  |              |
|    approx_kl            | 0.0048172977 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.746       |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00691     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.132        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=617856, episode_reward=0.76 +/- 0.43
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 38 completed in 171.24 seconds.
Time to collect_rollouts 171.25
Training model
Time to train 49.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.57         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 38           |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 0.0056033283 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.735       |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0134      |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.135        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=634112, episode_reward=0.77 +/- 0.42
Episode length: 1.98 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 39 completed in 170.99 seconds.
Time to collect_rollouts 171.0
Training model
Time to train 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.9         |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 39          |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.007164634 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 0.13        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=650368, episode_reward=0.76 +/- 0.43
Episode length: 1.98 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 40 completed in 168.75 seconds.
Time to collect_rollouts 168.76
Training model
Time to train 48.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.7          |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 40           |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0069102263 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.73        |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0432      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.131        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=666624, episode_reward=0.78 +/- 0.41
Episode length: 1.98 +/- 0.44
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 41 completed in 172.32 seconds.
Time to collect_rollouts 172.32
Training model
Time to train 47.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53        |
|    ep_rew_mean          | 0.33        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 41          |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.004276954 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.13        |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=682880, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.43
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 42 completed in 175.00 seconds.
Time to collect_rollouts 175.01
Training model
Time to train 48.6
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.24         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 42           |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 0.0062011187 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.725       |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0246      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.129        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=699136, episode_reward=0.78 +/- 0.42
Episode length: 1.99 +/- 0.47
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 43 completed in 168.31 seconds.
Time to collect_rollouts 168.32
Training model
Time to train 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.56         |
|    ep_rew_mean          | 0.3          |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 43           |
|    total_timesteps      | 704512       |
| train/                  |              |
|    approx_kl            | 0.0055082203 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0.279        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0237      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.125        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=715392, episode_reward=0.77 +/- 0.42
Episode length: 2.00 +/- 0.49
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 44 completed in 171.88 seconds.
Time to collect_rollouts 171.89
Training model
Time to train 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.62         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 44           |
|    total_timesteps      | 720896       |
| train/                  |              |
|    approx_kl            | 0.0046885116 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.127        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=731648, episode_reward=0.78 +/- 0.41
Episode length: 1.99 +/- 0.49
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 45 completed in 164.86 seconds.
Time to collect_rollouts 164.87
Training model
Time to train 48.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31        |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 45          |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.005253666 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0509     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.122       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=747904, episode_reward=0.79 +/- 0.41
Episode length: 1.99 +/- 0.49
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 46 completed in 168.30 seconds.
Time to collect_rollouts 168.31
Training model
Time to train 48.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 46          |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.004609015 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.125       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=764160, episode_reward=0.79 +/- 0.41
Episode length: 1.99 +/- 0.49
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 47 completed in 167.97 seconds.
Time to collect_rollouts 167.98
Training model
Time to train 48.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.68         |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 47           |
|    total_timesteps      | 770048       |
| train/                  |              |
|    approx_kl            | 0.0044118348 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.704       |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0285      |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.126        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=780416, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.47
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 48 completed in 170.18 seconds.
Time to collect_rollouts 170.19
Training model
Time to train 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47         |
|    ep_rew_mean          | 0.33         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 48           |
|    total_timesteps      | 786432       |
| train/                  |              |
|    approx_kl            | 0.0054056244 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0367      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 0.125        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=796672, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.45
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 49 completed in 165.27 seconds.
Time to collect_rollouts 165.28
Training model
Time to train 48.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.74         |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 49           |
|    total_timesteps      | 802816       |
| train/                  |              |
|    approx_kl            | 0.0048996047 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.709       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0135      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.121        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=812928, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.45
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 50 completed in 165.44 seconds.
Time to collect_rollouts 165.45
Training model
Time to train 47.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.23         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 50           |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0046414537 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.696       |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0254      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.123        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=829184, episode_reward=0.78 +/- 0.42
Episode length: 1.98 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 51 completed in 170.54 seconds.
Time to collect_rollouts 170.55
Training model
Time to train 48.42
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.48         |
|    ep_rew_mean          | 0.34         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 51           |
|    total_timesteps      | 835584       |
| train/                  |              |
|    approx_kl            | 0.0051415795 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.698       |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0382      |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.116        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=845440, episode_reward=0.78 +/- 0.42
Episode length: 1.99 +/- 0.46
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 52 completed in 167.80 seconds.
Time to collect_rollouts 167.81
Training model
Time to train 47.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.58       |
|    ep_rew_mean          | 0.38       |
| time/                   |            |
|    fps                  | 73         |
|    iterations           | 52         |
|    total_timesteps      | 851968     |
| train/                  |            |
|    approx_kl            | 0.00534442 |
|    clip_fraction        | 0.0287     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.683     |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00429   |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.118      |
----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
---------------evaluation started---------------
Eval num_timesteps=861696, episode_reward=0.77 +/- 0.42
Episode length: 1.98 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 53 completed in 168.39 seconds.
Time to collect_rollouts 168.4
Training model
Time to train 48.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.72         |
|    ep_rew_mean          | 0.29         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 53           |
|    total_timesteps      | 868352       |
| train/                  |              |
|    approx_kl            | 0.0047962046 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.685       |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.052       |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.113        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=877952, episode_reward=0.78 +/- 0.42
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 54 completed in 164.07 seconds.
Time to collect_rollouts 164.07
Training model
Time to train 48.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.69         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 54           |
|    total_timesteps      | 884736       |
| train/                  |              |
|    approx_kl            | 0.0067216735 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0305      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.115        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=894208, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 55 completed in 168.09 seconds.
Time to collect_rollouts 168.1
Training model
Time to train 47.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.66        |
|    ep_rew_mean          | 0.4         |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 55          |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.023679499 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.114       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=910464, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 56 completed in 166.17 seconds.
Time to collect_rollouts 166.17
Training model
Time to train 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.11         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 56           |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0044719307 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.667       |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0417      |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 0.114        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=926720, episode_reward=0.80 +/- 0.40
Episode length: 1.98 +/- 0.43
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 57 completed in 168.56 seconds.
Time to collect_rollouts 168.57
Training model
Time to train 48.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.65         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 57           |
|    total_timesteps      | 933888       |
| train/                  |              |
|    approx_kl            | 0.0049835774 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.675       |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0249      |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.111        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=942976, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 58 completed in 164.78 seconds.
Time to collect_rollouts 164.78
Training model
Time to train 48.66
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.33         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 58           |
|    total_timesteps      | 950272       |
| train/                  |              |
|    approx_kl            | 0.0053039705 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.662       |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0398      |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.112        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=959232, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.44
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 59 completed in 164.78 seconds.
Time to collect_rollouts 164.79
Training model
Time to train 48.38
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.49         |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 59           |
|    total_timesteps      | 966656       |
| train/                  |              |
|    approx_kl            | 0.0051724697 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.666       |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0391      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.106        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=975488, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 60 completed in 165.14 seconds.
Time to collect_rollouts 165.15
Training model
Time to train 47.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28        |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 60          |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.004844892 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0509     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.109       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=991744, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 61 completed in 164.21 seconds.
Time to collect_rollouts 164.21
Training model
Time to train 47.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 61          |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.005080575 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0252     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.104       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1008000, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 62 completed in 167.38 seconds.
Time to collect_rollouts 167.39
Training model
Time to train 45.61
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.97      |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 73        |
|    iterations           | 62        |
|    total_timesteps      | 1015808   |
| train/                  |           |
|    approx_kl            | 0.0050064 |
|    clip_fraction        | 0.0328    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.653    |
|    explained_variance   | 0.397     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0177   |
|    n_updates            | 620       |
|    policy_gradient_loss | -0.0111   |
|    value_loss           | 0.105     |
---------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1024256, episode_reward=0.79 +/- 0.41
Episode length: 1.98 +/- 0.45
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 63 completed in 165.83 seconds.
Time to collect_rollouts 165.83
Training model
Time to train 32.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.7          |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 63           |
|    total_timesteps      | 1032192      |
| train/                  |              |
|    approx_kl            | 0.0055650533 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.042       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 0.107        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1040512, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 64 completed in 160.05 seconds.
Time to collect_rollouts 160.05
Training model
Time to train 43.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 64           |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0049606003 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.642       |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0356      |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.104        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1056768, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 65 completed in 162.39 seconds.
Time to collect_rollouts 162.4
Training model
Time to train 48.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57        |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 65          |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.005446605 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0398     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.102       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1073024, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 66 completed in 166.04 seconds.
Time to collect_rollouts 166.04
Training model
Time to train 46.4
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 66          |
|    total_timesteps      | 1081344     |
| train/                  |             |
|    approx_kl            | 0.004990229 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0.431       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0446     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0998      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1089280, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 67 completed in 164.13 seconds.
Time to collect_rollouts 164.14
Training model
Time to train 35.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.81         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 67           |
|    total_timesteps      | 1097728      |
| train/                  |              |
|    approx_kl            | 0.0053733466 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.432        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0372      |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 0.0973       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1105536, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 68 completed in 161.73 seconds.
Time to collect_rollouts 161.74
Training model
Time to train 30.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.99         |
|    ep_rew_mean          | 0.37         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 68           |
|    total_timesteps      | 1114112      |
| train/                  |              |
|    approx_kl            | 0.0053848457 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.443        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0168      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.0958       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1121792, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 69 completed in 164.03 seconds.
Time to collect_rollouts 164.03
Training model
Time to train 29.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 69          |
|    total_timesteps      | 1130496     |
| train/                  |             |
|    approx_kl            | 0.005400693 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.102       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1138048, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 70 completed in 164.20 seconds.
Time to collect_rollouts 164.2
Training model
Time to train 33.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.77        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 70          |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.004860115 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.0959      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1154304, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 71 completed in 167.64 seconds.
Time to collect_rollouts 167.64
Training model
Time to train 48.38
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.53         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 71           |
|    total_timesteps      | 1163264      |
| train/                  |              |
|    approx_kl            | 0.0055525396 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.458        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0587      |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.0967       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1170560, episode_reward=0.79 +/- 0.41
Episode length: 1.95 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 72 completed in 162.43 seconds.
Time to collect_rollouts 162.43
Training model
Time to train 48.1
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 72          |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.005193994 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0397     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0966      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1186816, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 73 completed in 158.52 seconds.
Time to collect_rollouts 158.53
Training model
Time to train 47.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.88         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 73           |
|    total_timesteps      | 1196032      |
| train/                  |              |
|    approx_kl            | 0.0056986082 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.595       |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0376      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 0.0943       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1203072, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 74 completed in 161.80 seconds.
Improved rollout/ep_rew_mean to 0.4700
Time to collect_rollouts 161.88
Training model
Time to train 47.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.63         |
|    ep_rew_mean          | 0.47         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 74           |
|    total_timesteps      | 1212416      |
| train/                  |              |
|    approx_kl            | 0.0055314368 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.595       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0329      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.0935       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1219328, episode_reward=0.79 +/- 0.41
Episode length: 1.95 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 75 completed in 163.08 seconds.
Time to collect_rollouts 163.09
Training model
Time to train 47.39
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.98         |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 75           |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0055910554 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0337      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 0.0906       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1235584, episode_reward=0.79 +/- 0.41
Episode length: 1.95 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 76 completed in 166.96 seconds.
Time to collect_rollouts 166.96
Training model
Time to train 48.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.31         |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 76           |
|    total_timesteps      | 1245184      |
| train/                  |              |
|    approx_kl            | 0.0052496805 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0495      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.0944       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1251840, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.38
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 77 completed in 164.07 seconds.
Time to collect_rollouts 164.07
Training model
Time to train 48.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.74        |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 77          |
|    total_timesteps      | 1261568     |
| train/                  |             |
|    approx_kl            | 0.005692355 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0361     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0878      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
Collecting rollouts: 50/128 steps
---------------evaluation started---------------
Eval num_timesteps=1268096, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 78 completed in 160.62 seconds.
Time to collect_rollouts 160.62
Training model
Time to train 48.3
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.69         |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 78           |
|    total_timesteps      | 1277952      |
| train/                  |              |
|    approx_kl            | 0.0056430134 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.478        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0463      |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.0888       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1284352, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 79 completed in 159.51 seconds.
Time to collect_rollouts 159.52
Training model
Time to train 48.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 79          |
|    total_timesteps      | 1294336     |
| train/                  |             |
|    approx_kl            | 0.005436997 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0897      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1300608, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 80 completed in 160.23 seconds.
Time to collect_rollouts 160.24
Training model
Time to train 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.71        |
|    ep_rew_mean          | 0.44        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 80          |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.005588959 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0439     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.089       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1316864, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.42
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 81 completed in 164.97 seconds.
Time to collect_rollouts 164.98
Training model
Time to train 48.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.58        |
|    ep_rew_mean          | 0.36        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 81          |
|    total_timesteps      | 1327104     |
| train/                  |             |
|    approx_kl            | 0.005637766 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0409     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0884      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1333120, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 82 completed in 163.76 seconds.
Time to collect_rollouts 163.77
Training model
Time to train 48.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.71         |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 82           |
|    total_timesteps      | 1343488      |
| train/                  |              |
|    approx_kl            | 0.0056782505 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.042       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.0887       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1349376, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 83 completed in 159.81 seconds.
Time to collect_rollouts 159.81
Training model
Time to train 48.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.43         |
|    ep_rew_mean          | 0.41         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 83           |
|    total_timesteps      | 1359872      |
| train/                  |              |
|    approx_kl            | 0.0059365947 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0368      |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.0855       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1365632, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 84 completed in 164.95 seconds.
Time to collect_rollouts 164.96
Training model
Time to train 48.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.79        |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 84          |
|    total_timesteps      | 1376256     |
| train/                  |             |
|    approx_kl            | 0.005394649 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0819      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1381888, episode_reward=0.79 +/- 0.41
Episode length: 1.96 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 85 completed in 163.20 seconds.
Time to collect_rollouts 163.21
Training model
Time to train 48.84
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.21         |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 85           |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0053361976 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.489        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0476      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 0.0871       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1398144, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 86 completed in 164.74 seconds.
Time to collect_rollouts 164.74
Training model
Time to train 48.9
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.19        |
|    ep_rew_mean          | 0.45        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 86          |
|    total_timesteps      | 1409024     |
| train/                  |             |
|    approx_kl            | 0.005311173 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.039      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0825      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1414400, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.43
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 87 completed in 163.91 seconds.
Time to collect_rollouts 163.92
Training model
Time to train 48.81
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.19         |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 87           |
|    total_timesteps      | 1425408      |
| train/                  |              |
|    approx_kl            | 0.0053744437 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0493      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.0846       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1430656, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 88 completed in 160.70 seconds.
Time to collect_rollouts 160.71
Training model
Time to train 48.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.6          |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 88           |
|    total_timesteps      | 1441792      |
| train/                  |              |
|    approx_kl            | 0.0058594802 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | 0.478        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0309      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.0135      |
|    value_loss           | 0.084        |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1446912, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.40
New best mean reward!
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 89 completed in 163.06 seconds.
Improved rollout/ep_rew_mean to 0.4900
Time to collect_rollouts 163.14
Training model
Time to train 49.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48        |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 89          |
|    total_timesteps      | 1458176     |
| train/                  |             |
|    approx_kl            | 0.005445145 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0814      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1463168, episode_reward=0.80 +/- 0.40
Episode length: 1.96 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 90 completed in 162.76 seconds.
Time to collect_rollouts 162.77
Training model
Time to train 48.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.46         |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 90           |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0052690734 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.589       |
|    explained_variance   | 0.512        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0383      |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.0813       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1479424, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 91 completed in 159.64 seconds.
Time to collect_rollouts 159.65
Training model
Time to train 48.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73         |
|    ep_rew_mean          | 0.36         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 91           |
|    total_timesteps      | 1490944      |
| train/                  |              |
|    approx_kl            | 0.0057269465 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.519        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0366      |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.0808       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1495680, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.40
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 92 completed in 164.80 seconds.
Time to collect_rollouts 164.8
Training model
Time to train 49.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56        |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 92          |
|    total_timesteps      | 1507328     |
| train/                  |             |
|    approx_kl            | 0.005615386 |
|    clip_fraction        | 0.0383      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.079       |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1511936, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 93 completed in 160.74 seconds.
Time to collect_rollouts 160.75
Training model
Time to train 48.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.78         |
|    ep_rew_mean          | 0.38         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 93           |
|    total_timesteps      | 1523712      |
| train/                  |              |
|    approx_kl            | 0.0056233183 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.597       |
|    explained_variance   | 0.515        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0518      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.0796       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1528192, episode_reward=0.80 +/- 0.40
Episode length: 1.97 +/- 0.41
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 94 completed in 165.46 seconds.
Time to collect_rollouts 165.47
Training model
Time to train 48.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54        |
|    ep_rew_mean          | 0.44        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 94          |
|    total_timesteps      | 1540096     |
| train/                  |             |
|    approx_kl            | 0.005508123 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0788      |
-----------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1544448, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.39
---------------evaluation finished---------------
Collecting rollouts: 50/128 steps
Collecting rollouts: 75/128 steps
Collecting rollouts: 100/128 steps
Collecting rollouts: 125/128 steps
Epoch 95 completed in 166.42 seconds.
Time to collect_rollouts 166.42
Training model
Time to train 49.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.11         |
|    ep_rew_mean          | 0.33         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 95           |
|    total_timesteps      | 1556480      |
| train/                  |              |
|    approx_kl            | 0.0056197634 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.525        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0262      |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.0771       |
------------------------------------------
Collecting rollouts
Collecting rollouts: 0/128 steps
Collecting rollouts: 25/128 steps
---------------evaluation started---------------
Eval num_timesteps=1560704, episode_reward=0.79 +/- 0.41
Episode length: 1.97 +/- 0.40
---------------evaluation finished---------------
