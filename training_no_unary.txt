castellanoontiv@MSI:~/Batched_env$ /home/castellanoontiv/miniconda3/envs/rl/bin/python /home/castellanoontiv/Batched_env/runner.py
Using device: cuda


============================================================
Experiment 1/1
============================================================


============================================================
Seed 0 in [0]
============================================================

Run vars: countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl 
 {'atom_embedder': 'transe', 'atom_embedding_size': 250, 'batch_size': 8192, 'batch_size_env': 128, 'batch_size_env_eval': 128, 'canonical_action_order': False, 'clip_range': 0.2, 'clip_range_vf': None, 'constant_embedding_size': 250, 'corruption_mode': 'dynamic', 'corruption_scheme': ['tail'], 'data_path': './data/', 'dataset_name': 'countries_s3', 'debug_ppo': False, 'depth_info': True, 'deterministic': False, 'device': 'cuda', 'dropout_prob': 0.1, 'end_proof_action': True, 'ent_coef': 0.1, 'ent_coef_decay': True, 'ent_coef_end': 1.0, 'ent_coef_final_value': 0.01, 'ent_coef_init_value': 0.01, 'ent_coef_start': 0.0, 'ent_coef_transform': 'linear', 'eval_best_metric': 'mrr', 'eval_freq': 1024, 'eval_neg_samples': None, 'extended_eval_info': True, 'facts_file': 'train.txt', 'gae_lambda': 0.95, 'gamma': 0.99, 'hidden_dim': 128, 'learn_embeddings': True, 'load_best_metric': 'eval', 'load_depth_info': True, 'load_model': False, 'logger_path': './runs/', 'lr': 3e-05, 'lr_decay': True, 'lr_end': 1.0, 'lr_final_value': 1e-06, 'lr_init_value': 3e-05, 'lr_start': 0.0, 'lr_transform': 'linear', 'max_depth': 20, 'max_grad_norm': 0.5, 'max_total_vars': 100, 'memory_pruning': True, 'min_gpu_memory_gb': 2.0, 'model_name': 'PPO', 'models_path': 'models/', 'n_epochs': 5, 'n_eval_queries': None, 'n_steps': 256, 'n_test_queries': None, 'n_train_queries': None, 'num_layers': 8, 'padding_atoms': 6, 'padding_states': 20, 'plot': False, 'plot_trajectories': False, 'predicate_embedding_size': 250, 'profile': False, 'prover_verbose': False, 'restore_best_val_model': True, 'reward_type': 4, 'rollout_device': None, 'rules_file': 'rules.txt', 'run_signature': 'countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl', 'sample_deterministic_per_env': False, 'save_model': True, 'seed': [0], 'seed_run_i': 0, 'skip_unary_actions': True, 'sqrt_scale': False, 'state_embedder': 'mean', 'state_embedding_size': 250, 'target_kl': 0.07, 'temperature': 0.1, 'test_depth': None, 'test_file': 'test.txt', 'test_neg_samples': None, 'timesteps_train': 1000000, 'train_depth': {1, 2, 3, 4, 5, 6}, 'train_file': 'train_depths.txt', 'train_neg_ratio': 1, 'use_amp': True, 'use_compile': True, 'use_exact_memory': False, 'use_l2_norm': True, 'use_logger': True, 'use_wb': False, 'valid_depth': None, 'valid_file': 'valid.txt', 'verbose': False, 'verbose_cb': False, 'verbose_env': 0, 'verbose_prover': 0, 'vf_coef': 2.0, 'wb_path': './../wandb/'} 

Device: cuda. CUDA available: True, Device count: 1
Queries loaded - Train: 33/33, Valid: 24, Test: 24
[Create Environments] Predicate Mapping: True=5, False=6, Endf=7
LR Decay: 3e-05 -> 1e-06
Entropy Decay: 0.01 -> 0.01
[PPO] AMP enabled with bfloat16 (no GradScaler required)
[PPO] Device sync optimization: _same_device=True (self.device=cuda:0, env.device=cuda:0)
[PPO] Compiled self.policy with mode='reduce-overhead', fullgraph=True

============================================================
Initial evaluation (untrained model)
============================================================
Initial MRR: 0.9042
Initial Hits@1: 0.8333
Initial success_rate: 0.7917
============================================================

[MetricsCallback] Training started

[PPO] Starting training for 1000000 timesteps
[PPO] Rollout size: 256 steps x 128 envs = 32768 samples per rollout

[PPO] ===== Iteration 1 (0/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.66s
[PPO] Recent episodes: reward=0.600, length=7.2

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 4.41567, value 2.44633, policy 0.00016, entropy -0.72935, approx_kl 0.00010 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 3.81729, value 2.25180, policy 0.00023, entropy -0.72918, approx_kl 0.00028 clip_fraction 0.00002. 
Epoch 3/5. 
Losses: total 3.37663, value 2.09020, policy 0.00031, entropy -0.72889, approx_kl 0.00065 clip_fraction 0.00040. 
Epoch 4/5. 
Losses: total 2.98862, value 1.95622, policy 0.00042, entropy -0.72857, approx_kl 0.00119 clip_fraction 0.00150. 
Epoch 5/5. 
Losses: total 2.68616, value 1.84373, policy 0.00040, entropy -0.72801, approx_kl 0.00171 clip_fraction 0.00380. 
[PPO] Values: min=-3.750, max=5.844, mean=1.434, std=1.486
[PPO] Returns: min=-1.821, max=4.277, mean=0.752, std=0.732
[PPO] Explained variance: -3.1062
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.002                    |
|    clip_fraction        | 0.004                    |
|    entropy              | 0.728                    |
|    explained_var        | -3.106                   |
|    iterations           | 1                        |
|    policy_loss          | 0.000                    |
|    total_timesteps      | 32768                    |
|    value_loss           | 1.844                    |
----------------------------------------------------

[PPO] Training completed in 5.97s
[PPO] Metrics: policy_loss=0.0004, value_loss=1.8437, entropy=0.7280
[Metrics] Reward (train): current=0.642, best=0.642 (iter 1)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.196                    |
|    ep_rew_mean          | 0.642                    |
|    len                  | 6.196 +/- 5.95 (5192)    |
|    len_d_1_pos          | 2.114 +/- 1.15 (1022)    |
|    len_d_2_pos          | 5.723 +/- 4.14 (1224)    |
|    len_d_3_pos          | 9.733 +/- 5.51 (255)     |
|    len_d_4_pos          | 10.421 +/- 5.28 (133)    |
|    len_d_unknown_neg    | 7.481 +/- 6.93 (2558)    |
|    len_neg              | 7.481 +/- 6.93 (2558)    |
|    len_pos              | 4.948 +/- 4.46 (2634)    |
|    proven_d_1_pos       | 0.940 +/- 0.24 (1022)    |
|    proven_d_2_pos       | 0.722 +/- 0.45 (1224)    |
|    proven_d_3_pos       | 0.396 +/- 0.49 (255)     |
|    proven_d_4_pos       | 0.338 +/- 0.47 (133)     |
|    proven_d_unknown_neg | 0.112 +/- 0.32 (2558)    |
|    proven_neg           | 0.112 +/- 0.32 (2558)    |
|    proven_pos           | 0.756 +/- 0.43 (2634)    |
|    reward               | 0.642 +/- 0.77 (5192)    |
|    reward_d_1_pos       | 0.881 +/- 0.47 (1022)    |
|    reward_d_2_pos       | 0.444 +/- 0.90 (1224)    |
|    reward_d_3_pos       | -0.208 +/- 0.98 (255)    |
|    reward_d_4_pos       | -0.323 +/- 0.95 (133)    |
|    reward_d_unknown_neg | 0.776 +/- 0.63 (2558)    |
|    reward_neg           | 0.776 +/- 0.63 (2558)    |
|    reward_pos           | 0.512 +/- 0.86 (2634)    |
|    success_rate         | 0.439                    |
|    time/elapsed         | 28                       |
|    time/fps             | 1143                     |
|    total_timesteps      | 32768                    |
----------------------------------------------------

/home/castellanoontiv/Batched_env/model_eval.py:638: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)
  return Display._format_stat_string(t.mean().item(), t.std().item(), t.numel())
[Ranking] MRR: current=0.904, best=0.904 (iter 1)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.833                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.904                    |
|    _mrr                 | 0.904                    |
|    ep_len_mean          | 14.617                   |
|    ep_rew_mean          | 0.800                    |
|    len                  | 14.617 +/- 6.23 (120)    |
|    len_d_1_pos          | 6.300 +/- 7.39 (10)      |
|    len_d_2_pos          | 6.917 +/- 3.12 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 18.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.510 +/- 4.78 (96)     |
|    len_neg              | 16.510 +/- 4.78 (96)     |
|    len_pos              | 7.042 +/- 5.63 (24)      |
|    mrr_mean             | 0.904                    |
|    proven_d_1_pos       | 0.600 +/- 0.52 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.073 +/- 0.26 (96)      |
|    proven_neg           | 0.073 +/- 0.26 (96)      |
|    proven_pos           | 0.792 +/- 0.41 (24)      |
|    reward               | 0.583 +/- 0.83 (24)      |
|    reward_d_1_pos       | 0.200 +/- 1.03 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.854 +/- 0.52 (96)      |
|    reward_neg           | 0.854 +/- 0.52 (96)      |
|    reward_pos           | 0.583 +/- 0.83 (24)      |
|    success_rate         | 0.792                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.8000)
[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=0.9042)

[PPO] ===== Iteration 2 (32768/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.83s
[PPO] Recent episodes: reward=1.000, length=5.3

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 2.30313, value 1.21280, policy 0.00005, entropy -0.72790, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 2.10155, value 1.15021, policy -0.00013, entropy -0.72687, approx_kl 0.00035 clip_fraction 0.00011. 
Epoch 3/5. 
Losses: total 1.87010, value 1.09436, policy -0.00026, entropy -0.72580, approx_kl 0.00079 clip_fraction 0.00109. 
Epoch 4/5. 
Losses: total 1.71018, value 1.04359, policy -0.00038, entropy -0.72474, approx_kl 0.00145 clip_fraction 0.00440. 
Epoch 5/5. 
Losses: total 1.57445, value 0.99728, policy -0.00044, entropy -0.72393, approx_kl 0.00212 clip_fraction 0.00815. 
[PPO] Values: min=-3.219, max=3.969, mean=0.607, std=1.048
[PPO] Returns: min=-2.157, max=3.307, mean=0.565, std=0.670
[PPO] Explained variance: -1.8108
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.002                    |
|    clip_fraction        | 0.008                    |
|    entropy              | 0.724                    |
|    explained_var        | -1.811                   |
|    iterations           | 2                        |
|    policy_loss          | -0.000                   |
|    total_timesteps      | 65536                    |
|    value_loss           | 0.997                    |
----------------------------------------------------

[PPO] Training completed in 5.63s
[PPO] Metrics: policy_loss=-0.0004, value_loss=0.9973, entropy=0.7239
[Metrics] Reward (train): current=0.646, best=0.646 (iter 2)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.397                    |
|    ep_rew_mean          | 0.646                    |
|    len                  | 6.397 +/- 6.09 (10138)   |
|    len_d_1_pos          | 2.128 +/- 1.14 (1991)    |
|    len_d_2_pos          | 5.930 +/- 4.39 (2352)    |
|    len_d_3_pos          | 9.708 +/- 5.67 (487)     |
|    len_d_4_pos          | 10.949 +/- 5.29 (274)    |
|    len_d_unknown_neg    | 7.736 +/- 7.02 (5034)    |
|    len_neg              | 7.736 +/- 7.02 (5034)    |
|    len_pos              | 5.077 +/- 4.65 (5104)    |
|    proven_d_1_pos       | 0.942 +/- 0.23 (1991)    |
|    proven_d_2_pos       | 0.734 +/- 0.44 (2352)    |
|    proven_d_3_pos       | 0.405 +/- 0.49 (487)     |
|    proven_d_4_pos       | 0.365 +/- 0.48 (274)     |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (5034)    |
|    proven_neg           | 0.117 +/- 0.32 (5034)    |
|    proven_pos           | 0.764 +/- 0.42 (5104)    |
|    reward               | 0.646 +/- 0.76 (10138)   |
|    reward_d_1_pos       | 0.883 +/- 0.47 (1991)    |
|    reward_d_2_pos       | 0.469 +/- 0.88 (2352)    |
|    reward_d_3_pos       | -0.191 +/- 0.98 (487)    |
|    reward_d_4_pos       | -0.270 +/- 0.96 (274)    |
|    reward_d_unknown_neg | 0.765 +/- 0.64 (5034)    |
|    reward_neg           | 0.765 +/- 0.64 (5034)    |
|    reward_pos           | 0.528 +/- 0.85 (5104)    |
|    success_rate         | 0.443                    |
|    time/elapsed         | 58                       |
|    time/fps             | 1085                     |
|    total_timesteps      | 65536                    |
----------------------------------------------------

[Ranking] MRR: current=0.904, best=0.904 (iter 1)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.833                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.904                    |
|    _mrr                 | 0.904                    |
|    ep_len_mean          | 14.333                   |
|    ep_rew_mean          | 0.800                    |
|    len                  | 14.333 +/- 6.17 (120)    |
|    len_d_1_pos          | 6.500 +/- 7.29 (10)      |
|    len_d_2_pos          | 6.750 +/- 3.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 18.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.156 +/- 4.84 (96)     |
|    len_neg              | 16.156 +/- 4.84 (96)     |
|    len_pos              | 7.042 +/- 5.57 (24)      |
|    mrr_mean             | 0.904                    |
|    proven_d_1_pos       | 0.700 +/- 0.48 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.083 +/- 0.28 (96)      |
|    proven_neg           | 0.083 +/- 0.28 (96)      |
|    proven_pos           | 0.833 +/- 0.38 (24)      |
|    reward               | 0.667 +/- 0.76 (24)      |
|    reward_d_1_pos       | 0.400 +/- 0.97 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.833 +/- 0.56 (96)      |
|    reward_neg           | 0.833 +/- 0.56 (96)      |
|    reward_pos           | 0.667 +/- 0.76 (24)      |
|    success_rate         | 0.833                    |
----------------------------------------------------


[PPO] ===== Iteration 3 (65536/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.47s
[PPO] Recent episodes: reward=0.600, length=4.3

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 1.41142, value 0.72805, policy -0.00003, entropy -0.71609, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 1.33675, value 0.70329, policy -0.00032, entropy -0.71552, approx_kl 0.00019 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 1.24017, value 0.68123, policy -0.00064, entropy -0.71464, approx_kl 0.00041 clip_fraction 0.00005. 
Epoch 4/5. 
Losses: total 1.14470, value 0.66053, policy -0.00104, entropy -0.71344, approx_kl 0.00086 clip_fraction 0.00169. 
Epoch 5/5. 
Losses: total 1.04134, value 0.64056, policy -0.00132, entropy -0.71216, approx_kl 0.00141 clip_fraction 0.00463. 
[PPO] Values: min=-2.594, max=3.594, mean=0.550, std=0.717
[PPO] Returns: min=-1.327, max=2.083, mean=0.589, std=0.612
[PPO] Explained variance: -0.9960
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.005                    |
|    entropy              | 0.712                    |
|    explained_var        | -0.996                   |
|    iterations           | 3                        |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 98304                    |
|    value_loss           | 0.641                    |
----------------------------------------------------

[PPO] Training completed in 5.83s
[PPO] Metrics: policy_loss=-0.0013, value_loss=0.6406, entropy=0.7122
[Metrics] Reward (train): current=0.659, best=0.659 (iter 3)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.484                    |
|    ep_rew_mean          | 0.659                    |
|    len                  | 6.484 +/- 6.16 (15058)   |
|    len_d_1_pos          | 2.125 +/- 1.10 (2956)    |
|    len_d_2_pos          | 6.034 +/- 4.41 (3513)    |
|    len_d_3_pos          | 10.102 +/- 5.63 (689)    |
|    len_d_4_pos          | 10.978 +/- 5.02 (412)    |
|    len_d_unknown_neg    | 7.837 +/- 7.12 (7488)    |
|    len_neg              | 7.837 +/- 7.12 (7488)    |
|    len_pos              | 5.147 +/- 4.66 (7570)    |
|    proven_d_1_pos       | 0.937 +/- 0.24 (2956)    |
|    proven_d_2_pos       | 0.751 +/- 0.43 (3513)    |
|    proven_d_3_pos       | 0.438 +/- 0.50 (689)     |
|    proven_d_4_pos       | 0.383 +/- 0.49 (412)     |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (7488)    |
|    proven_neg           | 0.116 +/- 0.32 (7488)    |
|    proven_pos           | 0.775 +/- 0.42 (7570)    |
|    reward               | 0.659 +/- 0.75 (15058)   |
|    reward_d_1_pos       | 0.873 +/- 0.49 (2956)    |
|    reward_d_2_pos       | 0.503 +/- 0.86 (3513)    |
|    reward_d_3_pos       | -0.123 +/- 0.99 (689)    |
|    reward_d_4_pos       | -0.233 +/- 0.97 (412)    |
|    reward_d_unknown_neg | 0.768 +/- 0.64 (7488)    |
|    reward_neg           | 0.768 +/- 0.64 (7488)    |
|    reward_pos           | 0.551 +/- 0.83 (7570)    |
|    success_rate         | 0.447                    |
|    time/elapsed         | 88                       |
|    time/fps             | 1094                     |
|    total_timesteps      | 98304                    |
----------------------------------------------------

[Ranking] MRR: current=0.897, best=0.904 (iter 1)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.833                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.897                    |
|    _mrr                 | 0.897                    |
|    ep_len_mean          | 14.617                   |
|    ep_rew_mean          | 0.783                    |
|    len                  | 14.617 +/- 6.19 (120)    |
|    len_d_1_pos          | 6.500 +/- 7.29 (10)      |
|    len_d_2_pos          | 7.917 +/- 4.98 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 16.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.385 +/- 4.83 (96)     |
|    len_neg              | 16.385 +/- 4.83 (96)     |
|    len_pos              | 7.542 +/- 6.05 (24)      |
|    mrr_mean             | 0.897                    |
|    proven_d_1_pos       | 0.700 +/- 0.48 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.083 +/- 0.28 (96)      |
|    proven_neg           | 0.083 +/- 0.28 (96)      |
|    proven_pos           | 0.792 +/- 0.41 (24)      |
|    reward               | 0.583 +/- 0.83 (24)      |
|    reward_d_1_pos       | 0.400 +/- 0.97 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.833 +/- 0.56 (96)      |
|    reward_neg           | 0.833 +/- 0.56 (96)      |
|    reward_pos           | 0.583 +/- 0.83 (24)      |
|    success_rate         | 0.792                    |
----------------------------------------------------


[PPO] ===== Iteration 4 (98304/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 18.96s
[PPO] Recent episodes: reward=0.600, length=5.8

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 1.00568, value 0.53334, policy -0.00027, entropy -0.71010, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.96981, value 0.51739, policy -0.00088, entropy -0.70863, approx_kl 0.00028 clip_fraction 0.00012. 
Epoch 3/5. 
Losses: total 0.92484, value 0.50304, policy -0.00134, entropy -0.70682, approx_kl 0.00074 clip_fraction 0.00311. 
Epoch 4/5. 
Losses: total 0.87573, value 0.49003, policy -0.00164, entropy -0.70545, approx_kl 0.00118 clip_fraction 0.00598. 
Epoch 5/5. 
Losses: total 0.84960, value 0.47867, policy -0.00188, entropy -0.70453, approx_kl 0.00145 clip_fraction 0.00813. 
[PPO] Values: min=-2.094, max=2.875, mean=0.578, std=0.552
[PPO] Returns: min=-1.000, max=2.295, mean=0.576, std=0.613
[PPO] Explained variance: -0.4545
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.008                    |
|    entropy              | 0.705                    |
|    explained_var        | -0.455                   |
|    iterations           | 4                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 131072                   |
|    value_loss           | 0.479                    |
----------------------------------------------------

[PPO] Training completed in 6.73s
[PPO] Metrics: policy_loss=-0.0019, value_loss=0.4787, entropy=0.7045
[Metrics] Reward (train): current=0.666, best=0.666 (iter 4)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.618                    |
|    ep_rew_mean          | 0.666                    |
|    len                  | 6.618 +/- 6.26 (19691)   |
|    len_d_1_pos          | 2.140 +/- 1.14 (3865)    |
|    len_d_2_pos          | 6.150 +/- 4.48 (4552)    |
|    len_d_3_pos          | 10.337 +/- 5.55 (900)    |
|    len_d_4_pos          | 11.319 +/- 5.02 (562)    |
|    len_d_unknown_neg    | 7.989 +/- 7.22 (9812)    |
|    len_neg              | 7.989 +/- 7.22 (9812)    |
|    len_pos              | 5.257 +/- 4.75 (9879)    |
|    proven_d_1_pos       | 0.939 +/- 0.24 (3865)    |
|    proven_d_2_pos       | 0.761 +/- 0.43 (4552)    |
|    proven_d_3_pos       | 0.434 +/- 0.50 (900)     |
|    proven_d_4_pos       | 0.407 +/- 0.49 (562)     |
|    proven_d_unknown_neg | 0.114 +/- 0.32 (9812)    |
|    proven_neg           | 0.114 +/- 0.32 (9812)    |
|    proven_pos           | 0.781 +/- 0.41 (9879)    |
|    reward               | 0.666 +/- 0.75 (19691)   |
|    reward_d_1_pos       | 0.878 +/- 0.48 (3865)    |
|    reward_d_2_pos       | 0.522 +/- 0.85 (4552)    |
|    reward_d_3_pos       | -0.131 +/- 0.99 (900)    |
|    reward_d_4_pos       | -0.185 +/- 0.98 (562)    |
|    reward_d_unknown_neg | 0.772 +/- 0.64 (9812)    |
|    reward_neg           | 0.772 +/- 0.64 (9812)    |
|    reward_pos           | 0.562 +/- 0.83 (9879)    |
|    success_rate         | 0.449                    |
|    time/elapsed         | 118                      |
|    time/fps             | 1119                     |
|    total_timesteps      | 131072                   |
----------------------------------------------------

[Ranking] MRR: current=0.894, best=0.904 (iter 1)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.833                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.894                    |
|    _mrr                 | 0.894                    |
|    ep_len_mean          | 14.425                   |
|    ep_rew_mean          | 0.783                    |
|    len                  | 14.425 +/- 6.17 (120)    |
|    len_d_1_pos          | 6.500 +/- 7.29 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 16.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.188 +/- 4.80 (96)     |
|    len_neg              | 16.188 +/- 4.80 (96)     |
|    len_pos              | 7.375 +/- 6.09 (24)      |
|    mrr_mean             | 0.894                    |
|    proven_d_1_pos       | 0.700 +/- 0.48 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.083 +/- 0.28 (96)      |
|    proven_neg           | 0.083 +/- 0.28 (96)      |
|    proven_pos           | 0.792 +/- 0.41 (24)      |
|    reward               | 0.583 +/- 0.83 (24)      |
|    reward_d_1_pos       | 0.400 +/- 0.97 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.833 +/- 0.56 (96)      |
|    reward_neg           | 0.833 +/- 0.56 (96)      |
|    reward_pos           | 0.583 +/- 0.83 (24)      |
|    success_rate         | 0.792                    |
----------------------------------------------------


[PPO] ===== Iteration 5 (131072/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 18.68s
[PPO] Recent episodes: reward=0.800, length=10.8

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.82792, value 0.41708, policy -0.00012, entropy -0.70652, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.79201, value 0.41014, policy -0.00039, entropy -0.70593, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.77095, value 0.40343, policy -0.00081, entropy -0.70496, approx_kl 0.00025 clip_fraction 0.00017. 
Epoch 4/5. 
Losses: total 0.72812, value 0.39763, policy -0.00118, entropy -0.70381, approx_kl 0.00046 clip_fraction 0.00057. 
Epoch 5/5. 
Losses: total 0.74399, value 0.39217, policy -0.00151, entropy -0.70262, approx_kl 0.00075 clip_fraction 0.00183. 
[PPO] Values: min=-1.859, max=2.438, mean=0.573, std=0.448
[PPO] Returns: min=-1.000, max=1.707, mean=0.586, std=0.596
[PPO] Explained variance: -0.1859
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.002                    |
|    entropy              | 0.703                    |
|    explained_var        | -0.186                   |
|    iterations           | 5                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 163840                   |
|    value_loss           | 0.392                    |
----------------------------------------------------

[PPO] Training completed in 5.80s
[PPO] Metrics: policy_loss=-0.0015, value_loss=0.3922, entropy=0.7026
[Metrics] Reward (train): current=0.675, best=0.675 (iter 5)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.753                    |
|    ep_rew_mean          | 0.675                    |
|    len                  | 6.753 +/- 6.35 (24152)   |
|    len_d_1_pos          | 2.149 +/- 1.18 (4736)    |
|    len_d_2_pos          | 6.210 +/- 4.49 (5588)    |
|    len_d_3_pos          | 10.532 +/- 5.48 (1109)   |
|    len_d_4_pos          | 11.580 +/- 5.04 (685)    |
|    len_d_unknown_neg    | 8.194 +/- 7.32 (12034)   |
|    len_neg              | 8.194 +/- 7.32 (12034)   |
|    len_pos              | 5.322 +/- 4.80 (12118)   |
|    proven_d_1_pos       | 0.940 +/- 0.24 (4736)    |
|    proven_d_2_pos       | 0.773 +/- 0.42 (5588)    |
|    proven_d_3_pos       | 0.442 +/- 0.50 (1109)    |
|    proven_d_4_pos       | 0.431 +/- 0.50 (685)     |
|    proven_d_unknown_neg | 0.114 +/- 0.32 (12034)   |
|    proven_neg           | 0.114 +/- 0.32 (12034)   |
|    proven_pos           | 0.789 +/- 0.41 (12118)   |
|    reward               | 0.675 +/- 0.74 (24152)   |
|    reward_d_1_pos       | 0.879 +/- 0.48 (4736)    |
|    reward_d_2_pos       | 0.547 +/- 0.84 (5588)    |
|    reward_d_3_pos       | -0.116 +/- 0.99 (1109)   |
|    reward_d_4_pos       | -0.139 +/- 0.99 (685)    |
|    reward_d_unknown_neg | 0.773 +/- 0.63 (12034)   |
|    reward_neg           | 0.773 +/- 0.63 (12034)   |
|    reward_pos           | 0.577 +/- 0.82 (12118)   |
|    success_rate         | 0.452                    |
|    time/elapsed         | 146                      |
|    time/fps             | 1171                     |
|    total_timesteps      | 163840                   |
----------------------------------------------------

[Ranking] MRR: current=0.915, best=0.915 (iter 5)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.915                    |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.317                   |
|    ep_rew_mean          | 0.850                    |
|    len                  | 14.317 +/- 6.25 (120)    |
|    len_d_1_pos          | 5.900 +/- 7.22 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 16.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.115 +/- 4.86 (96)     |
|    len_neg              | 16.115 +/- 4.86 (96)     |
|    len_pos              | 7.125 +/- 6.10 (24)      |
|    mrr_mean             | 0.915                    |
|    proven_d_1_pos       | 0.800 +/- 0.42 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.052 +/- 0.22 (96)      |
|    proven_neg           | 0.052 +/- 0.22 (96)      |
|    proven_pos           | 0.833 +/- 0.38 (24)      |
|    reward               | 0.667 +/- 0.76 (24)      |
|    reward_d_1_pos       | 0.600 +/- 0.84 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.896 +/- 0.45 (96)      |
|    reward_neg           | 0.896 +/- 0.45 (96)      |
|    reward_pos           | 0.667 +/- 0.76 (24)      |
|    success_rate         | 0.833                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.8500)
[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=0.9146)

[PPO] ===== Iteration 6 (163840/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.66s
[PPO] Recent episodes: reward=0.800, length=4.5

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.69206, value 0.35969, policy -0.00025, entropy -0.69828, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.69191, value 0.35540, policy -0.00062, entropy -0.69680, approx_kl 0.00016 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.65926, value 0.35104, policy -0.00105, entropy -0.69521, approx_kl 0.00036 clip_fraction 0.00094. 
Epoch 4/5. 
Losses: total 0.66174, value 0.34685, policy -0.00138, entropy -0.69371, approx_kl 0.00061 clip_fraction 0.00258. 
Epoch 5/5. 
Losses: total 0.63138, value 0.34282, policy -0.00168, entropy -0.69242, approx_kl 0.00081 clip_fraction 0.00380. 
[PPO] Values: min=-1.633, max=2.797, mean=0.594, std=0.397
[PPO] Returns: min=-1.000, max=1.606, mean=0.604, std=0.582
[PPO] Explained variance: -0.0699
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.004                    |
|    entropy              | 0.692                    |
|    explained_var        | -0.070                   |
|    iterations           | 6                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 196608                   |
|    value_loss           | 0.343                    |
----------------------------------------------------

[PPO] Training completed in 5.76s
[PPO] Metrics: policy_loss=-0.0017, value_loss=0.3428, entropy=0.6924
[Metrics] Reward (train): current=0.683, best=0.683 (iter 6)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.861                    |
|    ep_rew_mean          | 0.683                    |
|    len                  | 6.861 +/- 6.43 (28531)   |
|    len_d_1_pos          | 2.146 +/- 1.17 (5580)    |
|    len_d_2_pos          | 6.216 +/- 4.48 (6611)    |
|    len_d_3_pos          | 10.823 +/- 5.50 (1294)   |
|    len_d_4_pos          | 11.758 +/- 5.02 (822)    |
|    len_d_unknown_neg    | 8.366 +/- 7.41 (14224)   |
|    len_neg              | 8.366 +/- 7.41 (14224)   |
|    len_pos              | 5.364 +/- 4.84 (14307)   |
|    proven_d_1_pos       | 0.942 +/- 0.23 (5580)    |
|    proven_d_2_pos       | 0.788 +/- 0.41 (6611)    |
|    proven_d_3_pos       | 0.454 +/- 0.50 (1294)    |
|    proven_d_4_pos       | 0.448 +/- 0.50 (822)     |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (14224)   |
|    proven_neg           | 0.116 +/- 0.32 (14224)   |
|    proven_pos           | 0.798 +/- 0.40 (14307)   |
|    reward               | 0.683 +/- 0.73 (28531)   |
|    reward_d_1_pos       | 0.884 +/- 0.47 (5580)    |
|    reward_d_2_pos       | 0.577 +/- 0.82 (6611)    |
|    reward_d_3_pos       | -0.093 +/- 1.00 (1294)   |
|    reward_d_4_pos       | -0.105 +/- 0.99 (822)    |
|    reward_d_unknown_neg | 0.769 +/- 0.64 (14224)   |
|    reward_neg           | 0.769 +/- 0.64 (14224)   |
|    reward_pos           | 0.597 +/- 0.80 (14307)   |
|    success_rate         | 0.458                    |
|    time/elapsed         | 176                      |
|    time/fps             | 1061                     |
|    total_timesteps      | 196608                   |
----------------------------------------------------

[Ranking] MRR: current=0.915, best=0.915 (iter 5)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.915                    |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.458                   |
|    ep_rew_mean          | 0.867                    |
|    len                  | 14.458 +/- 6.33 (120)    |
|    len_d_1_pos          | 5.900 +/- 7.22 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 16.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.292 +/- 4.92 (96)     |
|    len_neg              | 16.292 +/- 4.92 (96)     |
|    len_pos              | 7.125 +/- 6.10 (24)      |
|    mrr_mean             | 0.915                    |
|    proven_d_1_pos       | 0.800 +/- 0.42 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.042 +/- 0.20 (96)      |
|    proven_neg           | 0.042 +/- 0.20 (96)      |
|    proven_pos           | 0.833 +/- 0.38 (24)      |
|    reward               | 0.667 +/- 0.76 (24)      |
|    reward_d_1_pos       | 0.600 +/- 0.84 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.917 +/- 0.40 (96)      |
|    reward_neg           | 0.917 +/- 0.40 (96)      |
|    reward_pos           | 0.667 +/- 0.76 (24)      |
|    success_rate         | 0.833                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.8667)

[PPO] ===== Iteration 7 (196608/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.38s
[PPO] Recent episodes: reward=0.800, length=8.3

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.63095, value 0.31712, policy -0.00009, entropy -0.68080, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.59268, value 0.31333, policy -0.00050, entropy -0.67976, approx_kl 0.00013 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.59538, value 0.30973, policy -0.00098, entropy -0.67847, approx_kl 0.00030 clip_fraction 0.00041. 
Epoch 4/5. 
Losses: total 0.57407, value 0.30631, policy -0.00137, entropy -0.67713, approx_kl 0.00059 clip_fraction 0.00163. 
Epoch 5/5. 
Losses: total 0.54891, value 0.30291, policy -0.00169, entropy -0.67590, approx_kl 0.00091 clip_fraction 0.00294. 
[PPO] Values: min=-1.453, max=2.328, mean=0.609, std=0.364
[PPO] Returns: min=-1.000, max=1.336, mean=0.622, std=0.569
[PPO] Explained variance: 0.0144
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.003                    |
|    entropy              | 0.676                    |
|    explained_var        | 0.014                    |
|    iterations           | 7                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 229376                   |
|    value_loss           | 0.303                    |
----------------------------------------------------

[PPO] Training completed in 5.74s
[PPO] Metrics: policy_loss=-0.0017, value_loss=0.3029, entropy=0.6759
[Metrics] Reward (train): current=0.691, best=0.691 (iter 7)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 6.926                    |
|    ep_rew_mean          | 0.691                    |
|    len                  | 6.926 +/- 6.48 (33003)   |
|    len_d_1_pos          | 2.143 +/- 1.17 (6472)    |
|    len_d_2_pos          | 6.260 +/- 4.50 (7627)    |
|    len_d_3_pos          | 10.967 +/- 5.46 (1484)   |
|    len_d_4_pos          | 11.859 +/- 4.96 (955)    |
|    len_d_unknown_neg    | 8.464 +/- 7.46 (16465)   |
|    len_neg              | 8.464 +/- 7.46 (16465)   |
|    len_pos              | 5.394 +/- 4.86 (16538)   |
|    proven_d_1_pos       | 0.944 +/- 0.23 (6472)    |
|    proven_d_2_pos       | 0.798 +/- 0.40 (7627)    |
|    proven_d_3_pos       | 0.468 +/- 0.50 (1484)    |
|    proven_d_4_pos       | 0.470 +/- 0.50 (955)     |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (16465)   |
|    proven_neg           | 0.116 +/- 0.32 (16465)   |
|    proven_pos           | 0.807 +/- 0.39 (16538)   |
|    reward               | 0.691 +/- 0.72 (33003)   |
|    reward_d_1_pos       | 0.888 +/- 0.46 (6472)    |
|    reward_d_2_pos       | 0.597 +/- 0.80 (7627)    |
|    reward_d_3_pos       | -0.065 +/- 1.00 (1484)   |
|    reward_d_4_pos       | -0.060 +/- 1.00 (955)    |
|    reward_d_unknown_neg | 0.769 +/- 0.64 (16465)   |
|    reward_neg           | 0.769 +/- 0.64 (16465)   |
|    reward_pos           | 0.614 +/- 0.79 (16538)   |
|    success_rate         | 0.462                    |
|    time/elapsed         | 208                      |
|    time/fps             | 1029                     |
|    total_timesteps      | 229376                   |
----------------------------------------------------

[Ranking] MRR: current=0.915, best=0.915 (iter 5)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.915                    |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.383                   |
|    ep_rew_mean          | 0.883                    |
|    len                  | 14.383 +/- 6.38 (120)    |
|    len_d_1_pos          | 5.900 +/- 7.22 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 18.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.177 +/- 5.03 (96)     |
|    len_neg              | 16.177 +/- 5.03 (96)     |
|    len_pos              | 7.208 +/- 6.24 (24)      |
|    mrr_mean             | 0.915                    |
|    proven_d_1_pos       | 0.800 +/- 0.42 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.833 +/- 0.38 (24)      |
|    reward               | 0.667 +/- 0.76 (24)      |
|    reward_d_1_pos       | 0.600 +/- 0.84 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.667 +/- 0.76 (24)      |
|    success_rate         | 0.833                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.8833)

[PPO] ===== Iteration 8 (229376/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.15s
[PPO] Recent episodes: reward=0.600, length=7.5

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.53421, value 0.27505, policy -0.00010, entropy -0.66810, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.52854, value 0.27222, policy -0.00046, entropy -0.66731, approx_kl 0.00014 clip_fraction 0.00003. 
Epoch 3/5. 
Losses: total 0.50277, value 0.26931, policy -0.00089, entropy -0.66630, approx_kl 0.00028 clip_fraction 0.00040. 
Epoch 4/5. 
Losses: total 0.51190, value 0.26641, policy -0.00128, entropy -0.66527, approx_kl 0.00049 clip_fraction 0.00151. 
Epoch 5/5. 
Losses: total 0.49646, value 0.26384, policy -0.00162, entropy -0.66445, approx_kl 0.00065 clip_fraction 0.00244. 
[PPO] Values: min=-1.219, max=2.297, mean=0.624, std=0.338
[PPO] Returns: min=-1.000, max=1.292, mean=0.633, std=0.562
[PPO] Explained variance: 0.1242
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.002                    |
|    entropy              | 0.664                    |
|    explained_var        | 0.124                    |
|    iterations           | 8                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 262144                   |
|    value_loss           | 0.264                    |
----------------------------------------------------

[PPO] Training completed in 5.82s
[PPO] Metrics: policy_loss=-0.0016, value_loss=0.2638, entropy=0.6645
[Metrics] Reward (train): current=0.698, best=0.698 (iter 8)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.006                    |
|    ep_rew_mean          | 0.698                    |
|    len                  | 7.006 +/- 6.53 (37305)   |
|    len_d_1_pos          | 2.137 +/- 1.14 (7308)    |
|    len_d_2_pos          | 6.260 +/- 4.49 (8615)    |
|    len_d_3_pos          | 11.080 +/- 5.38 (1679)   |
|    len_d_4_pos          | 12.093 +/- 4.98 (1092)   |
|    len_d_unknown_neg    | 8.598 +/- 7.52 (18611)   |
|    len_neg              | 8.598 +/- 7.52 (18611)   |
|    len_pos              | 5.422 +/- 4.88 (18694)   |
|    proven_d_1_pos       | 0.947 +/- 0.22 (7308)    |
|    proven_d_2_pos       | 0.810 +/- 0.39 (8615)    |
|    proven_d_3_pos       | 0.479 +/- 0.50 (1679)    |
|    proven_d_4_pos       | 0.476 +/- 0.50 (1092)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (18611)   |
|    proven_neg           | 0.116 +/- 0.32 (18611)   |
|    proven_pos           | 0.814 +/- 0.39 (18694)   |
|    reward               | 0.698 +/- 0.72 (37305)   |
|    reward_d_1_pos       | 0.894 +/- 0.45 (7308)    |
|    reward_d_2_pos       | 0.621 +/- 0.78 (8615)    |
|    reward_d_3_pos       | -0.041 +/- 1.00 (1679)   |
|    reward_d_4_pos       | -0.048 +/- 1.00 (1092)   |
|    reward_d_unknown_neg | 0.768 +/- 0.64 (18611)   |
|    reward_neg           | 0.768 +/- 0.64 (18611)   |
|    reward_pos           | 0.629 +/- 0.78 (18694)   |
|    success_rate         | 0.466                    |
|    time/elapsed         | 238                      |
|    time/fps             | 1106                     |
|    total_timesteps      | 262144                   |
----------------------------------------------------

[Ranking] MRR: current=0.915, best=0.915 (iter 5)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.915                    |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.175                   |
|    ep_rew_mean          | 0.900                    |
|    len                  | 14.175 +/- 6.35 (120)    |
|    len_d_1_pos          | 4.600 +/- 5.58 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 18.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.052 +/- 4.98 (96)     |
|    len_neg              | 16.052 +/- 4.98 (96)     |
|    len_pos              | 6.667 +/- 5.72 (24)      |
|    mrr_mean             | 0.915                    |
|    proven_d_1_pos       | 0.900 +/- 0.32 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.875 +/- 0.34 (24)      |
|    reward               | 0.750 +/- 0.68 (24)      |
|    reward_d_1_pos       | 0.800 +/- 0.63 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.750 +/- 0.68 (24)      |
|    success_rate         | 0.875                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.9000)

[PPO] ===== Iteration 9 (262144/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.04s
[PPO] Recent episodes: reward=0.600, length=3.3

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.51279, value 0.26393, policy -0.00016, entropy -0.65164, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.51722, value 0.26173, policy -0.00057, entropy -0.65106, approx_kl 0.00012 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.51001, value 0.25968, policy -0.00102, entropy -0.65021, approx_kl 0.00023 clip_fraction 0.00005. 
Epoch 4/5. 
Losses: total 0.48669, value 0.25783, policy -0.00145, entropy -0.64935, approx_kl 0.00039 clip_fraction 0.00040. 
Epoch 5/5. 
Losses: total 0.47542, value 0.25615, policy -0.00185, entropy -0.64851, approx_kl 0.00056 clip_fraction 0.00097. 
[PPO] Values: min=-0.961, max=2.781, mean=0.631, std=0.335
[PPO] Returns: min=-1.000, max=1.290, mean=0.640, std=0.563
[PPO] Explained variance: 0.1634
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.001                    |
|    clip_fraction        | 0.001                    |
|    entropy              | 0.649                    |
|    explained_var        | 0.163                    |
|    iterations           | 9                        |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 294912                   |
|    value_loss           | 0.256                    |
----------------------------------------------------

[PPO] Training completed in 5.75s
[PPO] Metrics: policy_loss=-0.0018, value_loss=0.2561, entropy=0.6485
[Metrics] Reward (train): current=0.705, best=0.705 (iter 9)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.040                    |
|    ep_rew_mean          | 0.705                    |
|    len                  | 7.040 +/- 6.56 (41787)   |
|    len_d_1_pos          | 2.139 +/- 1.13 (8172)    |
|    len_d_2_pos          | 6.253 +/- 4.47 (9643)    |
|    len_d_3_pos          | 11.166 +/- 5.33 (1903)   |
|    len_d_4_pos          | 12.208 +/- 4.92 (1218)   |
|    len_d_unknown_neg    | 8.646 +/- 7.56 (20851)   |
|    len_neg              | 8.646 +/- 7.56 (20851)   |
|    len_pos              | 5.440 +/- 4.89 (20936)   |
|    proven_d_1_pos       | 0.948 +/- 0.22 (8172)    |
|    proven_d_2_pos       | 0.821 +/- 0.38 (9643)    |
|    proven_d_3_pos       | 0.494 +/- 0.50 (1903)    |
|    proven_d_4_pos       | 0.486 +/- 0.50 (1218)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (20851)   |
|    proven_neg           | 0.117 +/- 0.32 (20851)   |
|    proven_pos           | 0.822 +/- 0.38 (20936)   |
|    reward               | 0.705 +/- 0.71 (41787)   |
|    reward_d_1_pos       | 0.896 +/- 0.44 (8172)    |
|    reward_d_2_pos       | 0.642 +/- 0.77 (9643)    |
|    reward_d_3_pos       | -0.012 +/- 1.00 (1903)   |
|    reward_d_4_pos       | -0.028 +/- 1.00 (1218)   |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (20851)   |
|    reward_neg           | 0.767 +/- 0.64 (20851)   |
|    reward_pos           | 0.643 +/- 0.77 (20936)   |
|    success_rate         | 0.470                    |
|    time/elapsed         | 270                      |
|    time/fps             | 1021                     |
|    total_timesteps      | 294912                   |
----------------------------------------------------

[Ranking] MRR: current=0.915, best=0.915 (iter 5)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.915                    |
|    _mrr                 | 0.915                    |
|    ep_len_mean          | 14.258                   |
|    ep_rew_mean          | 0.900                    |
|    len                  | 14.258 +/- 6.36 (120)    |
|    len_d_1_pos          | 4.600 +/- 5.58 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 18.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.156 +/- 4.96 (96)     |
|    len_neg              | 16.156 +/- 4.96 (96)     |
|    len_pos              | 6.667 +/- 5.72 (24)      |
|    mrr_mean             | 0.915                    |
|    proven_d_1_pos       | 0.900 +/- 0.32 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.875 +/- 0.34 (24)      |
|    reward               | 0.750 +/- 0.68 (24)      |
|    reward_d_1_pos       | 0.800 +/- 0.63 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.750 +/- 0.68 (24)      |
|    success_rate         | 0.875                    |
----------------------------------------------------


[PPO] ===== Iteration 10 (294912/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.83s
[PPO] Recent episodes: reward=0.400, length=6.2

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.45883, value 0.23409, policy -0.00001, entropy -0.65059, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.44476, value 0.23295, policy -0.00034, entropy -0.64966, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.43889, value 0.23160, policy -0.00071, entropy -0.64873, approx_kl 0.00018 clip_fraction 0.00003. 
Epoch 4/5. 
Losses: total 0.44838, value 0.23027, policy -0.00105, entropy -0.64776, approx_kl 0.00031 clip_fraction 0.00017. 
Epoch 5/5. 
Losses: total 0.43070, value 0.22896, policy -0.00140, entropy -0.64678, approx_kl 0.00047 clip_fraction 0.00054. 
[PPO] Values: min=-0.934, max=2.844, mean=0.648, std=0.321
[PPO] Returns: min=-1.000, max=1.515, mean=0.667, std=0.534
[PPO] Explained variance: 0.1760
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.001                    |
|    entropy              | 0.647                    |
|    explained_var        | 0.176                    |
|    iterations           | 10                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 327680                   |
|    value_loss           | 0.229                    |
----------------------------------------------------

[PPO] Training completed in 5.74s
[PPO] Metrics: policy_loss=-0.0014, value_loss=0.2290, entropy=0.6468
[Metrics] Reward (train): current=0.713, best=0.713 (iter 10)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.092                    |
|    ep_rew_mean          | 0.713                    |
|    len                  | 7.092 +/- 6.60 (46090)   |
|    len_d_1_pos          | 2.133 +/- 1.09 (9032)    |
|    len_d_2_pos          | 6.248 +/- 4.45 (10638)   |
|    len_d_3_pos          | 11.270 +/- 5.29 (2076)   |
|    len_d_4_pos          | 12.282 +/- 4.86 (1341)   |
|    len_d_unknown_neg    | 8.750 +/- 7.61 (23003)   |
|    len_neg              | 8.750 +/- 7.61 (23003)   |
|    len_pos              | 5.440 +/- 4.88 (23087)   |
|    proven_d_1_pos       | 0.951 +/- 0.22 (9032)    |
|    proven_d_2_pos       | 0.830 +/- 0.38 (10638)   |
|    proven_d_3_pos       | 0.509 +/- 0.50 (2076)    |
|    proven_d_4_pos       | 0.503 +/- 0.50 (1341)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (23003)   |
|    proven_neg           | 0.117 +/- 0.32 (23003)   |
|    proven_pos           | 0.830 +/- 0.38 (23087)   |
|    reward               | 0.713 +/- 0.70 (46090)   |
|    reward_d_1_pos       | 0.902 +/- 0.43 (9032)    |
|    reward_d_2_pos       | 0.660 +/- 0.75 (10638)   |
|    reward_d_3_pos       | 0.017 +/- 1.00 (2076)    |
|    reward_d_4_pos       | 0.007 +/- 1.00 (1341)    |
|    reward_d_unknown_neg | 0.766 +/- 0.64 (23003)   |
|    reward_neg           | 0.766 +/- 0.64 (23003)   |
|    reward_pos           | 0.659 +/- 0.75 (23087)   |
|    success_rate         | 0.474                    |
|    time/elapsed         | 300                      |
|    time/fps             | 1081                     |
|    total_timesteps      | 327680                   |
----------------------------------------------------

[Ranking] MRR: current=0.925, best=0.925 (iter 10)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.925                    |
|    _mrr                 | 0.925                    |
|    ep_len_mean          | 14.175                   |
|    ep_rew_mean          | 0.917                    |
|    len                  | 14.175 +/- 6.40 (120)    |
|    len_d_1_pos          | 4.600 +/- 5.58 (10)      |
|    len_d_2_pos          | 6.417 +/- 3.29 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.177 +/- 4.96 (96)     |
|    len_neg              | 16.177 +/- 4.96 (96)     |
|    len_pos              | 6.167 +/- 5.18 (24)      |
|    mrr_mean             | 0.925                    |
|    proven_d_1_pos       | 0.900 +/- 0.32 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.031 +/- 0.17 (96)      |
|    proven_neg           | 0.031 +/- 0.17 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 0.800 +/- 0.63 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.938 +/- 0.35 (96)      |
|    reward_neg           | 0.938 +/- 0.35 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.9167)
[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=0.9250)

[PPO] ===== Iteration 11 (327680/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.21s
[PPO] Recent episodes: reward=0.600, length=10.4

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.42727, value 0.21898, policy -0.00015, entropy -0.63971, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.41517, value 0.21772, policy -0.00042, entropy -0.63874, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.42170, value 0.21649, policy -0.00076, entropy -0.63777, approx_kl 0.00018 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.41348, value 0.21524, policy -0.00111, entropy -0.63685, approx_kl 0.00028 clip_fraction 0.00006. 
Epoch 5/5. 
Losses: total 0.41527, value 0.21410, policy -0.00143, entropy -0.63607, approx_kl 0.00038 clip_fraction 0.00020. 
[PPO] Values: min=-0.734, max=2.109, mean=0.676, std=0.307
[PPO] Returns: min=-1.000, max=1.387, mean=0.672, std=0.537
[PPO] Explained variance: 0.2390
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.636                    |
|    explained_var        | 0.239                    |
|    iterations           | 11                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 360448                   |
|    value_loss           | 0.214                    |
----------------------------------------------------

[PPO] Training completed in 5.75s
[PPO] Metrics: policy_loss=-0.0014, value_loss=0.2141, entropy=0.6361
[Metrics] Reward (train): current=0.719, best=0.719 (iter 11)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.136                    |
|    ep_rew_mean          | 0.719                    |
|    len                  | 7.136 +/- 6.63 (50396)   |
|    len_d_1_pos          | 2.134 +/- 1.09 (9850)    |
|    len_d_2_pos          | 6.219 +/- 4.41 (11635)   |
|    len_d_3_pos          | 11.341 +/- 5.25 (2281)   |
|    len_d_4_pos          | 12.360 +/- 4.87 (1471)   |
|    len_d_unknown_neg    | 8.832 +/- 7.65 (25159)   |
|    len_neg              | 8.832 +/- 7.65 (25159)   |
|    len_pos              | 5.445 +/- 4.88 (25237)   |
|    proven_d_1_pos       | 0.953 +/- 0.21 (9850)    |
|    proven_d_2_pos       | 0.839 +/- 0.37 (11635)   |
|    proven_d_3_pos       | 0.524 +/- 0.50 (2281)    |
|    proven_d_4_pos       | 0.512 +/- 0.50 (1471)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (25159)   |
|    proven_neg           | 0.117 +/- 0.32 (25159)   |
|    proven_pos           | 0.836 +/- 0.37 (25237)   |
|    reward               | 0.719 +/- 0.69 (50396)   |
|    reward_d_1_pos       | 0.906 +/- 0.42 (9850)    |
|    reward_d_2_pos       | 0.678 +/- 0.74 (11635)   |
|    reward_d_3_pos       | 0.049 +/- 1.00 (2281)    |
|    reward_d_4_pos       | 0.024 +/- 1.00 (1471)    |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (25159)   |
|    reward_neg           | 0.767 +/- 0.64 (25159)   |
|    reward_pos           | 0.672 +/- 0.74 (25237)   |
|    success_rate         | 0.477                    |
|    time/elapsed         | 330                      |
|    time/fps             | 1110                     |
|    total_timesteps      | 360448                   |
----------------------------------------------------

[Ranking] MRR: current=0.913, best=0.925 (iter 10)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.875                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.913                    |
|    _mrr                 | 0.913                    |
|    ep_len_mean          | 14.233                   |
|    ep_rew_mean          | 0.917                    |
|    len                  | 14.233 +/- 6.34 (120)    |
|    len_d_1_pos          | 4.600 +/- 5.58 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.104 +/- 4.92 (96)     |
|    len_neg              | 16.104 +/- 4.92 (96)     |
|    len_pos              | 6.750 +/- 5.90 (24)      |
|    mrr_mean             | 0.913                    |
|    proven_d_1_pos       | 0.900 +/- 0.32 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.021 +/- 0.14 (96)      |
|    proven_neg           | 0.021 +/- 0.14 (96)      |
|    proven_pos           | 0.875 +/- 0.34 (24)      |
|    reward               | 0.750 +/- 0.68 (24)      |
|    reward_d_1_pos       | 0.800 +/- 0.63 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.958 +/- 0.29 (96)      |
|    reward_neg           | 0.958 +/- 0.29 (96)      |
|    reward_pos           | 0.750 +/- 0.68 (24)      |
|    success_rate         | 0.875                    |
----------------------------------------------------


[PPO] ===== Iteration 12 (360448/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.65s
[PPO] Recent episodes: reward=0.600, length=6.8

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.43694, value 0.21138, policy -0.00014, entropy -0.63419, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.41497, value 0.21064, policy -0.00031, entropy -0.63371, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.40118, value 0.20977, policy -0.00054, entropy -0.63330, approx_kl 0.00012 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.39898, value 0.20885, policy -0.00085, entropy -0.63282, approx_kl 0.00018 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.39509, value 0.20797, policy -0.00114, entropy -0.63234, approx_kl 0.00026 clip_fraction 0.00004. 
[PPO] Values: min=-0.754, max=1.969, mean=0.655, std=0.312
[PPO] Returns: min=-1.000, max=1.236, mean=0.666, std=0.537
[PPO] Explained variance: 0.2656
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.632                    |
|    explained_var        | 0.266                    |
|    iterations           | 12                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 393216                   |
|    value_loss           | 0.208                    |
----------------------------------------------------

[PPO] Training completed in 5.73s
[PPO] Metrics: policy_loss=-0.0011, value_loss=0.2080, entropy=0.6323
[Metrics] Reward (train): current=0.724, best=0.724 (iter 12)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.187                    |
|    ep_rew_mean          | 0.724                    |
|    len                  | 7.187 +/- 6.67 (54592)   |
|    len_d_1_pos          | 2.136 +/- 1.11 (10681)   |
|    len_d_2_pos          | 6.207 +/- 4.40 (12581)   |
|    len_d_3_pos          | 11.401 +/- 5.22 (2467)   |
|    len_d_4_pos          | 12.466 +/- 4.83 (1608)   |
|    len_d_unknown_neg    | 8.925 +/- 7.68 (27255)   |
|    len_neg              | 8.925 +/- 7.68 (27255)   |
|    len_pos              | 5.453 +/- 4.89 (27337)   |
|    proven_d_1_pos       | 0.954 +/- 0.21 (10681)   |
|    proven_d_2_pos       | 0.846 +/- 0.36 (12581)   |
|    proven_d_3_pos       | 0.535 +/- 0.50 (2467)    |
|    proven_d_4_pos       | 0.523 +/- 0.50 (1608)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (27255)   |
|    proven_neg           | 0.116 +/- 0.32 (27255)   |
|    proven_pos           | 0.841 +/- 0.37 (27337)   |
|    reward               | 0.724 +/- 0.69 (54592)   |
|    reward_d_1_pos       | 0.908 +/- 0.42 (10681)   |
|    reward_d_2_pos       | 0.692 +/- 0.72 (12581)   |
|    reward_d_3_pos       | 0.069 +/- 1.00 (2467)    |
|    reward_d_4_pos       | 0.046 +/- 1.00 (1608)    |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (27255)   |
|    reward_neg           | 0.767 +/- 0.64 (27255)   |
|    reward_pos           | 0.682 +/- 0.73 (27337)   |
|    success_rate         | 0.479                    |
|    time/elapsed         | 359                      |
|    time/fps             | 1134                     |
|    total_timesteps      | 393216                   |
----------------------------------------------------

[Ranking] MRR: current=0.946, best=0.946 (iter 12)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.917                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.946                    |
|    _mrr                 | 0.946                    |
|    ep_len_mean          | 14.200                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.200 +/- 6.42 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.229 +/- 4.93 (96)     |
|    len_neg              | 16.229 +/- 4.93 (96)     |
|    len_pos              | 6.083 +/- 5.20 (24)      |
|    mrr_mean             | 0.946                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.9500)
[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=0.9458)

[PPO] ===== Iteration 13 (393216/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.59s
[PPO] Recent episodes: reward=1.000, length=7.9

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.35918, value 0.18908, policy -0.00022, entropy -0.62077, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.36085, value 0.18836, policy -0.00046, entropy -0.62034, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.35196, value 0.18758, policy -0.00073, entropy -0.61988, approx_kl 0.00014 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.35650, value 0.18685, policy -0.00103, entropy -0.61930, approx_kl 0.00021 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.35651, value 0.18612, policy -0.00132, entropy -0.61874, approx_kl 0.00029 clip_fraction 0.00009. 
[PPO] Values: min=-0.695, max=2.031, mean=0.678, std=0.304
[PPO] Returns: min=-1.000, max=1.284, mean=0.713, std=0.497
[PPO] Explained variance: 0.2358
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.619                    |
|    explained_var        | 0.236                    |
|    iterations           | 13                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 425984                   |
|    value_loss           | 0.186                    |
----------------------------------------------------

[PPO] Training completed in 5.86s
[PPO] Metrics: policy_loss=-0.0013, value_loss=0.1861, entropy=0.6187
[Metrics] Reward (train): current=0.731, best=0.731 (iter 13)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.208                    |
|    ep_rew_mean          | 0.731                    |
|    len                  | 7.208 +/- 6.68 (58977)   |
|    len_d_1_pos          | 2.134 +/- 1.09 (11530)   |
|    len_d_2_pos          | 6.170 +/- 4.37 (13601)   |
|    len_d_3_pos          | 11.400 +/- 5.15 (2662)   |
|    len_d_4_pos          | 12.495 +/- 4.78 (1737)   |
|    len_d_unknown_neg    | 8.983 +/- 7.71 (29447)   |
|    len_neg              | 8.983 +/- 7.71 (29447)   |
|    len_pos              | 5.438 +/- 4.86 (29530)   |
|    proven_d_1_pos       | 0.956 +/- 0.21 (11530)   |
|    proven_d_2_pos       | 0.854 +/- 0.35 (13601)   |
|    proven_d_3_pos       | 0.552 +/- 0.50 (2662)    |
|    proven_d_4_pos       | 0.541 +/- 0.50 (1737)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (29447)   |
|    proven_neg           | 0.116 +/- 0.32 (29447)   |
|    proven_pos           | 0.848 +/- 0.36 (29530)   |
|    reward               | 0.731 +/- 0.68 (58977)   |
|    reward_d_1_pos       | 0.912 +/- 0.41 (11530)   |
|    reward_d_2_pos       | 0.707 +/- 0.71 (13601)   |
|    reward_d_3_pos       | 0.104 +/- 0.99 (2662)    |
|    reward_d_4_pos       | 0.081 +/- 1.00 (1737)    |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (29447)   |
|    reward_neg           | 0.767 +/- 0.64 (29447)   |
|    reward_pos           | 0.696 +/- 0.72 (29530)   |
|    success_rate         | 0.483                    |
|    time/elapsed         | 388                      |
|    time/fps             | 1131                     |
|    total_timesteps      | 425984                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=0.967 (iter 13)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.183                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.183 +/- 6.39 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.208 +/- 4.89 (96)     |
|    len_neg              | 16.208 +/- 4.89 (96)     |
|    len_pos              | 6.083 +/- 5.20 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------

[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=0.9667)

[PPO] ===== Iteration 14 (425984/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.64s
[PPO] Recent episodes: reward=1.000, length=8.6

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.36473, value 0.18933, policy -0.00013, entropy -0.61505, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.36171, value 0.18881, policy -0.00036, entropy -0.61439, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.37532, value 0.18817, policy -0.00060, entropy -0.61375, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.35827, value 0.18754, policy -0.00086, entropy -0.61311, approx_kl 0.00016 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.37093, value 0.18689, policy -0.00112, entropy -0.61249, approx_kl 0.00023 clip_fraction 0.00001. 
[PPO] Values: min=-0.582, max=2.047, mean=0.729, std=0.294
[PPO] Returns: min=-1.000, max=1.184, mean=0.717, std=0.501
[PPO] Explained variance: 0.2449
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.612                    |
|    explained_var        | 0.245                    |
|    iterations           | 14                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 458752                   |
|    value_loss           | 0.187                    |
----------------------------------------------------

[PPO] Training completed in 6.82s
[PPO] Metrics: policy_loss=-0.0011, value_loss=0.1869, entropy=0.6125
[Metrics] Reward (train): current=0.737, best=0.737 (iter 14)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.230                    |
|    ep_rew_mean          | 0.737                    |
|    len                  | 7.230 +/- 6.70 (63331)   |
|    len_d_1_pos          | 2.134 +/- 1.09 (12422)   |
|    len_d_2_pos          | 6.146 +/- 4.34 (14563)   |
|    len_d_3_pos          | 11.434 +/- 5.11 (2872)   |
|    len_d_4_pos          | 12.539 +/- 4.75 (1850)   |
|    len_d_unknown_neg    | 9.038 +/- 7.74 (31624)   |
|    len_neg              | 9.038 +/- 7.74 (31624)   |
|    len_pos              | 5.426 +/- 4.85 (31707)   |
|    proven_d_1_pos       | 0.957 +/- 0.20 (12422)   |
|    proven_d_2_pos       | 0.859 +/- 0.35 (14563)   |
|    proven_d_3_pos       | 0.568 +/- 0.50 (2872)    |
|    proven_d_4_pos       | 0.550 +/- 0.50 (1850)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (31624)   |
|    proven_neg           | 0.117 +/- 0.32 (31624)   |
|    proven_pos           | 0.853 +/- 0.35 (31707)   |
|    reward               | 0.737 +/- 0.68 (63331)   |
|    reward_d_1_pos       | 0.914 +/- 0.41 (12422)   |
|    reward_d_2_pos       | 0.719 +/- 0.70 (14563)   |
|    reward_d_3_pos       | 0.136 +/- 0.99 (2872)    |
|    reward_d_4_pos       | 0.101 +/- 0.99 (1850)    |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (31624)   |
|    reward_neg           | 0.767 +/- 0.64 (31624)   |
|    reward_pos           | 0.706 +/- 0.71 (31707)   |
|    success_rate         | 0.485                    |
|    time/elapsed         | 419                      |
|    time/fps             | 1054                     |
|    total_timesteps      | 458752                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=0.967 (iter 13)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.042                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.042 +/- 6.49 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.583 +/- 5.11 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.031 +/- 5.12 (96)     |
|    len_neg              | 16.031 +/- 5.12 (96)     |
|    len_pos              | 6.083 +/- 5.20 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 15 (458752/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.90s
[PPO] Recent episodes: reward=0.600, length=5.4

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.37478, value 0.18517, policy -0.00014, entropy -0.61769, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.35533, value 0.18431, policy -0.00046, entropy -0.61673, approx_kl 0.00010 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.35070, value 0.18337, policy -0.00084, entropy -0.61572, approx_kl 0.00017 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.34655, value 0.18244, policy -0.00124, entropy -0.61465, approx_kl 0.00027 clip_fraction 0.00001. 
Epoch 5/5. 
Losses: total 0.34418, value 0.18160, policy -0.00160, entropy -0.61353, approx_kl 0.00041 clip_fraction 0.00004. 
[PPO] Values: min=-0.562, max=1.875, mean=0.715, std=0.284
[PPO] Returns: min=-1.000, max=1.171, mean=0.712, std=0.500
[PPO] Explained variance: 0.2576
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.614                    |
|    explained_var        | 0.258                    |
|    iterations           | 15                       |
|    policy_loss          | -0.002                   |
|    total_timesteps      | 491520                   |
|    value_loss           | 0.182                    |
----------------------------------------------------

[PPO] Training completed in 6.87s
[PPO] Metrics: policy_loss=-0.0016, value_loss=0.1816, entropy=0.6135
[Metrics] Reward (train): current=0.741, best=0.741 (iter 15)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.255                    |
|    ep_rew_mean          | 0.741                    |
|    len                  | 7.255 +/- 6.72 (67630)   |
|    len_d_1_pos          | 2.133 +/- 1.09 (13269)   |
|    len_d_2_pos          | 6.120 +/- 4.32 (15538)   |
|    len_d_3_pos          | 11.448 +/- 5.05 (3063)   |
|    len_d_4_pos          | 12.572 +/- 4.73 (1987)   |
|    len_d_unknown_neg    | 9.097 +/- 7.76 (33773)   |
|    len_neg              | 9.097 +/- 7.76 (33773)   |
|    len_pos              | 5.418 +/- 4.83 (33857)   |
|    proven_d_1_pos       | 0.959 +/- 0.20 (13269)   |
|    proven_d_2_pos       | 0.865 +/- 0.34 (15538)   |
|    proven_d_3_pos       | 0.584 +/- 0.49 (3063)    |
|    proven_d_4_pos       | 0.557 +/- 0.50 (1987)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (33773)   |
|    proven_neg           | 0.117 +/- 0.32 (33773)   |
|    proven_pos           | 0.858 +/- 0.35 (33857)   |
|    reward               | 0.741 +/- 0.67 (67630)   |
|    reward_d_1_pos       | 0.917 +/- 0.40 (13269)   |
|    reward_d_2_pos       | 0.730 +/- 0.68 (15538)   |
|    reward_d_3_pos       | 0.167 +/- 0.99 (3063)    |
|    reward_d_4_pos       | 0.114 +/- 0.99 (1987)    |
|    reward_d_unknown_neg | 0.766 +/- 0.64 (33773)   |
|    reward_neg           | 0.766 +/- 0.64 (33773)   |
|    reward_pos           | 0.716 +/- 0.70 (33857)   |
|    success_rate         | 0.488                    |
|    time/elapsed         | 450                      |
|    time/fps             | 1035                     |
|    total_timesteps      | 491520                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=0.967 (iter 13)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.358                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.358 +/- 6.57 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.417 +/- 5.20 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.448 +/- 5.05 (96)     |
|    len_neg              | 16.448 +/- 5.05 (96)     |
|    len_pos              | 6.000 +/- 5.22 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 16 (491520/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.13s
[PPO] Recent episodes: reward=0.600, length=8.6

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.34746, value 0.17781, policy -0.00011, entropy -0.59865, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.34832, value 0.17710, policy -0.00038, entropy -0.59761, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.33303, value 0.17631, policy -0.00068, entropy -0.59652, approx_kl 0.00016 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.33286, value 0.17561, policy -0.00099, entropy -0.59552, approx_kl 0.00025 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.35398, value 0.17498, policy -0.00134, entropy -0.59453, approx_kl 0.00037 clip_fraction 0.00001. 
[PPO] Values: min=-0.684, max=1.992, mean=0.708, std=0.287
[PPO] Returns: min=-1.000, max=1.195, mean=0.719, std=0.492
[PPO] Explained variance: 0.2647
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.595                    |
|    explained_var        | 0.265                    |
|    iterations           | 16                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 524288                   |
|    value_loss           | 0.175                    |
----------------------------------------------------

[PPO] Training completed in 5.75s
[PPO] Metrics: policy_loss=-0.0013, value_loss=0.1750, entropy=0.5945
[Metrics] Reward (train): current=0.746, best=0.746 (iter 16)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.274                    |
|    ep_rew_mean          | 0.746                    |
|    len                  | 7.274 +/- 6.74 (71955)   |
|    len_d_1_pos          | 2.136 +/- 1.10 (14120)   |
|    len_d_2_pos          | 6.085 +/- 4.28 (16534)   |
|    len_d_3_pos          | 11.457 +/- 5.01 (3243)   |
|    len_d_4_pos          | 12.629 +/- 4.72 (2122)   |
|    len_d_unknown_neg    | 9.147 +/- 7.79 (35936)   |
|    len_neg              | 9.147 +/- 7.79 (35936)   |
|    len_pos              | 5.406 +/- 4.82 (36019)   |
|    proven_d_1_pos       | 0.960 +/- 0.20 (14120)   |
|    proven_d_2_pos       | 0.871 +/- 0.34 (16534)   |
|    proven_d_3_pos       | 0.594 +/- 0.49 (3243)    |
|    proven_d_4_pos       | 0.564 +/- 0.50 (2122)    |
|    proven_d_unknown_neg | 0.117 +/- 0.32 (35936)   |
|    proven_neg           | 0.117 +/- 0.32 (35936)   |
|    proven_pos           | 0.863 +/- 0.34 (36019)   |
|    reward               | 0.746 +/- 0.67 (71955)   |
|    reward_d_1_pos       | 0.919 +/- 0.39 (14120)   |
|    reward_d_2_pos       | 0.742 +/- 0.67 (16534)   |
|    reward_d_3_pos       | 0.188 +/- 0.98 (3243)    |
|    reward_d_4_pos       | 0.127 +/- 0.99 (2122)    |
|    reward_d_unknown_neg | 0.767 +/- 0.64 (35936)   |
|    reward_neg           | 0.767 +/- 0.64 (35936)   |
|    reward_pos           | 0.725 +/- 0.69 (36019)   |
|    success_rate         | 0.490                    |
|    time/elapsed         | 479                      |
|    time/fps             | 1140                     |
|    total_timesteps      | 524288                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=0.967 (iter 13)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.533                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.533 +/- 6.70 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.708 +/- 5.08 (96)     |
|    len_neg              | 16.708 +/- 5.08 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 17 (524288/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.04s
[PPO] Recent episodes: reward=1.000, length=8.7

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.31509, value 0.16148, policy -0.00004, entropy -0.57966, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.32740, value 0.16094, policy -0.00026, entropy -0.57887, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.31622, value 0.16035, policy -0.00049, entropy -0.57807, approx_kl 0.00012 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.30582, value 0.15977, policy -0.00074, entropy -0.57720, approx_kl 0.00018 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.29569, value 0.15916, policy -0.00104, entropy -0.57629, approx_kl 0.00026 clip_fraction 0.00004. 
[PPO] Values: min=-0.613, max=1.969, mean=0.725, std=0.281
[PPO] Returns: min=-1.000, max=1.308, mean=0.745, std=0.470
[PPO] Explained variance: 0.2694
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.576                    |
|    explained_var        | 0.269                    |
|    iterations           | 17                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 557056                   |
|    value_loss           | 0.159                    |
----------------------------------------------------

[PPO] Training completed in 5.73s
[PPO] Metrics: policy_loss=-0.0010, value_loss=0.1592, entropy=0.5763
[Metrics] Reward (train): current=0.751, best=0.751 (iter 17)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.276                    |
|    ep_rew_mean          | 0.751                    |
|    len                  | 7.276 +/- 6.74 (76448)   |
|    len_d_1_pos          | 2.133 +/- 1.09 (14991)   |
|    len_d_2_pos          | 6.040 +/- 4.23 (17551)   |
|    len_d_3_pos          | 11.431 +/- 4.96 (3461)   |
|    len_d_4_pos          | 12.607 +/- 4.69 (2262)   |
|    len_d_unknown_neg    | 9.170 +/- 7.80 (38183)   |
|    len_neg              | 9.170 +/- 7.80 (38183)   |
|    len_pos              | 5.385 +/- 4.79 (38265)   |
|    proven_d_1_pos       | 0.961 +/- 0.19 (14991)   |
|    proven_d_2_pos       | 0.876 +/- 0.33 (17551)   |
|    proven_d_3_pos       | 0.609 +/- 0.49 (3461)    |
|    proven_d_4_pos       | 0.576 +/- 0.49 (2262)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (38183)   |
|    proven_neg           | 0.116 +/- 0.32 (38183)   |
|    proven_pos           | 0.868 +/- 0.34 (38265)   |
|    reward               | 0.751 +/- 0.66 (76448)   |
|    reward_d_1_pos       | 0.922 +/- 0.39 (14991)   |
|    reward_d_2_pos       | 0.753 +/- 0.66 (17551)   |
|    reward_d_3_pos       | 0.218 +/- 0.98 (3461)    |
|    reward_d_4_pos       | 0.152 +/- 0.99 (2262)    |
|    reward_d_unknown_neg | 0.768 +/- 0.64 (38183)   |
|    reward_neg           | 0.768 +/- 0.64 (38183)   |
|    reward_pos           | 0.735 +/- 0.68 (38265)   |
|    success_rate         | 0.492                    |
|    time/elapsed         | 508                      |
|    time/fps             | 1118                     |
|    total_timesteps      | 557056                   |
----------------------------------------------------

[Ranking] MRR: current=1.000, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 1.000                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 1.000                    |
|    MRR                  | 1.000                    |
|    _mrr                 | 1.000                    |
|    ep_len_mean          | 14.575                   |
|    ep_rew_mean          | 0.967                    |
|    len                  | 14.575 +/- 6.74 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 5.917 +/- 3.15 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.906 +/- 5.02 (96)     |
|    len_neg              | 16.906 +/- 5.02 (96)     |
|    len_pos              | 5.250 +/- 4.17 (24)      |
|    mrr_mean             | 1.000                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.958 +/- 0.20 (24)      |
|    reward               | 0.917 +/- 0.41 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.917 +/- 0.41 (24)      |
|    success_rate         | 0.958                    |
----------------------------------------------------

[Checkpoint] New best train model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_train.pt (ep_rew_mean=0.9667)
[Checkpoint] New best eval model saved to models/countries_s3-250-True-0.1-0.2-1-4-5-3e-05-128-torchrl/best_model_eval.pt (mrr_mean=1.0000)

[PPO] ===== Iteration 18 (557056/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.89s
[PPO] Recent episodes: reward=0.800, length=7.0

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.29110, value 0.14968, policy -0.00024, entropy -0.57366, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.28021, value 0.14927, policy -0.00049, entropy -0.57291, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.30667, value 0.14878, policy -0.00080, entropy -0.57216, approx_kl 0.00015 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.27751, value 0.14829, policy -0.00111, entropy -0.57151, approx_kl 0.00024 clip_fraction 0.00002. 
Epoch 5/5. 
Losses: total 0.28074, value 0.14782, policy -0.00142, entropy -0.57091, approx_kl 0.00033 clip_fraction 0.00006. 
[PPO] Values: min=-0.672, max=2.000, mean=0.759, std=0.271
[PPO] Returns: min=-1.000, max=1.125, mean=0.759, std=0.460
[PPO] Explained variance: 0.2907
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.571                    |
|    explained_var        | 0.291                    |
|    iterations           | 18                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 589824                   |
|    value_loss           | 0.148                    |
----------------------------------------------------

[PPO] Training completed in 5.87s
[PPO] Metrics: policy_loss=-0.0014, value_loss=0.1478, entropy=0.5709
[Metrics] Reward (train): current=0.757, best=0.757 (iter 18)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.280                    |
|    ep_rew_mean          | 0.757                    |
|    len                  | 7.280 +/- 6.75 (80914)   |
|    len_d_1_pos          | 2.131 +/- 1.08 (15874)   |
|    len_d_2_pos          | 6.007 +/- 4.19 (18571)   |
|    len_d_3_pos          | 11.409 +/- 4.90 (3660)   |
|    len_d_4_pos          | 12.592 +/- 4.66 (2392)   |
|    len_d_unknown_neg    | 9.198 +/- 7.82 (40417)   |
|    len_neg              | 9.198 +/- 7.82 (40417)   |
|    len_pos              | 5.365 +/- 4.76 (40497)   |
|    proven_d_1_pos       | 0.962 +/- 0.19 (15874)   |
|    proven_d_2_pos       | 0.881 +/- 0.32 (18571)   |
|    proven_d_3_pos       | 0.622 +/- 0.48 (3660)    |
|    proven_d_4_pos       | 0.586 +/- 0.49 (2392)    |
|    proven_d_unknown_neg | 0.115 +/- 0.32 (40417)   |
|    proven_neg           | 0.115 +/- 0.32 (40417)   |
|    proven_pos           | 0.872 +/- 0.33 (40497)   |
|    reward               | 0.757 +/- 0.65 (80914)   |
|    reward_d_1_pos       | 0.925 +/- 0.38 (15874)   |
|    reward_d_2_pos       | 0.763 +/- 0.65 (18571)   |
|    reward_d_3_pos       | 0.244 +/- 0.97 (3660)    |
|    reward_d_4_pos       | 0.172 +/- 0.99 (2392)    |
|    reward_d_unknown_neg | 0.769 +/- 0.64 (40417)   |
|    reward_neg           | 0.769 +/- 0.64 (40417)   |
|    reward_pos           | 0.744 +/- 0.67 (40497)   |
|    success_rate         | 0.494                    |
|    time/elapsed         | 538                      |
|    time/fps             | 1117                     |
|    total_timesteps      | 589824                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.392                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.392 +/- 6.72 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.531 +/- 5.20 (96)     |
|    len_neg              | 16.531 +/- 5.20 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 19 (589824/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.89s
[PPO] Recent episodes: reward=0.800, length=4.7

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.31203, value 0.15460, policy -0.00017, entropy -0.56158, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.29721, value 0.15427, policy -0.00034, entropy -0.56113, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.30937, value 0.15385, policy -0.00054, entropy -0.56057, approx_kl 0.00010 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.29749, value 0.15341, policy -0.00074, entropy -0.55996, approx_kl 0.00014 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.30203, value 0.15296, policy -0.00100, entropy -0.55929, approx_kl 0.00019 clip_fraction 0.00000. 
[PPO] Values: min=-0.695, max=1.945, mean=0.761, std=0.269
[PPO] Returns: min=-1.000, max=1.454, mean=0.756, std=0.466
[PPO] Explained variance: 0.2875
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.559                    |
|    explained_var        | 0.287                    |
|    iterations           | 19                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 622592                   |
|    value_loss           | 0.153                    |
----------------------------------------------------

[PPO] Training completed in 5.84s
[PPO] Metrics: policy_loss=-0.0010, value_loss=0.1530, entropy=0.5593
[Metrics] Reward (train): current=0.761, best=0.761 (iter 19)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.279                    |
|    ep_rew_mean          | 0.761                    |
|    len                  | 7.279 +/- 6.75 (85414)   |
|    len_d_1_pos          | 2.129 +/- 1.06 (16783)   |
|    len_d_2_pos          | 5.970 +/- 4.15 (19566)   |
|    len_d_3_pos          | 11.411 +/- 4.85 (3867)   |
|    len_d_4_pos          | 12.533 +/- 4.65 (2530)   |
|    len_d_unknown_neg    | 9.219 +/- 7.83 (42668)   |
|    len_neg              | 9.219 +/- 7.83 (42668)   |
|    len_pos              | 5.343 +/- 4.73 (42746)   |
|    proven_d_1_pos       | 0.964 +/- 0.19 (16783)   |
|    proven_d_2_pos       | 0.886 +/- 0.32 (19566)   |
|    proven_d_3_pos       | 0.635 +/- 0.48 (3867)    |
|    proven_d_4_pos       | 0.597 +/- 0.49 (2530)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (42668)   |
|    proven_neg           | 0.116 +/- 0.32 (42668)   |
|    proven_pos           | 0.877 +/- 0.33 (42746)   |
|    reward               | 0.761 +/- 0.65 (85414)   |
|    reward_d_1_pos       | 0.927 +/- 0.37 (16783)   |
|    reward_d_2_pos       | 0.771 +/- 0.64 (19566)   |
|    reward_d_3_pos       | 0.270 +/- 0.96 (3867)    |
|    reward_d_4_pos       | 0.194 +/- 0.98 (2530)    |
|    reward_d_unknown_neg | 0.769 +/- 0.64 (42668)   |
|    reward_neg           | 0.769 +/- 0.64 (42668)   |
|    reward_pos           | 0.753 +/- 0.66 (42746)   |
|    success_rate         | 0.496                    |
|    time/elapsed         | 567                      |
|    time/fps             | 1117                     |
|    total_timesteps      | 622592                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.425                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.425 +/- 6.68 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.573 +/- 5.11 (96)     |
|    len_neg              | 16.573 +/- 5.11 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 20 (622592/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.26s
[PPO] Recent episodes: reward=1.000, length=3.5

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.31119, value 0.16266, policy 0.00002, entropy -0.55455, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.32511, value 0.16224, policy -0.00021, entropy -0.55400, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.30689, value 0.16181, policy -0.00041, entropy -0.55344, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.30920, value 0.16143, policy -0.00060, entropy -0.55288, approx_kl 0.00012 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.30682, value 0.16107, policy -0.00079, entropy -0.55236, approx_kl 0.00016 clip_fraction 0.00000. 
[PPO] Values: min=-0.703, max=2.719, mean=0.748, std=0.269
[PPO] Returns: min=-1.000, max=1.042, mean=0.744, std=0.479
[PPO] Explained variance: 0.2903
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.552                    |
|    explained_var        | 0.290                    |
|    iterations           | 20                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 655360                   |
|    value_loss           | 0.161                    |
----------------------------------------------------

[PPO] Training completed in 5.82s
[PPO] Metrics: policy_loss=-0.0008, value_loss=0.1611, entropy=0.5524
[Metrics] Reward (train): current=0.765, best=0.765 (iter 20)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.277                    |
|    ep_rew_mean          | 0.765                    |
|    len                  | 7.277 +/- 6.75 (89955)   |
|    len_d_1_pos          | 2.127 +/- 1.05 (17669)   |
|    len_d_2_pos          | 5.936 +/- 4.12 (20609)   |
|    len_d_3_pos          | 11.404 +/- 4.80 (4084)   |
|    len_d_4_pos          | 12.488 +/- 4.63 (2662)   |
|    len_d_unknown_neg    | 9.233 +/- 7.84 (44931)   |
|    len_neg              | 9.233 +/- 7.84 (44931)   |
|    len_pos              | 5.324 +/- 4.70 (45024)   |
|    proven_d_1_pos       | 0.965 +/- 0.18 (17669)   |
|    proven_d_2_pos       | 0.890 +/- 0.31 (20609)   |
|    proven_d_3_pos       | 0.648 +/- 0.48 (4084)    |
|    proven_d_4_pos       | 0.607 +/- 0.49 (2662)    |
|    proven_d_unknown_neg | 0.116 +/- 0.32 (44931)   |
|    proven_neg           | 0.116 +/- 0.32 (44931)   |
|    proven_pos           | 0.881 +/- 0.32 (45024)   |
|    reward               | 0.765 +/- 0.64 (89955)   |
|    reward_d_1_pos       | 0.929 +/- 0.37 (17669)   |
|    reward_d_2_pos       | 0.780 +/- 0.63 (20609)   |
|    reward_d_3_pos       | 0.295 +/- 0.96 (4084)    |
|    reward_d_4_pos       | 0.215 +/- 0.98 (2662)    |
|    reward_d_unknown_neg | 0.768 +/- 0.64 (44931)   |
|    reward_neg           | 0.768 +/- 0.64 (44931)   |
|    reward_pos           | 0.761 +/- 0.65 (45024)   |
|    success_rate         | 0.499                    |
|    time/elapsed         | 596                      |
|    time/fps             | 1147                     |
|    total_timesteps      | 655360                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.392                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.392 +/- 6.66 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.531 +/- 5.11 (96)     |
|    len_neg              | 16.531 +/- 5.11 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 21 (655360/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.56s
[PPO] Recent episodes: reward=1.000, length=8.0

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.26968, value 0.13328, policy -0.00002, entropy -0.55114, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.26105, value 0.13291, policy -0.00015, entropy -0.55074, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.24034, value 0.13253, policy -0.00031, entropy -0.55038, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.25898, value 0.13215, policy -0.00049, entropy -0.54998, approx_kl 0.00012 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.25384, value 0.13179, policy -0.00071, entropy -0.54961, approx_kl 0.00016 clip_fraction 0.00000. 
[PPO] Values: min=-0.676, max=2.656, mean=0.759, std=0.260
[PPO] Returns: min=-1.000, max=1.272, mean=0.776, std=0.433
[PPO] Explained variance: 0.2907
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.550                    |
|    explained_var        | 0.291                    |
|    iterations           | 21                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 688128                   |
|    value_loss           | 0.132                    |
----------------------------------------------------

[PPO] Training completed in 5.73s
[PPO] Metrics: policy_loss=-0.0007, value_loss=0.1318, entropy=0.5496
[Metrics] Reward (train): current=0.769, best=0.769 (iter 21)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.282                    |
|    ep_rew_mean          | 0.769                    |
|    len                  | 7.282 +/- 6.75 (94370)   |
|    len_d_1_pos          | 2.128 +/- 1.06 (18518)   |
|    len_d_2_pos          | 5.900 +/- 4.07 (21611)   |
|    len_d_3_pos          | 11.357 +/- 4.74 (4310)   |
|    len_d_4_pos          | 12.431 +/- 4.62 (2793)   |
|    len_d_unknown_neg    | 9.263 +/- 7.85 (47138)   |
|    len_neg              | 9.263 +/- 7.85 (47138)   |
|    len_pos              | 5.305 +/- 4.67 (47232)   |
|    proven_d_1_pos       | 0.966 +/- 0.18 (18518)   |
|    proven_d_2_pos       | 0.893 +/- 0.31 (21611)   |
|    proven_d_3_pos       | 0.661 +/- 0.47 (4310)    |
|    proven_d_4_pos       | 0.619 +/- 0.49 (2793)    |
|    proven_d_unknown_neg | 0.115 +/- 0.32 (47138)   |
|    proven_neg           | 0.115 +/- 0.32 (47138)   |
|    proven_pos           | 0.884 +/- 0.32 (47232)   |
|    reward               | 0.769 +/- 0.64 (94370)   |
|    reward_d_1_pos       | 0.931 +/- 0.36 (18518)   |
|    reward_d_2_pos       | 0.787 +/- 0.62 (21611)   |
|    reward_d_3_pos       | 0.322 +/- 0.95 (4310)    |
|    reward_d_4_pos       | 0.239 +/- 0.97 (2793)    |
|    reward_d_unknown_neg | 0.770 +/- 0.64 (47138)   |
|    reward_neg           | 0.770 +/- 0.64 (47138)   |
|    reward_pos           | 0.769 +/- 0.64 (47232)   |
|    success_rate         | 0.500                    |
|    time/elapsed         | 624                      |
|    time/fps             | 1133                     |
|    total_timesteps      | 688128                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.425                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.425 +/- 6.64 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.573 +/- 5.05 (96)     |
|    len_neg              | 16.573 +/- 5.05 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 22 (688128/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.75s
[PPO] Recent episodes: reward=0.800, length=5.2

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.27203, value 0.13830, policy -0.00016, entropy -0.54602, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.28444, value 0.13798, policy -0.00033, entropy -0.54563, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.26867, value 0.13764, policy -0.00048, entropy -0.54520, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.25679, value 0.13727, policy -0.00064, entropy -0.54477, approx_kl 0.00011 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.26152, value 0.13690, policy -0.00081, entropy -0.54433, approx_kl 0.00015 clip_fraction 0.00000. 
[PPO] Values: min=-0.711, max=2.578, mean=0.781, std=0.250
[PPO] Returns: min=-1.000, max=1.187, mean=0.780, std=0.435
[PPO] Explained variance: 0.2667
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.544                    |
|    explained_var        | 0.267                    |
|    iterations           | 22                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 720896                   |
|    value_loss           | 0.137                    |
----------------------------------------------------

[PPO] Training completed in 5.85s
[PPO] Metrics: policy_loss=-0.0008, value_loss=0.1369, entropy=0.5443
[Metrics] Reward (train): current=0.773, best=0.773 (iter 22)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.280                    |
|    ep_rew_mean          | 0.773                    |
|    len                  | 7.280 +/- 6.76 (98908)   |
|    len_d_1_pos          | 2.127 +/- 1.04 (19428)   |
|    len_d_2_pos          | 5.860 +/- 4.04 (22653)   |
|    len_d_3_pos          | 11.334 +/- 4.71 (4497)   |
|    len_d_4_pos          | 12.405 +/- 4.61 (2916)   |
|    len_d_unknown_neg    | 9.287 +/- 7.86 (49414)   |
|    len_neg              | 9.287 +/- 7.86 (49414)   |
|    len_pos              | 5.277 +/- 4.64 (49494)   |
|    proven_d_1_pos       | 0.967 +/- 0.18 (19428)   |
|    proven_d_2_pos       | 0.897 +/- 0.30 (22653)   |
|    proven_d_3_pos       | 0.671 +/- 0.47 (4497)    |
|    proven_d_4_pos       | 0.629 +/- 0.48 (2916)    |
|    proven_d_unknown_neg | 0.115 +/- 0.32 (49414)   |
|    proven_neg           | 0.115 +/- 0.32 (49414)   |
|    proven_pos           | 0.888 +/- 0.32 (49494)   |
|    reward               | 0.773 +/- 0.63 (98908)   |
|    reward_d_1_pos       | 0.934 +/- 0.36 (19428)   |
|    reward_d_2_pos       | 0.794 +/- 0.61 (22653)   |
|    reward_d_3_pos       | 0.342 +/- 0.94 (4497)    |
|    reward_d_4_pos       | 0.257 +/- 0.97 (2916)    |
|    reward_d_unknown_neg | 0.770 +/- 0.64 (49414)   |
|    reward_neg           | 0.770 +/- 0.64 (49414)   |
|    reward_pos           | 0.776 +/- 0.63 (49494)   |
|    success_rate         | 0.502                    |
|    time/elapsed         | 655                      |
|    time/fps             | 1085                     |
|    total_timesteps      | 720896                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.425                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.425 +/- 6.64 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.573 +/- 5.05 (96)     |
|    len_neg              | 16.573 +/- 5.05 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 23 (720896/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 20.26s
[PPO] Recent episodes: reward=1.000, length=8.3

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.25946, value 0.13700, policy -0.00004, entropy -0.53881, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.27845, value 0.13678, policy -0.00018, entropy -0.53846, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.25243, value 0.13654, policy -0.00030, entropy -0.53814, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.25240, value 0.13631, policy -0.00044, entropy -0.53781, approx_kl 0.00009 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.25943, value 0.13607, policy -0.00060, entropy -0.53748, approx_kl 0.00011 clip_fraction 0.00000. 
[PPO] Values: min=-0.625, max=1.836, mean=0.777, std=0.255
[PPO] Returns: min=-1.000, max=1.750, mean=0.776, std=0.439
[PPO] Explained variance: 0.2888
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.537                    |
|    explained_var        | 0.289                    |
|    iterations           | 23                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 753664                   |
|    value_loss           | 0.136                    |
----------------------------------------------------

[PPO] Training completed in 6.84s
[PPO] Metrics: policy_loss=-0.0006, value_loss=0.1361, entropy=0.5375
[Metrics] Reward (train): current=0.777, best=0.777 (iter 23)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.278                    |
|    ep_rew_mean          | 0.777                    |
|    len                  | 7.278 +/- 6.76 (103446)  |
|    len_d_1_pos          | 2.126 +/- 1.04 (20311)   |
|    len_d_2_pos          | 5.823 +/- 4.00 (23679)   |
|    len_d_3_pos          | 11.309 +/- 4.68 (4718)   |
|    len_d_4_pos          | 12.386 +/- 4.58 (3061)   |
|    len_d_unknown_neg    | 9.299 +/- 7.87 (51677)   |
|    len_neg              | 9.299 +/- 7.87 (51677)   |
|    len_pos              | 5.260 +/- 4.62 (51769)   |
|    proven_d_1_pos       | 0.968 +/- 0.18 (20311)   |
|    proven_d_2_pos       | 0.901 +/- 0.30 (23679)   |
|    proven_d_3_pos       | 0.680 +/- 0.47 (4718)    |
|    proven_d_4_pos       | 0.637 +/- 0.48 (3061)    |
|    proven_d_unknown_neg | 0.114 +/- 0.32 (51677)   |
|    proven_neg           | 0.114 +/- 0.32 (51677)   |
|    proven_pos           | 0.891 +/- 0.31 (51769)   |
|    reward               | 0.777 +/- 0.63 (103446)  |
|    reward_d_1_pos       | 0.936 +/- 0.35 (20311)   |
|    reward_d_2_pos       | 0.801 +/- 0.60 (23679)   |
|    reward_d_3_pos       | 0.360 +/- 0.93 (4718)    |
|    reward_d_4_pos       | 0.274 +/- 0.96 (3061)    |
|    reward_d_unknown_neg | 0.771 +/- 0.64 (51677)   |
|    reward_neg           | 0.771 +/- 0.64 (51677)   |
|    reward_pos           | 0.783 +/- 0.62 (51769)   |
|    success_rate         | 0.503                    |
|    time/elapsed         | 685                      |
|    time/fps             | 1068                     |
|    total_timesteps      | 753664                   |
----------------------------------------------------

[Ranking] MRR: current=0.967, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.967                    |
|    _mrr                 | 0.967                    |
|    ep_len_mean          | 14.358                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.358 +/- 6.64 (120)    |
|    len_d_1_pos          | 3.000 +/- 1.41 (10)      |
|    len_d_2_pos          | 7.083 +/- 5.14 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.490 +/- 5.09 (96)     |
|    len_neg              | 16.490 +/- 5.09 (96)     |
|    len_pos              | 5.833 +/- 5.15 (24)      |
|    mrr_mean             | 0.967                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 24 (753664/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 18.72s
[PPO] Recent episodes: reward=0.600, length=3.4

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.24936, value 0.12691, policy -0.00013, entropy -0.53636, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.22832, value 0.12664, policy -0.00021, entropy -0.53607, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.24371, value 0.12640, policy -0.00037, entropy -0.53576, approx_kl 0.00008 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.23371, value 0.12616, policy -0.00051, entropy -0.53543, approx_kl 0.00010 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.24439, value 0.12592, policy -0.00065, entropy -0.53513, approx_kl 0.00012 clip_fraction 0.00000. 
[PPO] Values: min=-0.656, max=1.883, mean=0.782, std=0.254
[PPO] Returns: min=-1.000, max=1.055, mean=0.790, std=0.421
[PPO] Explained variance: 0.2817
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.535                    |
|    explained_var        | 0.282                    |
|    iterations           | 24                       |
|    policy_loss          | -0.001                   |
|    total_timesteps      | 786432                   |
|    value_loss           | 0.126                    |
----------------------------------------------------

[PPO] Training completed in 6.86s
[PPO] Metrics: policy_loss=-0.0007, value_loss=0.1259, entropy=0.5351
[Metrics] Reward (train): current=0.781, best=0.781 (iter 24)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.284                    |
|    ep_rew_mean          | 0.781                    |
|    len                  | 7.284 +/- 6.77 (107867)  |
|    len_d_1_pos          | 2.123 +/- 1.02 (21182)   |
|    len_d_2_pos          | 5.789 +/- 3.96 (24666)   |
|    len_d_3_pos          | 11.300 +/- 4.64 (4940)   |
|    len_d_4_pos          | 12.335 +/- 4.56 (3189)   |
|    len_d_unknown_neg    | 9.329 +/- 7.88 (53890)   |
|    len_neg              | 9.329 +/- 7.88 (53890)   |
|    len_pos              | 5.241 +/- 4.59 (53977)   |
|    proven_d_1_pos       | 0.969 +/- 0.17 (21182)   |
|    proven_d_2_pos       | 0.904 +/- 0.29 (24666)   |
|    proven_d_3_pos       | 0.690 +/- 0.46 (4940)    |
|    proven_d_4_pos       | 0.647 +/- 0.48 (3189)    |
|    proven_d_unknown_neg | 0.114 +/- 0.32 (53890)   |
|    proven_neg           | 0.114 +/- 0.32 (53890)   |
|    proven_pos           | 0.895 +/- 0.31 (53977)   |
|    reward               | 0.781 +/- 0.62 (107867)  |
|    reward_d_1_pos       | 0.938 +/- 0.35 (21182)   |
|    reward_d_2_pos       | 0.808 +/- 0.59 (24666)   |
|    reward_d_3_pos       | 0.379 +/- 0.93 (4940)    |
|    reward_d_4_pos       | 0.294 +/- 0.96 (3189)    |
|    reward_d_unknown_neg | 0.772 +/- 0.64 (53890)   |
|    reward_neg           | 0.772 +/- 0.64 (53890)   |
|    reward_pos           | 0.789 +/- 0.61 (53977)   |
|    success_rate         | 0.505                    |
|    time/elapsed         | 715                      |
|    time/fps             | 1112                     |
|    total_timesteps      | 786432                   |
----------------------------------------------------

[Ranking] MRR: current=1.000, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 1.000                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 1.000                    |
|    MRR                  | 1.000                    |
|    _mrr                 | 1.000                    |
|    ep_len_mean          | 14.308                   |
|    ep_rew_mean          | 0.967                    |
|    len                  | 14.308 +/- 6.60 (120)    |
|    len_d_1_pos          | 3.800 +/- 3.19 (10)      |
|    len_d_2_pos          | 5.917 +/- 3.15 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.490 +/- 5.09 (96)     |
|    len_neg              | 16.490 +/- 5.09 (96)     |
|    len_pos              | 5.583 +/- 4.38 (24)      |
|    mrr_mean             | 1.000                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.958 +/- 0.20 (24)      |
|    reward               | 0.917 +/- 0.41 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.917 +/- 0.41 (24)      |
|    success_rate         | 0.958                    |
----------------------------------------------------


[PPO] ===== Iteration 25 (786432/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 18.56s
[PPO] Recent episodes: reward=0.600, length=3.9

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.23568, value 0.12026, policy -0.00013, entropy -0.53651, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.24452, value 0.12010, policy -0.00019, entropy -0.53628, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.24743, value 0.11990, policy -0.00028, entropy -0.53603, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.22487, value 0.11968, policy -0.00038, entropy -0.53579, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.23245, value 0.11948, policy -0.00050, entropy -0.53556, approx_kl 0.00009 clip_fraction 0.00000. 
[PPO] Values: min=-0.684, max=1.859, mean=0.789, std=0.242
[PPO] Returns: min=-1.000, max=1.049, mean=0.796, std=0.414
[PPO] Explained variance: 0.2973
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.536                    |
|    explained_var        | 0.297                    |
|    iterations           | 25                       |
|    policy_loss          | -0.000                   |
|    total_timesteps      | 819200                   |
|    value_loss           | 0.119                    |
----------------------------------------------------

[PPO] Training completed in 5.84s
[PPO] Metrics: policy_loss=-0.0005, value_loss=0.1195, entropy=0.5356
[Metrics] Reward (train): current=0.784, best=0.784 (iter 25)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.287                    |
|    ep_rew_mean          | 0.784                    |
|    len                  | 7.287 +/- 6.77 (112301)  |
|    len_d_1_pos          | 2.122 +/- 1.02 (22031)   |
|    len_d_2_pos          | 5.758 +/- 3.92 (25689)   |
|    len_d_3_pos          | 11.272 +/- 4.61 (5149)   |
|    len_d_4_pos          | 12.307 +/- 4.54 (3324)   |
|    len_d_unknown_neg    | 9.353 +/- 7.89 (56108)   |
|    len_neg              | 9.353 +/- 7.89 (56108)   |
|    len_pos              | 5.225 +/- 4.56 (56193)   |
|    proven_d_1_pos       | 0.970 +/- 0.17 (22031)   |
|    proven_d_2_pos       | 0.907 +/- 0.29 (25689)   |
|    proven_d_3_pos       | 0.699 +/- 0.46 (5149)    |
|    proven_d_4_pos       | 0.655 +/- 0.48 (3324)    |
|    proven_d_unknown_neg | 0.114 +/- 0.32 (56108)   |
|    proven_neg           | 0.114 +/- 0.32 (56108)   |
|    proven_pos           | 0.897 +/- 0.30 (56193)   |
|    reward               | 0.784 +/- 0.62 (112301)  |
|    reward_d_1_pos       | 0.939 +/- 0.34 (22031)   |
|    reward_d_2_pos       | 0.814 +/- 0.58 (25689)   |
|    reward_d_3_pos       | 0.397 +/- 0.92 (5149)    |
|    reward_d_4_pos       | 0.310 +/- 0.95 (3324)    |
|    reward_d_unknown_neg | 0.773 +/- 0.63 (56108)   |
|    reward_neg           | 0.773 +/- 0.63 (56108)   |
|    reward_pos           | 0.795 +/- 0.61 (56193)   |
|    success_rate         | 0.506                    |
|    time/elapsed         | 743                      |
|    time/fps             | 1166                     |
|    total_timesteps      | 819200                   |
----------------------------------------------------

[Ranking] MRR: current=1.000, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 1.000                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 1.000                    |
|    MRR                  | 1.000                    |
|    _mrr                 | 1.000                    |
|    ep_len_mean          | 14.292                   |
|    ep_rew_mean          | 0.967                    |
|    len                  | 14.292 +/- 6.59 (120)    |
|    len_d_1_pos          | 3.800 +/- 3.19 (10)      |
|    len_d_2_pos          | 5.917 +/- 3.15 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.469 +/- 5.08 (96)     |
|    len_neg              | 16.469 +/- 5.08 (96)     |
|    len_pos              | 5.583 +/- 4.38 (24)      |
|    mrr_mean             | 1.000                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.917 +/- 0.29 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.958 +/- 0.20 (24)      |
|    reward               | 0.917 +/- 0.41 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.833 +/- 0.58 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.917 +/- 0.41 (24)      |
|    success_rate         | 0.958                    |
----------------------------------------------------


[PPO] ===== Iteration 26 (819200/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 19.03s
[PPO] Recent episodes: reward=1.000, length=10.8

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.25131, value 0.13056, policy 0.00014, entropy -0.53361, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.26512, value 0.13040, policy 0.00008, entropy -0.53335, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.25371, value 0.13023, policy -0.00004, entropy -0.53319, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.24706, value 0.13007, policy -0.00016, entropy -0.53304, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.26394, value 0.12991, policy -0.00028, entropy -0.53293, approx_kl 0.00009 clip_fraction 0.00000. 
[PPO] Values: min=-0.699, max=1.844, mean=0.790, std=0.249
[PPO] Returns: min=-1.000, max=1.045, mean=0.784, std=0.429
[PPO] Explained variance: 0.2915
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.533                    |
|    explained_var        | 0.291                    |
|    iterations           | 26                       |
|    policy_loss          | -0.000                   |
|    total_timesteps      | 851968                   |
|    value_loss           | 0.130                    |
----------------------------------------------------

[PPO] Training completed in 5.86s
[PPO] Metrics: policy_loss=-0.0003, value_loss=0.1299, entropy=0.5329
[Metrics] Reward (train): current=0.787, best=0.787 (iter 26)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.291                    |
|    ep_rew_mean          | 0.787                    |
|    len                  | 7.291 +/- 6.77 (116740)  |
|    len_d_1_pos          | 2.120 +/- 1.01 (22910)   |
|    len_d_2_pos          | 5.737 +/- 3.90 (26690)   |
|    len_d_3_pos          | 11.245 +/- 4.58 (5355)   |
|    len_d_4_pos          | 12.267 +/- 4.54 (3454)   |
|    len_d_unknown_neg    | 9.376 +/- 7.90 (58331)   |
|    len_neg              | 9.376 +/- 7.90 (58331)   |
|    len_pos              | 5.209 +/- 4.54 (58409)   |
|    proven_d_1_pos       | 0.971 +/- 0.17 (22910)   |
|    proven_d_2_pos       | 0.909 +/- 0.29 (26690)   |
|    proven_d_3_pos       | 0.706 +/- 0.46 (5355)    |
|    proven_d_4_pos       | 0.662 +/- 0.47 (3454)    |
|    proven_d_unknown_neg | 0.113 +/- 0.32 (58331)   |
|    proven_neg           | 0.113 +/- 0.32 (58331)   |
|    proven_pos           | 0.900 +/- 0.30 (58409)   |
|    reward               | 0.787 +/- 0.62 (116740)  |
|    reward_d_1_pos       | 0.941 +/- 0.34 (22910)   |
|    reward_d_2_pos       | 0.818 +/- 0.57 (26690)   |
|    reward_d_3_pos       | 0.413 +/- 0.91 (5355)    |
|    reward_d_4_pos       | 0.324 +/- 0.95 (3454)    |
|    reward_d_unknown_neg | 0.774 +/- 0.63 (58331)   |
|    reward_neg           | 0.774 +/- 0.63 (58331)   |
|    reward_pos           | 0.800 +/- 0.60 (58409)   |
|    success_rate         | 0.507                    |
|    time/elapsed         | 772                      |
|    time/fps             | 1106                     |
|    total_timesteps      | 851968                   |
----------------------------------------------------

[Ranking] MRR: current=0.969, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.958                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.958                    |
|    MRR                  | 0.969                    |
|    _mrr                 | 0.969                    |
|    ep_len_mean          | 14.525                   |
|    ep_rew_mean          | 0.950                    |
|    len                  | 14.525 +/- 6.54 (120)    |
|    len_d_1_pos          | 3.800 +/- 3.19 (10)      |
|    len_d_2_pos          | 7.250 +/- 5.07 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.594 +/- 5.03 (96)     |
|    len_neg              | 16.594 +/- 5.03 (96)     |
|    len_pos              | 6.250 +/- 5.26 (24)      |
|    mrr_mean             | 0.969                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.833 +/- 0.39 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.917 +/- 0.28 (24)      |
|    reward               | 0.833 +/- 0.56 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.667 +/- 0.78 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.833 +/- 0.56 (24)      |
|    success_rate         | 0.917                    |
----------------------------------------------------


[PPO] ===== Iteration 27 (851968/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 22.63s
[PPO] Recent episodes: reward=1.000, length=8.1

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.25196, value 0.13121, policy -0.00004, entropy -0.52979, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.25495, value 0.13103, policy -0.00011, entropy -0.52962, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.24293, value 0.13084, policy -0.00019, entropy -0.52946, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.25961, value 0.13064, policy -0.00032, entropy -0.52923, approx_kl 0.00007 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.27119, value 0.13044, policy -0.00044, entropy -0.52903, approx_kl 0.00008 clip_fraction 0.00000. 
[PPO] Values: min=-0.684, max=2.547, mean=0.789, std=0.240
[PPO] Returns: min=-1.000, max=1.088, mean=0.789, std=0.426
[PPO] Explained variance: 0.2751
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.529                    |
|    explained_var        | 0.275                    |
|    iterations           | 27                       |
|    policy_loss          | -0.000                   |
|    total_timesteps      | 884736                   |
|    value_loss           | 0.130                    |
----------------------------------------------------

[PPO] Training completed in 5.85s
[PPO] Metrics: policy_loss=-0.0004, value_loss=0.1304, entropy=0.5290
[Metrics] Reward (train): current=0.790, best=0.790 (iter 27)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.290                    |
|    ep_rew_mean          | 0.790                    |
|    len                  | 7.290 +/- 6.77 (121273)  |
|    len_d_1_pos          | 2.118 +/- 0.99 (23778)   |
|    len_d_2_pos          | 5.710 +/- 3.87 (27722)   |
|    len_d_3_pos          | 11.217 +/- 4.55 (5579)   |
|    len_d_4_pos          | 12.237 +/- 4.53 (3594)   |
|    len_d_unknown_neg    | 9.387 +/- 7.91 (60600)   |
|    len_neg              | 9.387 +/- 7.91 (60600)   |
|    len_pos              | 5.195 +/- 4.52 (60673)   |
|    proven_d_1_pos       | 0.971 +/- 0.17 (23778)   |
|    proven_d_2_pos       | 0.912 +/- 0.28 (27722)   |
|    proven_d_3_pos       | 0.715 +/- 0.45 (5579)    |
|    proven_d_4_pos       | 0.669 +/- 0.47 (3594)    |
|    proven_d_unknown_neg | 0.113 +/- 0.32 (60600)   |
|    proven_neg           | 0.113 +/- 0.32 (60600)   |
|    proven_pos           | 0.903 +/- 0.30 (60673)   |
|    reward               | 0.790 +/- 0.61 (121273)  |
|    reward_d_1_pos       | 0.942 +/- 0.33 (23778)   |
|    reward_d_2_pos       | 0.823 +/- 0.57 (27722)   |
|    reward_d_3_pos       | 0.431 +/- 0.90 (5579)    |
|    reward_d_4_pos       | 0.338 +/- 0.94 (3594)    |
|    reward_d_unknown_neg | 0.774 +/- 0.63 (60600)   |
|    reward_neg           | 0.774 +/- 0.63 (60600)   |
|    reward_pos           | 0.805 +/- 0.59 (60673)   |
|    success_rate         | 0.508                    |
|    time/elapsed         | 805                      |
|    time/fps             | 1005                     |
|    total_timesteps      | 884736                   |
----------------------------------------------------

[Ranking] MRR: current=0.935, best=1.000 (iter 17)
----------------------------------------------------
| eval_rank/              |                          |
|    Hits@1               | 0.917                    |
|    Hits@10              | 1.000                    |
|    Hits@3               | 0.917                    |
|    MRR                  | 0.935                    |
|    _mrr                 | 0.935                    |
|    ep_len_mean          | 14.692                   |
|    ep_rew_mean          | 0.933                    |
|    len                  | 14.692 +/- 6.53 (120)    |
|    len_d_1_pos          | 3.800 +/- 3.19 (10)      |
|    len_d_2_pos          | 8.417 +/- 6.23 (12)      |
|    len_d_3_pos          | 5.000 +/- nan (1)        |
|    len_d_4_pos          | 20.000 +/- nan (1)       |
|    len_d_unknown_neg    | 16.656 +/- 5.03 (96)     |
|    len_neg              | 16.656 +/- 5.03 (96)     |
|    len_pos              | 6.833 +/- 5.96 (24)      |
|    mrr_mean             | 0.935                    |
|    proven_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    proven_d_2_pos       | 0.750 +/- 0.45 (12)      |
|    proven_d_3_pos       | 1.000 +/- nan (1)        |
|    proven_d_4_pos       | 1.000 +/- nan (1)        |
|    proven_d_unknown_neg | 0.010 +/- 0.10 (96)      |
|    proven_neg           | 0.010 +/- 0.10 (96)      |
|    proven_pos           | 0.875 +/- 0.34 (24)      |
|    reward               | 0.750 +/- 0.68 (24)      |
|    reward_d_1_pos       | 1.000 +/- 0.00 (10)      |
|    reward_d_2_pos       | 0.500 +/- 0.90 (12)      |
|    reward_d_3_pos       | 1.000 +/- nan (1)        |
|    reward_d_4_pos       | 1.000 +/- nan (1)        |
|    reward_d_unknown_neg | 0.979 +/- 0.20 (96)      |
|    reward_neg           | 0.979 +/- 0.20 (96)      |
|    reward_pos           | 0.750 +/- 0.68 (24)      |
|    success_rate         | 0.875                    |
----------------------------------------------------


[PPO] ===== Iteration 28 (884736/1000000 steps) =====
Collecting rollouts: 0/256 steps
Collecting rollouts: 51/256 steps
Collecting rollouts: 102/256 steps
Collecting rollouts: 153/256 steps
Collecting rollouts: 204/256 steps
Collecting rollouts: 255/256 steps
[PPO] Rollout collected in 21.27s
[PPO] Recent episodes: reward=1.000, length=8.7

[PPO] ===== Training policy =====
[PPO] Training for 5 epochs...
Epoch 1/5. 
Losses: total 0.25993, value 0.13199, policy -0.00006, entropy -0.52517, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 2/5. 
Losses: total 0.25534, value 0.13183, policy -0.00010, entropy -0.52506, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 3/5. 
Losses: total 0.27186, value 0.13168, policy -0.00017, entropy -0.52498, approx_kl 0.00005 clip_fraction 0.00000. 
Epoch 4/5. 
Losses: total 0.25065, value 0.13155, policy -0.00027, entropy -0.52483, approx_kl 0.00006 clip_fraction 0.00000. 
Epoch 5/5. 
Losses: total 0.25069, value 0.13139, policy -0.00036, entropy -0.52472, approx_kl 0.00007 clip_fraction 0.00000. 
[PPO] Values: min=-0.707, max=1.852, mean=0.795, std=0.243
[PPO] Returns: min=-1.000, max=1.282, mean=0.782, std=0.437
[PPO] Explained variance: 0.3101
----------------------------------------------------
| train/                  |                          |
|    approx_kl            | 0.000                    |
|    clip_fraction        | 0.000                    |
|    entropy              | 0.525                    |
|    explained_var        | 0.310                    |
|    iterations           | 28                       |
|    policy_loss          | -0.000                   |
|    total_timesteps      | 917504                   |
|    value_loss           | 0.131                    |
----------------------------------------------------

[PPO] Training completed in 5.80s
[PPO] Metrics: policy_loss=-0.0004, value_loss=0.1314, entropy=0.5247
[Metrics] Reward (train): current=0.792, best=0.792 (iter 28)
----------------------------------------------------
| rollout/                |                          |
|    ep_len_mean          | 7.291                    |
|    ep_rew_mean          | 0.792                    |
|    len                  | 7.291 +/- 6.78 (125719)  |
|    len_d_1_pos          | 2.116 +/- 0.98 (24628)   |
|    len_d_2_pos          | 5.677 +/- 3.84 (28750)   |
|    len_d_3_pos          | 11.193 +/- 4.53 (5791)   |
|    len_d_4_pos          | 12.203 +/- 4.52 (3736)   |
|    len_d_unknown_neg    | 9.406 +/- 7.92 (62814)   |
|    len_neg              | 9.406 +/- 7.92 (62814)   |
|    len_pos              | 5.178 +/- 4.50 (62905)   |
|    proven_d_1_pos       | 0.972 +/- 0.16 (24628)   |
|    proven_d_2_pos       | 0.914 +/- 0.28 (28750)   |
|    proven_d_3_pos       | 0.722 +/- 0.45 (5791)    |
|    proven_d_4_pos       | 0.676 +/- 0.47 (3736)    |
|    proven_d_unknown_neg | 0.113 +/- 0.32 (62814)   |
|    proven_neg           | 0.113 +/- 0.32 (62814)   |
|    proven_pos           | 0.905 +/- 0.29 (62905)   |
|    reward               | 0.792 +/- 0.61 (125719)  |
|    reward_d_1_pos       | 0.944 +/- 0.33 (24628)   |
|    reward_d_2_pos       | 0.828 +/- 0.56 (28750)   |
|    reward_d_3_pos       | 0.444 +/- 0.90 (5791)    |
|    reward_d_4_pos       | 0.352 +/- 0.94 (3736)    |
|    reward_d_unknown_neg | 0.774 +/- 0.63 (62814)   |
|    reward_neg           | 0.774 +/- 0.63 (62814)   |
|    reward_pos           | 0.810 +/- 0.59 (62905)   |
|    success_rate         | 0.509                    |
|    time/elapsed         | 836                      |
|    time/fps             | 1042                     |
|    total_timesteps      | 917504                   |
----------------------------------------------------

^CTraceback (most recent call last):